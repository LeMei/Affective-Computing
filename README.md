
_Papers_
----------------
### _Missing_modality_ ###
* [1] T Zhou, S Canu, P Vera, S Ruan.modality fusion based deep neural network for brain tumor segmentation with missing MR modalities
    Abstract Using multimodal Magnetic Resonance Imaging (MRI) is necessary for accurate brain tumor segmentation. The main problem is that not all types of MRIs are always available in clinical exams. Based on the fact that there is a strong correlation between MR modalities of the same patient, in this work, we propose a novel brain tumor segmentation network in the case of missing one or more modalities. The proposed network consists of three sub-networks: a feature-enhanced generator, a correlation constraint block and a …
    [[_paper_]](https://www.sciencedirect.com/science/article/pii/S0925231221013904)

* [2] J Zhao, R Li, Q Jin.Missing Modality Imagination Network for Emotion Recognition with Uncertain Missing Modalities
    Multimodal fusion has been proved to improve emotion recognition performance in previous works. However, in real-world applications, we often encounter the problem of missing modality, and which modalities will be missing is uncertain. It makes the fixed multimodal fusion fail in such cases. In this work, we propose a unified model, Missing Modality Imagination Network (MMIN), to deal with the uncertain missing modality problem. MMIN learns robust joint multimodal representations, which can predict the representation of any …
    [[_paper_]](https://aclanthology.org/2021.acl-long.203/)

* [3] W Luo, M Xu, H Lai.Multimodal Reconstruct and Align Net for Missing Modality Problem in Sentiment Analysis
    Abstract Multimodal Sentiment Analysis (MSA) aims at recognizing emotion categories by textual, visual, and acoustic cues. However, in real-life scenarios, one or two modalities may be missing due to various reasons. And when text modality is missing, obvious deterioration will be observed since text modality contains much more semantic information compared to vision and audio modality. To this end, we propose the Multimodal Reconstruct and Align Net (MRAN) to tackle the missing modality problem, especially to relieve the decline caused …
    [[_paper_]](https://link.springer.com/chapter/10.1007/978-3-031-27818-1_34)

* [4] Q Zhang, L Shi, P Liu, Z Zhu, L Xu - 2023 - Springer. integrating consistency and difference networks by transformer for multimodal sentiment analysis
    The Editor-in-Chief has retracted this article because of an overlap with a previously-published article by different authors [1] due to insufficient citations and inappropriate expressions quoted from [1]. As Table 1 in this article overlaps with Table 2 in [1], and formulas in Sect. 3.2 are the same as in [1] without explicit indication. Moreover, without fair indication, the schematic presentation in the article, is the same as in [1]. The authors did not explicitly state whether they agree to this retraction.
    [[_paper_]](https://link.springer.com/article/10.1007/s10489-023-04869-x)

* [5] J Zeng, T Liu, J Zhou.assisted Multimodal Sentiment Analysis under Uncertain Missing Modalities
    Multimodal sentiment analysis has been studied under the assumption that all modalities are available. However, such a strong assumption does not always hold in practice, and most of multimodal fusion models may fail when partial modalities are missing. Several works have addressed the missing modality problem; but most of them only considered the single modality missing case, and ignored the practically more general cases of multiple modalities missing. To this end, in this paper, we propose a Tag-Assisted Transformer …
    [[_paper_]](https://dl.acm.org/doi/abs/10.1145/3477495.3532064)

* [6] S Parthasarathy, S Sundaram.Visual Expression Recognition
    Automatic audio-visual expression recognition can play an important role in communication services such as tele-health, VOIP calls and human-machine interaction. Accuracy of audio-visual expression recognition could benefit from the interplay between the two modalities. However, most audio-visual expression recognition systems, trained in ideal conditions, fail to generalize in real world scenarios where either the audio or visual modality could be missing due to a number of reasons such as limited bandwidth, interactors' orientation, caller …
    [[_paper_]](https://dl.acm.org/doi/abs/10.1145/3395035.3425202)

* [7] C Shang, A Palmer, J Sun, KS Chen…. Missing View Imputation with Generative Adversarial Networks
    In an era when big data are becoming the norm, there is less concern with the quantity but more with the quality and completeness of the data. In many disciplines, data are collected from heterogeneous sources, resulting in multi-view or multi-modal datasets. The missing data problem has been challenging to address in multi-view data analysis. Especially, when certain samples miss an entire view of data, it creates the missing view problem. Classic multiple imputations or matrix completion methods are hardly effective here when no …
    [[_paper_]](https://ieeexplore.ieee.org/abstract/document/8257992/)


  
* [1]Zhou, T., Canu, S., Vera, P., & Ruan, S. (2021). Feature-enhanced generation and multi-modality fusion based deep neural network for brain tumor segmentation with missing MR modalities. Neurocomputing, 466, 102-112.  
    [[_paper_]](https://www.sciencedirect.com/science/article/abs/pii/S0925231221013904)
* [2]Zhao, J., Li, R., & Jin, Q. (2021, August). Missing modality imagination network for emotion recognition with uncertain missing modalities. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) (pp. 2608-2618).  
    [[_paper_]](https://aclanthology.org/2021.acl-long.203/) [[_code_]](https://github.com/AIM3-RUC/MMIN)
* [3]Luo, W., Xu, M., & Lai, H. (2023, January). Multimodal reconstruct and align net for missing modality problem in sentiment analysis. In International Conference on Multimedia Modeling (pp. 411-422). Cham: Springer Nature Switzerland.  
    [[_paper_]](https://link.springer.com/chapter/10.1007/978-3-031-27818-1_34)
* [4]Zhang, Q., Shi, L., Liu, P., Zhu, Z., & Xu, L. (2023). Retraction Note: ICDN: integrating consistency and difference networks by transformer for multimodal sentiment analysis.  
    [[_paper_]](https://link.springer.com/article/10.1007/s10489-023-04869-x)
* [5]Zeng, J., Liu, T., & Zhou, J. (2022, July). Tag-assisted multimodal sentiment analysis under uncertain missing modalities. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval (pp. 1545-1554).  
    [[_paper_]](https://dl.acm.org/doi/abs/10.1145/3477495.3532064) [[_code_]](https://github.com/JaydenZeng/TATE)
* [6]Parthasarathy, S., & Sundaram, S. (2020, October). Training strategies to handle missing modalities for audio-visual expression recognition. In Companion Publication of the 2020 International Conference on Multimodal Interaction (pp. 400-404).  
    [[_paper_]](https://dl.acm.org/doi/abs/10.1145/3395035.3425202)
* [7]Shang, C., Palmer, A., Sun, J., Chen, K. S., Lu, J., & Bi, J. (2017, December). VIGAN: Missing view imputation with generative adversarial networks. In 2017 IEEE International conference on big data (Big Data) (pp. 766-775). IEEE.  
    [[_paper_]](https://ieeexplore.ieee.org/abstract/document/8257992) [[_code_]](https://github.com/chaoshangcs/VIGAN)



### _multimodal affective_ ###
#### _MABSA_ ####
* [1] H Zhao, M Yang, X Bai, H Liu.Based Sentiment Analysis
    Multimodal Aspect-Based Sentiment Analysis (MABSA), as an emerging task in the field of sentiment analysis, has recently received widespread attention. Its aim is to combine relevant multimodal data to determine the sentiment polarity of a given aspect in text. Researchers have surveyed both aspect-based sentiment analysis and multimodal sentiment analysis, but, to the best of our knowledge, there is no survey on MABSA. Therefore, in order to assist related researchers to know MABSA better, we surveyed the …
    [[_paper_]](https://ieeexplore.ieee.org/abstract/document/10401113/)

* [2] C Wang, Y Luo, C Meng, F Yuan.Based Sentiment Analysis 
    Aspect-based Sentiment Analysis (ABSA), also known as fine-grained sentiment analysis, aims to predict the sentiment polarity of specific aspect words in the sentence. Some studies have explored the semantic correlation between words in sentences through attention-based methods. Other studies have learned syntactic knowledge by using graph convolution networks to introduce dependency relations. These methods have achieved satisfactory results in the ABSA tasks. However, due to the complexity of language, effectively capturing …
    [[_paper_]](https://dl.acm.org/doi/abs/10.1145/3659579)

* [3] R Zhou, W Guo, X Liu, S Yu, Y Zhang….oriented Information for Multimodal
    Multimodal aspect-based sentiment analysis (MABSA) aims to extract aspects from text-image pairs and recognize their sentiments. Existing methods make great efforts to align the whole image to corresponding aspects. However, different regions of the image may relate to different aspects in the same sentence, and coarsely establishing image-aspect alignment will introduce noise to aspect-based sentiment analysis (ie, visual noise). Besides, the sentiment of a specific aspect can also be interfered by descriptions of other aspects (ie …
    [[_paper_]](https://arxiv.org/abs/2306.01004)

* [4] L Xiao, X Wu, S Yang, J Xu, J Zhou, L He.based sentiment analysis
    Abstract Multi-modal Aspect-based Sentiment Analysis (MABSA) aims to forecast the polarity of sentiment concerning aspects within a given sentence based on the correlation between the sentence and its accompanying image. Comprehending multi-modal sentiment expression requires strong cross-modal alignment and fusion ability. Previous state-of-the-art (SOTA) models fail to explicitly align valuable visual clues with aspect and sentiment information in textual representations and overlook the utilization of syntactic dependency …
    [[_paper_]](https://www.sciencedirect.com/science/article/pii/S0306457323002455)

* [5] L Yang, JC Na, J Yu.Based Sentiment Analysis
    As an emerging task in opinion mining, End-to-End Multimodal Aspect-Based Sentiment Analysis (MABSA) aims to extract all the aspect-sentiment pairs mentioned in a pair of sentence and image. Most existing methods of MABSA do not explicitly incorporate aspect and sentiment information in their textual and visual representations and fail to consider the different contributions of visual representations to each word or aspect in the text. To tackle these limitations, we propose a multi-task learning framework named Cross-Modal Multitask …
    [[_paper_]](https://www.sciencedirect.com/science/article/pii/S0306457322001479)

* [6] Z Yu, J Wang, LC Yu, X Zhang.based Sentiment Analysis
    Multimodal aspect-based sentiment analysis (MABSA) aims to extract the aspect terms from text and image pairs, and then analyze their corresponding sentiment. Recent studies typically use either a pipeline method or a unified transformer based on a cross-attention mechanism. However, these methods fail to explicitly and effectively incorporate the alignment between text and image. Supervised finetuning of the universal transformers for MABSA still requires a certain number of aligned image-text pairs. This study proposes a …
    [[_paper_]](https://aclanthology.org/2022.aacl-main.32/)

* [7] Not found.Level_Multimodal_Sentiment_Classification
    Not found
    [[_paper_]](Not found)

* [8] X Yang, S Feng, D Wang, S Qi, W Wu, Y Zhang….Sentiment Analysis Based on Generative Multimodal Prompt
    We have witnessed the rapid proliferation of multimodal data on numerous social media platforms. Conventional studies typically require massive labeled data to train models for Multimodal Aspect-Based Sentiment Analysis (MABSA). However, collecting and annotating fine-grained multimodal data for MABSA is tough. To alleviate the above issue, we perform three MABSA-related tasks with quite a small number of labeled multimodal samples. We first build diverse and comprehensive multimodal few-shot datasets according to the data …
    [[_paper_]](https://arxiv.org/abs/2305.10169)

* [9] X Yang, S Feng, D Wang, S Qi, W Wu, Y Zhang….sentiment analysis based on generative
    We have witnessed the rapid proliferation of multimodal data on numerous social media platforms. Conventional studies typically require massive labeled data to train models for Multimodal Aspect-Based Sentiment Analysis (MABSA). However, collecting and annotating fine-grained multimodal data for MABSA is tough. To alleviate the above issue, we perform three MABSA-related tasks with quite a small number of labeled multimodal samples. We first build diverse and comprehensive multimodal few-shot datasets according to the data …
    [[_paper_]](https://arxiv.org/abs/2305.10169)

* [10] Not found.ResNeXt_Network_for_Aspect_Based_Multimodal_Sentiment_Analysis
    Not found
    [[_paper_]](Not found)

* [11] Not found.level_Sentiment_Classification
    Not found
    [[_paper_]](Not found)

* [12] Not found.Based_Multimodal_Sentiment_Analysis
    Not found
    [[_paper_]](Not found)

* [13] K Zhang, K Zhang, M Zhang, H Zhao, Q Liu….based Sentiment Analysis
    … To equip the pre-trained models with the ability to capture … dynamic semantics, we present 
a Dynamic Re-weighting BERT (DRBERT) model, which considers the aspect-aware dynamic …
    [[_paper_]](https://arxiv.org/abs/2203.16369)

* [14] X Ju, D Zhang, R Xiao, J Li, S Li….modal Relation Detection
    Aspect terms extraction (ATE) and aspect sentiment classification (ASC) are two fundamental and fine-grained sub-tasks in aspect-level sentiment analysis (ALSA). In the textual analysis, joint extracting both aspect terms and sentiment polarities has been drawn much attention due to the better applications than individual sub-task. However, in the multi-modal scenario, the existing studies are limited to handle each sub-task independently, which fails to model the innate connection between the above two objectives and ignores …
    [[_paper_]](https://aclanthology.org/2021.emnlp-main.360/)

* [15] X Ju, D Zhang, R Xiao, J Li, S Li….modal
    Aspect terms extraction (ATE) and aspect sentiment classification (ASC) are two fundamental and fine-grained sub-tasks in aspect-level sentiment analysis (ALSA). In the textual analysis, joint extracting both aspect terms and sentiment polarities has been drawn much attention due to the better applications than individual sub-task. However, in the multi-modal scenario, the existing studies are limited to handle each sub-task independently, which fails to model the innate connection between the above two objectives and ignores …
    [[_paper_]](https://aclanthology.org/2021.emnlp-main.360/)

* [16] F Zhao, C Li, Z Wu, Y Ouyang, J Zhang….curriculum Denoising Framework for
    Multimodal Aspect-based Sentiment Analysis (MABSA) is a fine-grained Sentiment Analysis task, which has attracted growing research interests recently. Existing work mainly utilizes image information to improve the performance of MABSA task. However, most of the studies overestimate the importance of images since there are many noise images unrelated to the text in the dataset, which will have a negative impact on model learning. Although some work attempts to filter low-quality noise images by setting thresholds, relying on thresholds …
    [[_paper_]](https://arxiv.org/abs/2310.14605)

* [17] F Zhao, C Li, Z Wu, Y Ouyang, J Zhang….based Sentiment Analysis
    Multimodal Aspect-based Sentiment Analysis (MABSA) is a fine-grained Sentiment Analysis task, which has attracted growing research interests recently. Existing work mainly utilizes image information to improve the performance of MABSA task. However, most of the studies overestimate the importance of images since there are many noise images unrelated to the text in the dataset, which will have a negative impact on model learning. Although some work attempts to filter low-quality noise images by setting thresholds, relying on thresholds …
    [[_paper_]](https://arxiv.org/abs/2310.14605)

* [18] J Zhou, J Zhao, JX Huang, QV Hu, L He.based sentiment
    Aspect-based sentiment analysis has obtained great success in recent years. Most of the existing work focuses on determining the sentiment polarity of the given aspect according to the given text, while little attention has been paid to the visual information as well as multimodality content for aspect-based sentiment analysis. Multimodal content is becoming increasingly popular in mainstream online social platforms and can help better extract user sentiments toward a given aspect. There are only few studies focusing on this new task …
    [[_paper_]](https://www.sciencedirect.com/science/article/pii/S0925231221007931)

* [19] Not found.Based
    Not found
    [[_paper_]](Not found)

* [20] Not found.Level_Sentiment_Analysis
    Not found
    [[_paper_]](Not found)

* [21] Z Zhang, Z Wang, X Li, N Liu, B Guo, Z Yu.level sentiment classification model by exploring multimodal data with fusion discriminant attentional networks
    Aspect-level sentiment classification aims to identify sentiment polarity over each aspect of a sentence. In the past, such analysis tasks mainly relied on text data. Nowadays, due to the popularization of smart devices and Internet services, people are generating more abundant data, including text, image, video, et al. Multimodal data from the same post (eg, a tweet) usually has certain correlation. For example, image data might has an auxiliary effect on the text data, and reasonable processing of such multimodal data can help obtain much richer …
    [[_paper_]](https://link.springer.com/article/10.1007/s11280-021-00955-7)

* [22] H Jin, J Tan, L Liu, L Qiu, S Yao, X Chen….Training
    To enhance the effectiveness of matching user requests with millions of online products, practitioners invest significant efforts in developing semantic relevance models on large-scale e-commerce platforms. Generally, such semantic relevance models are formulated as text-matching approaches, which measure the relevance between users' search queries and the titles of candidate items (ie, products). However, we argue that conventional relevance methods may lead to sub-optimal performance due to the limited information provided by the …
    [[_paper_]](https://dl.acm.org/doi/abs/10.1145/3583780.3615224)

* [23] J Yang, Y Xiao, X Du.based multimodal sentiment analysis 
    Aspect-based multimodal sentiment analysis (ABMSA) is an important branch of multimodal sentiment analysis. The goal of ABMSA is to use multimodal information to infer users' sentiment polarity toward the targeted aspect for supporting corresponding decision-making. The existing ABMSA methods usually focus on exploring aspect-aware fine-grained interactions and demonstrate the benefits of integrating multimodal information. However, such approaches still suffer from the following limitations:(1) coarse-grained semantic …
    [[_paper_]](https://www.sciencedirect.com/science/article/pii/S0950705124003599)

* [24] N Xu, W Mao, G Chen.Interactive Memory Network for Aspect Based Multimodal Sentiment Analysis
    As a fundamental task of sentiment analysis, aspect-level sentiment analysis aims to identify the sentiment polarity of a specific aspect in the context. Previous work on aspect-level sentiment analysis is text-based. With the prevalence of multimodal user-generated content (eg text and image) on the Internet, multimodal sentiment analysis has attracted increasing research attention in recent years. In the context of aspect-level sentiment analysis, multimodal data are often more important than text-only data, and have various correlations …
    [[_paper_]](https://ojs.aaai.org/index.php/AAAI/article/view/3807)

* [25] N Xu, W Mao, G Chen.Multi_Interactive Memory Network for Aspect Based Multimodal Sentiment Analysis
    As a fundamental task of sentiment analysis, aspect-level sentiment analysis aims to identify the sentiment polarity of a specific aspect in the context. Previous work on aspect-level sentiment analysis is text-based. With the prevalence of multimodal user-generated content (eg text and image) on the Internet, multimodal sentiment analysis has attracted increasing research attention in recent years. In the context of aspect-level sentiment analysis, multimodal data are often more important than text-only data, and have various correlations …
    [[_paper_]](https://ojs.aaai.org/index.php/AAAI/article/view/3807)

* [26] M Anschütz, T Eder, G Groh.Based Sentiment Analysis
    People post their opinions and experiences on social media, yielding rich databases of end-users' sentiments. This paper shows to what extent machine learning can analyze and structure these databases. An automated data analysis pipeline is deployed to provide insights into user-generated content for researchers in other domains. First, the domain expert can select an image and a term of interest. Then, the pipeline uses image retrieval to find all images showing similar content and applies aspect-based sentiment analysis to …
    [[_paper_]](https://ieeexplore.ieee.org/abstract/document/10066699/)

* [27] J Yu, J Wang, R Xia, J Li.Targeted Multimodal Sentiment Classification Based on
    Abstract Targeted Multimodal Sentiment Classification (TMSC) aims to identify the sentiment polarities over each target mentioned in a pair of sentence and image. Existing methods to TMSC failed to explicitly capture both coarse-grained and fine-grained image-target matching, including 1) the relevance between the image and the target and 2) the alignment between visual objects and the target. To tackle this issue, we propose a new multi-task learning architecture named coarse-to-fine grained Image-Target Matching network (ITM) …
    [[_paper_]](https://www.ijcai.org/proceedings/2022/0622.pdf)

* [28] Not found.Head_Fusion_Network
    Not found
    [[_paper_]](Not found)

* [29] Y Ling, J Yu, R Xia.Based Sentiment Analysis 2
    As an important task in sentiment analysis, Multimodal Aspect-Based Sentiment Analysis (MABSA) has attracted increasing attention in recent years. However, previous approaches either (i) use separately pre-trained visual and textual models, which ignore the crossmodal alignment or (ii) use vision-language models pre-trained with general pre-training tasks, which are inadequate to identify finegrained aspects, opinions, and their alignments across modalities. To tackle these limitations, we propose a task-specific Vision-Language Pre …
    [[_paper_]](https://arxiv.org/abs/2204.07955)

* [30] Y Ling, J Yu, R Xia.Based Sentiment Analysis
    As an important task in sentiment analysis, Multimodal Aspect-Based Sentiment Analysis (MABSA) has attracted increasing attention in recent years. However, previous approaches either (i) use separately pre-trained visual and textual models, which ignore the crossmodal alignment or (ii) use vision-language models pre-trained with general pre-training tasks, which are inadequate to identify finegrained aspects, opinions, and their alignments across modalities. To tackle these limitations, we propose a task-specific Vision-Language Pre …
    [[_paper_]](https://arxiv.org/abs/2204.07955)

* [31] QT Truong, HW Lauw.VistaNet_Visual Aspect Attention Network for Multimodal Sentiment Analysis
    Detecting the sentiment expressed by a document is a key task for many applications, eg, modeling user preferences, monitoring consumer behaviors, assessing product quality. Traditionally, the sentiment analysis task primarily relies on textual content. Fueled by the rise of mobile phones that are often the only cameras on hand, documents on the Web (eg, reviews, blog posts, tweets) are increasingly multimodal in nature, with photos in addition to textual content. A question arises whether the visual component could be useful for …
    [[_paper_]](https://ojs.aaai.org/index.php/AAAI/article/view/3799)

* [32] D Lu, L Neves, V Carvalho, N Zhang….Visual Attention Model for Name Tagging in Multimodal Social Media
    Everyday billions of multimodal posts containing both images and text are shared in social media sites such as Snapchat, Twitter or Instagram. This combination of image and text in a single message allows for more creative and expressive forms of communication, and has become increasingly common in such sites. This new paradigm brings new challenges for natural language understanding, as the textual component tends to be shorter, more informal, and often is only understood if combined with the visual context. In this paper, we …
    [[_paper_]](https://aclanthology.org/P18-1185/)


* [1]Zhao, H., Yang, M., Bai, X., & Liu, H. (2024). A Survey on Multimodal Aspect-Based Sentiment Analysis. IEEE Access.   
    [[_paper_]](https://ieeexplore.ieee.org/abstract/document/10401113)
* [2]Wang, C., Luo, Y., Meng, C., & Yuan, F. (2024). An adaptive Dual Graph Convolution Fusion Network for Aspect-Based Sentiment Analysis. ACM Transactions on Asian and Low-Resource Language Information Processing.    
    [[_paper_]](https://dl.acm.org/doi/abs/10.1145/3659579)
* [3]Zhou, R., Guo, W., Liu, X., Yu, S., Zhang, Y., & Yuan, X. (2023). AoM: Detecting aspect-oriented information for multimodal aspect-based sentiment analysis. arXiv preprint arXiv:2306.01004.     
    [[_paper_]](https://arxiv.org/abs/2306.01004) [[_code_]](https://github.com/SilyRab/AoM)
* [4]Xiao, L., Wu, X., Yang, S., Xu, J., Zhou, J., & He, L. (2023). Cross-modal fine-grained alignment and fusion network for multimodal aspect-based sentiment analysis. Information Processing & Management, 60(6), 103508.    
    [[_paper_]](https://www.sciencedirect.com/science/article/abs/pii/S0306457323002455)
* [5]Yu, Z., Wang, J., Yu, L. C., & Zhang, X. (2022, November). Dual-encoder transformers with cross-modal alignment for multimodal aspect-based sentiment analysis. In Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) (pp. 414-423).    
    [[_paper_]](https://aclanthology.org/2022.aacl-main.32/) [[_code_]](https://github.
com/windforfurture/DTCA)
* [6]Yang, L., Na, J. C., & Yu, J. (2022). Cross-modal multitask transformer for end-to-end multimodal aspect-based sentiment analysis. Information Processing & Management, 59(5), 103038.    
    [[_paper_]](https://www.sciencedirect.com/science/article/abs/pii/S0306457322001479)
* [7]J. Yu, J. Jiang and R. Xia, "Entity-Sensitive Attention and Fusion Network for Entity-Level Multimodal Sentiment Classification," in IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 28, pp. 429-439, 2020, doi: 10.1109/TASLP.2019.2957872.         
    [[_paper_]](https://ieeexplore.ieee.org/document/8926404) [[_code_]](https://github.com/jefferyYu/ESAFN)
* [8]Yang, X., Feng, S., Wang, D., Qi, S., Wu, W., Zhang, Y., ... & Poria, S. (2023). Few-shot joint multimodal aspect-sentiment analysis based on generative multimodal prompt. arXiv preprint arXiv:2305.10169.     
    [[_paper_]](https://arxiv.org/abs/2305.10169) [[_code_]](https://github.com/YangXiaocui1215/GMP.)
* [9]J. Zhao and F. Yang, "Fusion with GCN and SE-ResNeXt Network for Aspect Based Multimodal Sentiment Analysis," 2023 IEEE 6th Information Technology,Networking,Electronic and Automation Control Conference (ITNEC), Chongqing, China, 2023, pp. 336-340, doi: 10.1109/ITNEC56291.2023.10082618.    
    [[_paper_]](https://ieeexplore.ieee.org/document/10082618) 
* [10]



#### _MECPE_ ####
* [1] W Li, Y Li, V Pandelea, M Ge, L Zhu….Cause Pair Extraction in Conversations
    Conversational sentiment analysis (CSA) and emotion-cause pair extraction (ECPE) tasks have attracted increasing attention in recent years. The former aims to predict the sentiment states of speakers in a conversation, and the latter is about extracting emotion-cause clauses in a document. However, one drawback of CSA is that it cannot model the causal reasoning among emotion and neutral utterances from different speakers. In this work, we propose a new task: emotion-cause pair extraction in conversations (ECPEC), which aims to …
    [[_paper_]](https://ieeexplore.ieee.org/abstract/document/9926166/)

* [2] SV Mathur, AR Jindal, H Mittal…. Exploring Multimodal Emotion Cause Pair Extraction as Sequence Labelling Task
    Conversation is the most natural form of human communication, where each utterance can range over a variety of possible emotions. While significant work has been done towards the detection of emotions in text, relatively little work has been done towards finding the cause of the said emotions, especially in multimodal settings. SemEval 2024 introduces the task of Multimodal Emotion Cause Analysis in Conversations, which aims to extract emotions reflected in individual utterances in a conversation involving multiple modalities (textual …
    [[_paper_]](https://arxiv.org/abs/2404.02088)

* [3] F Wang, Z Ding, R Xia, Z Li, J Yu.Cause Pair Extraction in Conversations
    Conversation is an important form of human communication and contains a large number of emotions. It is interesting to discover emotions and their causes in conversations. Conversation in its natural form is multimodal. Many studies have been carried out on multimodal emotion recognition in conversations, yet there is still a lack of work on multimodal emotion cause analysis. In this article, we introduce a new task named Multimodal Emotion-Cause Pair Extraction in Conversations, aiming to jointly extract …
    [[_paper_]](https://ieeexplore.ieee.org/abstract/document/9969873/)

#### _MERC_ ####
* [1] S Harata, T Sakuma, S Kato.6_18
    In Affective Computing, a mathematical representation of emotions in the computer is 
desirable for emotionally interactive agents. This study aims to obtain a latent representation of …
    [[_paper_]](https://link.springer.com/chapter/10.1007/978-3-031-06388-6_18)

* [2] FA Rahman, G Lu.Time Multimodal Emotion Recognition for Conversational Agents using Graph Convolutional Networks in Reinforcement Learning
    Owing to the recent developments in Generative Artificial Intelligence (GenAI) and Large Language Models (LLM), conversational agents are becoming increasingly popular and accepted. They provide a human touch by interacting in ways familiar to us and by providing support as virtual companions. Therefore, it is important to understand the user's emotions in order to respond considerately. Compared to the standard problem of emotion recognition, conversational agents face an additional constraint in that recognition must be real-time …
    [[_paper_]](https://arxiv.org/abs/2310.18363)

* [3] W Zheng, J Yu, R Xia, S Wang.party Conversations
    Abstract Multimodal Emotion Recognition in Multiparty Conversations (MERMC) has recently attracted considerable attention. Due to the complexity of visual scenes in multi-party conversations, most previous MERMC studies mainly focus on text and audio modalities while ignoring visual information. Recently, several works proposed to extract face sequences as visual features and have shown the importance of visual information in MERMC. However, given an utterance, the face sequence extracted by previous methods …
    [[_paper_]](https://aclanthology.org/2023.acl-long.861/)

* [4] Y Zhang, J Wang, Y Liu, L Rong, Q Zheng, D Song….A Multitask learning model for multimodal sarcasm, sentiment and emotion recognition in conversations
    Sarcasm, sentiment and emotion are tightly coupled with each other in that one helps the understanding of another, which makes the joint recognition of sarcasm, sentiment and emotion in conversation a focus in the research in artificial intelligence (AI) and affective computing. Three main challenges exist: Context dependency, multimodal fusion and multitask interaction. However, most of the existing works fail to explicitly leverage and model the relationships among related tasks. In this paper, we aim to generically address …
    [[_paper_]](https://www.sciencedirect.com/science/article/pii/S1566253523000040)

* [5] M Sharafi, M Yazdchi, R Rasti, F Nasimi.temporal convolutional neural framework for multimodal emotion recognition
    Proposing a practical method for high-performance emotion recognition could facilitate human–computer interaction. Among existing methods, deep learning techniques have improved the performance of emotion recognition systems. In this work, a new multimodal neural design is presented wherein audio and visual data are combined as the input to a hybrid network comprised of a bidirectional long short term memory (BiLSTM) network and two convolutional neural networks (CNNs). The spatial and temporal features extracted from …
    [[_paper_]](https://www.sciencedirect.com/science/article/pii/S1746809422004694)

* [6] B Pan, K Hirota, Z Jia, Y Dai.A review of multimodal emotion recognition from datasets, preprocessing, features, and fusion methods
    Affective computing is one of the most important research fields in modern human–computer interaction (HCI). The goal of affective computing is to study and develop the theories, methods, and systems that can recognize, explain, process, and simulate human emotions. As a branch of affective computing, emotion recognition aims to enlighten the machine/computer automatically analyzing human emotions, which has received increasing attention from researchers in various fields. Human beings generally observe and …
    [[_paper_]](https://www.sciencedirect.com/science/article/pii/S092523122300989X)

* [7] DS Chauhan, GV Singh, A Arora, A Ekbal….A Sentiment and Emotion aware Multimodal Multiparty Humor Recognition in Multilingual Conversational Setting
    In this paper, we hypothesize that humor is closely related to sentiment and emotions. Also, due to the tremendous growth in multilingual content, there is a great demand for building models and systems that support multilingual information access. To end this, we first extend the recently released Multimodal Multiparty Hindi Humor (M2H2) dataset by adding parallel English utterances corresponding to Hindi utterances and then annotating each utterance with sentiment and emotion classes. We name it Sentiment, Humor, and Emotion aware …
    [[_paper_]](https://aclanthology.org/2022.coling-1.587/)

* [8] X Li, J Song, Z Zhao, C Wang….GRANULARITY CONTRASTIVE LEARNING FRAMEWORK FOR EEG BASED EMOTION RECOGNITION
    This study introduces a novel Supervised Info-enhanced Contrastive Learning framework for EEG based Emotion Recognition (SI-CLEER). SI-CLEER employs multi-granularity contrastive learning to create robust EEG contextual representations, potentially improving emotion recognition effectiveness. Unlike existing methods solely guided by classification loss, we propose a joint learning model combining self-supervised contrastive learning loss and supervised classification loss. This model optimizes both loss functions, capturing subtle …
    [[_paper_]](https://ieeexplore.ieee.org/abstract/document/10447740/)

* [9] H Ma, J Wang, H Lin, B Zhang….distillation for Multimodal Emotion Recognition in Conversations
    Emotion recognition in conversations (ERC), the task of recognizing the emotion of each utterance in a conversation, is crucial for building empathetic machines. Existing studies focus mainly on capturing context-and speaker-sensitive dependencies on the textual modality but ignore the significance of multimodal information. Different from emotion recognition in textual conversations, capturing intra-and inter-modal interactions between utterances, learning weights between different modalities, and enhancing modal …
    [[_paper_]](https://ieeexplore.ieee.org/abstract/document/10109845/)

* [10] K Ali, CE Hughes.based Network for Multimodal Emotion Recognition
    The development of transformer-based models has resulted in significant advances in addressing various vision and NLP-based research challenges. However, the progress made in transformer-based methods has not been effectively applied to biosensing research. This paper presents a novel Unified Biosensor-Vision Multi-modal Transformer-based (UBVMT) method to classify emotions in an arousal-valence space by combining a 2D representation of an ECG/PPG signal with the face information. To achieve this goal, we …
    [[_paper_]](https://arxiv.org/abs/2308.14160)

* [11] J Vazquez-Rodriguez, G Lefebvre….CONTINUOUS MULTIMODAL EMOTION RECOGNITION
    Decades of research indicate that emotion recognition is more effective when drawing information from multiple modalities. But what if some modalities are sometimes missing? To address this problem, we propose a novel Transformer-based architecture for recognizing valence and arousal in a time-continuous manner even with missing input modalities. We use a coupling of cross-attention and self-attention mechanisms to emphasize relationships between modalities during time and enhance the learning process on weak salient inputs …
    [[_paper_]](https://ieeexplore.ieee.org/abstract/document/10388079/)

* [12] S Poria, D Hazarika, N Majumder, G Naik….Party Dataset for Emotion Recognition in Conversations
    Emotion recognition in conversations is a challenging task that has recently gained popularity due to its potential applications. Until now, however, a large-scale multimodal multi-party emotional conversational database containing more than two speakers per dialogue was missing. Thus, we propose the Multimodal EmotionLines Dataset (MELD), an extension and enhancement of EmotionLines. MELD contains about 13,000 utterances from 1,433 dialogues from the TV-series Friends. Each utterance is annotated with emotion and …
    [[_paper_]](https://arxiv.org/abs/1810.02508)

* [13] JA Miranda-Correa, MK Abadi…. A Dataset for Affect, Personality and Mood Research on Individuals and Groups
    We present AMIGOS-A dataset for Multimodal research of affect, personality traits and mood on Individuals and GrOupS. Different to other databases, we elicited affect using both short and long videos in two social contexts, one with individual viewers and one with groups of viewers. The database allows the multimodal study of the affective responses, by means of neuro-physiological signals of individuals in relation to their personality and mood, and with respect to the social context and videos' duration. The data is collected in two experimental …
    [[_paper_]](https://ieeexplore.ieee.org/abstract/document/8554112/)

* [14] NK Devulapally, S Anand, SD Bhattacharjee…. Adaptive Multimodal Analysis for Speaker Emotion Recognition in Group Conversations
    Analyzing individual emotions during group conversation is crucial in developing intelligent agents capable of natural human-machine interaction. While reliable emotion recognition techniques depend on different modalities (text, audio, video), the inherent heterogeneity between these modalities and the dynamic cross-modal interactions influenced by an individual's unique behavioral patterns make the task of emotion recognition very challenging. This difficulty is compounded in group settings, where the emotion and its …
    [[_paper_]](https://arxiv.org/abs/2401.15164)

* [15] Not found.Visual_Emotion_Recognition_With_Missing_Labels_And_Missing_Modalities
    Not found
    [[_paper_]](Not found)

* [16] Not found.Distillation_for_Multimodal_Emotion_Recognition_in_Conversations
    Not found
    [[_paper_]](Not found)

* [17] A Jia, Y He, Y Zhang, S Uprety, D Song….Modal Dataset for Human Desire Understanding
    Desire is a strong wish to do or have something, which involves not only a linguistic expression, but also underlying cognitive phenomena driving human feelings. As the most primitive and basic human instinct, conscious desire is often accompanied by a range of emotional responses. As a strikingly understudied task, it is difficult for machines to model and understand desire due to the unavailability of benchmarking datasets with desire and emotion labels. To bridge this gap, we present MSED, the first multi-modal and multi-task …
    [[_paper_]](https://aclanthology.org/2022.naacl-main.108/)

* [18] X Zhang, Y Li.Modality Context Fusion and Semantic Refinement Network for Emotion Recognition in Conversation
    Emotion recognition in conversation (ERC) has attracted enormous attention for its applications in empathetic dialogue systems. However, most previous researches simply concatenate multimodal representations, leading to an accumulation of redundant information and a limited context interaction between modalities. Furthermore, they only consider simple contextual features ignoring semantic clues, resulting in an insufficient capture of the semantic coherence and consistency in conversations. To address these …
    [[_paper_]](https://aclanthology.org/2023.acl-long.732/)

* [19] A Joshi, A Bhat, A Jain, A Singh…. COntextualized GNN based Multimodal Emotion recognitioN
    Emotions are an inherent part of human interactions, and consequently, it is imperative to develop AI systems that understand and recognize human emotions. During a conversation involving various people, a person's emotions are influenced by the other speaker's utterances and their own emotional state over the utterances. In this paper, we propose COntextualized Graph Neural Network based Multi-modal Emotion recognitioN (COGMEN) system that leverages local information (ie, inter/intra dependency between speakers) and …
    [[_paper_]](https://aclanthology.org/2022.naacl-main.306/)

* [20] Not found.Aware_Graph_Convolutional_Network_for_Multimodal_Emotion_Recognition
    Not found
    [[_paper_]](Not found)

* [21] WY Choi, KY Song, CW Lee.Convolutional Attention Networks for Multimodal Emotion Recognition from Speech and Text Data
    Emotion recognition has become a popular topic of interest, especially in the field of human computer interaction. Previous works involve unimodal analysis of emotion, while recent efforts focus on multimodal emotion recognition from vision and speech. In this paper, we propose a new method of learning about the hidden representations between just speech and text data using convolutional attention networks. Compared to the shallow model which employs simple concatenation of feature vectors, the proposed attention model performs …
    [[_paper_]](https://aclanthology.org/W18-3304/)

* [22] Y Wang, Y Li, P Bell, C Lai - researchgate.net.Aware Dynamic Hierarchical Fusion for Multimodal Affect Recognition
    Fusing multiple modalities for affective computing tasks has proven effective for performance improvement. However, how multimodal fusion works is not well understood, and its use in the real world usually results in large model sizes. In this work, on sentiment and emotion analysis, we first analyze how the salient affective information in one modality can be affected by the other in crossmodal attention. We find that inter-modal incongruity exists at the latent level due to crossmodal attention. Based on this finding, we propose a lightweight …
    [[_paper_]](https://www.researchgate.net/profile/Yuanchao-Li-5/publication/370981780_Cross-Attention_is_Not_Enough_Incongruity-Aware_Multimodal_Sentiment_Analysis_and_Emotion_Recognition/links/64a48b888de7ed28ba74a8aa/Cross-Attention-is-Not-Enough-Incongruity-Aware-Multimodal-Sentiment-Analysis-and-Emotion-Recognition.pdf)

* [23] SAM Zaidi, S Latif, J Qadi.Language Speech Emotion Recognition Using Multimodal Dual Attention Transformers
    Despite the recent progress in speech emotion recognition (SER), state-of-the-art systems are unable to achieve improved performance in cross-language settings. In this paper, we propose a Multimodal Dual Attention Transformer (MDAT) model to improve cross-language SER. Our model utilises pre-trained models for multimodal feature extraction and is equipped with a dual attention mechanism including graph attention and co-attention to capture complex dependencies across different modalities and achieve improved cross …
    [[_paper_]](https://arxiv.org/abs/2306.13804)

* [24] Y Zhang, H Liu, D Wang, D Zhang, T Lou….based multimodal emotion recognition
    Objective. The study of emotion recognition through electroencephalography (EEG) has garnered significant attention recently. Integrating EEG with other peripheral physiological signals may greatly enhance performance in emotion recognition. Nonetheless, existing approaches still suffer from two predominant challenges: modality heterogeneity, stemming from the diverse mechanisms across modalities, and fusion credibility, which arises when one or multiple modalities fail to provide highly credible signals. Approach. In this paper, we …
    [[_paper_]](https://iopscience.iop.org/article/10.1088/1741-2552/ad3987/meta)

* [25] L Xiao, X Wu, S Yang, J Xu, J Zhou, L He.based sentiment analysis
    Abstract Multi-modal Aspect-based Sentiment Analysis (MABSA) aims to forecast the polarity of sentiment concerning aspects within a given sentence based on the correlation between the sentence and its accompanying image. Comprehending multi-modal sentiment expression requires strong cross-modal alignment and fusion ability. Previous state-of-the-art (SOTA) models fail to explicitly align valuable visual clues with aspect and sentiment information in textual representations and overlook the utilization of syntactic dependency …
    [[_paper_]](https://www.sciencedirect.com/science/article/pii/S0306457323002455)

* [26] Y Li, Y Wang, Z Cui.Decoupled Multimodal Distilling for Emotion Recognition
    Human multimodal emotion recognition (MER) aims to perceive human emotions via language, visual and acoustic modalities. Despite the impressive performance of previous MER approaches, the inherent multimodal heterogeneities still haunt and the contribution of different modalities varies significantly. In this work, we mitigate this issue by proposing a decoupled multimodal distillation (DMD) approach that facilitates flexible and adaptive crossmodal knowledge distillation, aiming to enhance the discriminative features of each …
    [[_paper_]](http://openaccess.thecvf.com/content/CVPR2023/html/Li_Decoupled_Multimodal_Distilling_for_Emotion_Recognition_CVPR_2023_paper.html)

* [27] F Zhang, XC Li, CP Lim, Q Hua, CR Dong, JH Zhai.Deep Emotional Arousal Network for Multimodal Sentiment Analysis and Emotion Recognition 
    Multimodal sentiment analysis and emotion recognition has become an increasingly popular research area, where the biggest challenge is to efficiently fuse the input information from different modality. The recent success is largely credited to the attention-based models, eg, transformer and its variants. However, the attention-based mechanism often neglects the coherency of human emotion due to its parallel structure. Inspired by the emotional arousal model in cognitive science, a Deep Emotional Arousal Network (DEAN) that is capable of …
    [[_paper_]](https://www.sciencedirect.com/science/article/pii/S1566253522000653)

* [28] AI Middya, B Nag, S Roy.level fusion of audio–visual modalities
    Emotion identification based on multimodal data (eg, audio, video, text, etc.) is one of the most demanding and important research fields, with various uses. In this context, this research work has conducted a rigorous exploration of model-level fusion to find out the optimal multimodal model for emotion recognition using audio and video modalities. More specifically, separate novel feature extractor networks for audio and video data are proposed. After that, an optimal multimodal emotion recognition model is created by fusing …
    [[_paper_]](https://www.sciencedirect.com/science/article/pii/S0950705122002593)

* [29] G Arora, M Sabharwal, P Kapila….Deep Residual Adaptive Neural Network Based Feature Extraction for Cognitive Computing with Multimodal Sentiment Sensing and Emotion Recognition Process
    For the healthcare framework, automatic recognition of patients' emotions is considered to be a good facilitator. Feedback about the status of patients and satisfaction levels can be provided automatically to the stakeholders of the healthcare industry. Multimodal sentiment analysis of human is considered as the attractive and hot topic of research in artificial intelligence (AI) and is the much finer classification issue which differs from other classification issues. In cognitive science, as emotional processing procedure has inspired …
    [[_paper_]](https://search.proquest.com/openview/5b82ac981d0318b3a57fe3bd2e737a49/1?pq-origsite=gscholar&cbl=52057)

* [30] Y Mao, Q Sun, G Liu, X Wang, W Gao, X Li….Modal Emotion Dynamics in Conversations
    … information in a conversation, however, ignoring the differentiated emotional behaviors within 
… tiated multi-modal emotional behaviors can produce more accurate emotional predictions. …
    [[_paper_]](https://arxiv.org/abs/2010.07637)

* [31] Y Guo, C Tang, H Wu, B Chen.EEG EMOTION RECOGNITION BASED ON DYNAMICAL GRAPH ATTENTION NETWORK
    Emotion recognition based on electroencephalography (EEG) signals is one of the current research challenges in this field. In order to learn the optimal graph structure information for each subject, we propose a dynamic graph attention neural network model. The model utilizes a graph attention neural network as a feature learner, dynamically learning channel connections, and enriching feature representations between channels through global attention. To verify the effectiveness of the proposed method, we conducted experiments on …
    [[_paper_]](https://ieeexplore.ieee.org/abstract/document/10447925/)

* [32] GV Singh, M Firdaus, A Ekbal…. A Multimodal Transformer for Identifying Emotions and Intents in Social Conversations
    In the natural language processing community, open-domain conversational agents, also known as chatbots, are gaining popularity. One of the difficulties is getting them to communicate in an emotionally intelligent manner. To generate dialogues, current neural response generation methods depend solely on end-to-end learning from large scale conversation data. Therefore, we introduce a large-scale multi Emotion and Intent guided Multimodal Dialogue (EmoInt-MD) dataset labelled with 32 emotions and 15 empathetic …
    [[_paper_]](https://ieeexplore.ieee.org/abstract/document/9961847/)

* [33] K Ezzameli, H Mahersia. A review
    The omnipresence of numerous information sources () in our () daily lives () brings up new alternatives () for emotion recognition in several domains including e-health, e-learning, robotics, and e-commerce. Due to the variety of data,() the research area of multimodal machine learning poses special problems for computer scientists; how did the field of emotion9 recognition progress in each modality and what are the most common strategies for recognizing emotions? What part does deep () learning play in this? What's …
    [[_paper_]](https://www.sciencedirect.com/science/article/pii/S156625352300163X)

* [34] J Vazquez-Rodriguez, G Lefebvre….Trained Transformers Using Multimodal Signals
    In this paper, we address the problem of multimodal emotion recognition from multiple physiological signals. We demonstrate that a Transformer-based approach is suitable for this task. In addition, we present how such models may be pre-trained in a multimodal scenario to improve emotion recognition performances. We evaluate the benefits of using multimodal inputs and pre-training with our approach on a state-of-the-art dataset.
    [[_paper_]](https://ieeexplore.ieee.org/abstract/document/9953852/)

* [35] D Andreoletti, F Cardoso, A Arzillo, L Luceri… - 2021 - ceur-ws.org.Emotion_Recognition_and_Detection_Methods_A_Compre
    The paradigm of Smart Cities is based on the idea of enhancing citizens’ life by means of 
digital technologies. The widespread generation of data is seen as the main enabling factor of …
    [[_paper_]](https://ceur-ws.org/Vol-3116/Paper_2.pdf)

* [36] Not found.Emotion_Recognition_With_Multimodal_Transformer_Fusion_Framework_Based_on_Acoustic_and_Lexical_Information
    Not found
    [[_paper_]](Not found)

* [37] MM Islam, S Nooruddin, F Karray….level fusion approach
    Deep learning techniques have drawn considerable interest in emotion recognition due to recent technological developments in healthcare analytics. Automatic patient emotion recognition can assist healthcare analytics by providing feedback to the stakeholders of competent healthcare about the conditions of the patients and their satisfaction levels. In this paper, we propose a novel model-level fusion technique based on deep learning for enhanced emotion recognition from multimodal signals to monitor patients in connected …
    [[_paper_]](https://www.sciencedirect.com/science/article/pii/S1746809424002994)

* [38] H Zuo, R Liu, J Zhao, G Gao, H Li.INVARIANT FEATURE FOR ROBUST MULTIMODAL EMOTION RECOGNITION WITH MISSING MODALITIES
    Multimodal emotion recognition leverages complementary information across modalities to gain performance. However, we cannot guarantee that the data of all modalities are always present in practice. In the studies to predict the missing data across modalities, the inherent difference between heterogeneous modalities, namely the modality gap, presents a challenge. To address this, we propose to use invariant features for a missing modality imagination network (IF-MMIN) which includes two novel mechanisms: 1) an invariant …
    [[_paper_]](https://ieeexplore.ieee.org/abstract/document/10095836/)

* [39] Not found.Invariant_Feature_for_Robust_Multimodal_Emotion_Recognition_with_Missing_Modalities
    Not found
    [[_paper_]](Not found)

* [40] Z Fang, A He, Q Yu, B Gao, W Ding, T Zhang…. A novel multimodal emotion recognition approach integrating face, body and text
    Multimodal emotion analysis performed better in emotion recognition depending on more comprehensive emotional clues and multimodal emotion dataset. In this paper, we developed a large multimodal emotion dataset, named" HED" dataset, to facilitate the emotion recognition task, and accordingly propose a multimodal emotion recognition method. To promote recognition accuracy," Feature After Feature" framework was used to explore crucial emotional information from the aligned face, body and text samples. We …
    [[_paper_]](https://arxiv.org/abs/2211.15425)

* [41] X Yang, S Feng, D Wang, S Qi, W Wu, Y Zhang….Sentiment Analysis Based on Generative Multimodal Prompt
    We have witnessed the rapid proliferation of multimodal data on numerous social media platforms. Conventional studies typically require massive labeled data to train models for Multimodal Aspect-Based Sentiment Analysis (MABSA). However, collecting and annotating fine-grained multimodal data for MABSA is tough. To alleviate the above issue, we perform three MABSA-related tasks with quite a small number of labeled multimodal samples. We first build diverse and comprehensive multimodal few-shot datasets according to the data …
    [[_paper_]](https://arxiv.org/abs/2305.10169)

* [42] YP Ruan, S Han, T Li, Y Wu.SPECIFIC REPRESENTATIONS AND DECISIONS FOR MULTIMODAL EMOTION RECOGNITION
    Multimodal emotion recognition (MER) is important for building humanoid chatbots and has gained increasing attention in recent years. Existing studies have proven that extracting better modality-specific representations, which keep both commonality and individuality information of different modalities, is important for the MER task. However, all these works are restricted in making final predictions based on fusing modality-specific representations, and the effectiveness of the modality-specific decisions has not been studied. In this paper …
    [[_paper_]](https://ieeexplore.ieee.org/abstract/document/10447035/)

* [43] M Jin, J Li. Learning Deep Representations for Multimodal Emotion Recognition
    Multimodal emotion recognition based on electroencephalogram (EEG) and compensating physiological signals (eg, eye tracking) has shown potential in the diagnosis and rehabilitation tracking of depression. Since the multi-channel EEG signals are generally processed as one-dimensional (1-D) graph-like features, existing approaches can only adopt underdeveloped shallow models to recognize emotions. However, these simple models have difficulty decoupling complex emotion patterns due to their limited …
    [[_paper_]](https://dl.acm.org/doi/abs/10.1145/3581783.3612074)

* [44] J Li, X Wang, G Lv, Z Zeng. A graph network based multimodal fusion technique for emotion recognition in conversation
    Multimodal machine learning is an emerging area of research, which has received a great deal of scholarly attention in recent years. Up to now, there are few studies on multimodal Emotion Recognition in Conversation (ERC). Since Graph Neural Networks (GNNs) possess the powerful capacity of relational modeling, they have an inherent advantage in the field of multimodal learning. GNNs leverage the graph constructed from multimodal data to perform intra-and inter-modal information interaction, which effectively facilitates the integration and …
    [[_paper_]](https://www.sciencedirect.com/science/article/pii/S0925231223005507)

* [45] M Sun, X Zhang, J Ma, S Xie, Y Liu….modal Rumor Detection
    Rumor spreaders are increasingly utilizing multimedia content to attract the attention and trust of news consumers. Though quite a few rumor detection models have exploited the multi-modal data, they seldom consider the inconsistent semantics between images and texts, and rarely spot the inconsistency among the post contents and background knowledge. In addition, they commonly assume the completeness of multiple modalities and thus are incapable of handling handle missing modalities in real-life scenarios. Motivated by …
    [[_paper_]](https://ieeexplore.ieee.org/abstract/document/10123962/)

* [46] Not found.Interspeech19_Learning Alignment for Multimodal Emotion Recognition from Speech
    Not found
    [[_paper_]](Not found)

* [47] C Ziems, W Held, O Shaikh, J Chen, Z Zhang….Is GPT a Computational Model of Emotion_ 
    … Plutchik’s model is one of the three most recognized discrete emotion … we include GPT-4 
(OpenAI 2023), which is a multimodal model that, at 1.7 trillion parameters, scales up the GPT-3 …
    [[_paper_]](https://direct.mit.edu/coli/article/doi/10.1162/coli_a_00502/118498)

* [48] D Li, Y Wang, K Funakoshi, M Okumura. Joint Modality Fusion and Graph Contrastive Learning for Multimodal Emotion Recognition
    Multimodal emotion recognition aims to recognize emotions for each utterance of multiple modalities, which has received increasing attention for its application in human-machine interaction. Current graph-based methods fail to simultaneously depict global contextual features and local diverse uni-modal features in a dialogue. Furthermore, with the number of graph layers increasing, they easily fall into over-smoothing. In this paper, we propose a method for joint modality fusion and graph contrastive learning for multimodal emotion …
    [[_paper_]](https://arxiv.org/abs/2311.11009)

* [49] J Sun, S Han, YP Ruan, X Zhang….modal Emotion Recognition
    Multi-modal emotion recognition has gained increasing attention in recent years due to its widespread applications and the advances in multi-modal learning approaches. However, previous studies primarily focus on developing models that exploit the unification of multiple modalities. In this paper, we propose that maintaining modality independence is beneficial for the model performance. According to this principle, we construct a dataset, and devise a multi-modal transformer model. The new dataset, CHinese Emotion Recognition dataset …
    [[_paper_]](https://aclanthology.org/2023.acl-long.39/)

* [50] Z Tang, Q Xiao, X Zhou, Y Li, C Chen, K Li.relation representations for multimodal sentiment analysis
    Modality representation learning is a critical issue in multimodal sentiment analysis (MSA). A good sentiment representation should contain as much effective information as possible while being discriminative enough to be better recognized. Previous attention-based MSA methods mainly rely on word-level feature interactions to capture intra-modality and inter-modality relations, which may lead to the loss of essential sentiment information. Furthermore, they primarily focus on information fusion but do not give enough importance to …
    [[_paper_]](https://www.sciencedirect.com/science/article/pii/S0020025523007107)

* [51] P Wang, S Zeng, J Chen, L Fan, M Chen, Y Wu….Leveraging Label Information for Multimodal Emotion Recognition
    Multimodal emotion recognition (MER) aims to detect the emotional status of a given expression by combining the speech and text information. Intuitively, label information should be capable of helping the model locate the salient tokens/frames relevant to the specific emotion, which finally facilitates the MER task. Inspired by this, we propose a novel approach for MER by leveraging label information. Specifically, we first obtain the representative label embeddings for both text and speech modalities, then learn the label …
    [[_paper_]](https://arxiv.org/abs/2309.02106)

* [52] V Chudasama, P Kar, A Gudmalwar….modal Fusion Network for Emotion Recognition in Conversation
    Abstract Emotion Recognition in Conversations (ERC) is crucial in developing sympathetic human-machine interaction. In conversational videos, emotion can be present in multiple modalities, ie, audio, video, and transcript. However, due to the inherent characteristics of these modalities, multi-modal ERC has always been considered a challenging undertaking. Existing ERC research focuses mainly on using text information in a discussion, ignoring the other two modalities. We anticipate that emotion recognition accuracy can be improved by …
    [[_paper_]](http://openaccess.thecvf.com/content/CVPR2022W/MULA/html/Chudasama_M2FNet_Multi-Modal_Fusion_Network_for_Emotion_Recognition_in_Conversation_CVPRW_2022_paper.html)

* [53] N Wang, H Cao, J Zhao, R Chen….Modality Robust Emotion Recognition Framework With Iterative Data Augmentation
    This article deals with the utterance-level modalities missing problem with uncertain patterns on emotion recognition in conversation (ERC) task. Present models generally predict the speaker's emotions by its current utterance and context, which is degraded by modality missing considerably. Our work proposes a framework missing-modality robust emotion recognition (M2R2), which trains emotion recognition model with iterative data augmentation by learned common representation. First, a network called party attentive network (PANet) is …
    [[_paper_]](https://ieeexplore.ieee.org/abstract/document/9868120/)

* [54] Y Zhang, A Jia, B Wang, P Zhang, D Zhao, P Li….task Interactive Graph Attention Network for Conversational Sentiment Analysis and Emotion Recognition
    Sentiment and emotion, which correspond to long-term and short-lived human feelings, are closely linked to each other, leading to the fact that sentiment analysis and emotion recognition are also two interdependent tasks in natural language processing (NLP). One task often leverages the shared knowledge from another task and performs better when solved in a joint learning paradigm. Conversational context dependency, multi-modal interaction, and multi-task correlation are three key factors that contribute to this joint …
    [[_paper_]](https://dl.acm.org/doi/abs/10.1145/3593583)

* [55] M Ren, X Huang, J Liu, M Liu, X Li…. Multimodal Adversarial Learning Network for Conversational Emotion Recognition
    Multimodal emotion recognition in conversations (ERC) aims to identify the emotional state of constituent utterances expressed by multiple speakers in dialogue from multimodal data. Existing multimodal ERC approaches focus on modeling the global context of the dialogue and neglect to mine the characteristic information from the corresponding utterances expressed by the same speaker. Additionally, information from different modalities exhibits commonality and diversity for emotional expression. The commonality and diversity of …
    [[_paper_]](https://ieeexplore.ieee.org/abstract/document/10121331/)

* [56] M Firdaus, H Chauhan, A Ekbal….Label Emotion, Intensity and Sentiment Dialogue Dataset for Emotion Recognition and Sentiment Analysis in Conversations
    Emotion and sentiment classification in dialogues is a challenging task that has gained popularity in recent times. Humans tend to have multiple emotions with varying intensities while expressing their thoughts and feelings. Emotions in an utterance of dialogue can either be independent or dependent on the previous utterances, thus making the task complex and interesting. Multi-label emotion detection in conversations is a significant task that provides the ability to the system to understand the various emotions of the users …
    [[_paper_]](https://aclanthology.org/2020.coling-main.393/)

* [57] Z Lian, H Sun, L Sun, Z Wen, S Zhang, S Chen….Vocabulary Multimodal Emotion Recognition
    Multimodal emotion recognition is an important research topic in artificial intelligence. Over the past few decades, researchers have made remarkable progress by increasing dataset size and building more effective architectures. However, due to various reasons (such as complex environments and inaccurate labels), current systems still cannot meet the demands of practical applications. Therefore, we plan to organize a series of challenges around emotion recognition to further promote the development of this field. Last year, we …
    [[_paper_]](https://arxiv.org/abs/2404.17113)

* [58] J Zhao, R Li, Q Jin.Missing Modality Imagination Network for Emotion Recognition with Uncertain Missing Modalities
    Multimodal fusion has been proved to improve emotion recognition performance in previous works. However, in real-world applications, we often encounter the problem of missing modality, and which modalities will be missing is uncertain. It makes the fixed multimodal fusion fail in such cases. In this work, we propose a unified model, Missing Modality Imagination Network (MMIN), to deal with the uncertain missing modality problem. MMIN learns robust joint multimodal representations, which can predict the representation of any …
    [[_paper_]](https://aclanthology.org/2021.acl-long.203/)

* [59] X Chen.BASED NETWORK FOR MULTIMODAL EMOTION RECOGNITION
    Human emotion is usually expressed in multiple modalities, like audio and text. Multimodal methods can boost Emotion Recognition. However, the relationship between audio and text, and their roles in emotion expression have not been fully studied, and hence hinder Multimodal Emotion Recognition (MER). In this work, taking into consideration of the above two things, we propose two rules for MER, which are Rule 1: The audio module should be more expressive than the text module, and Rule 2: The single-modality emotional …
    [[_paper_]](https://ieeexplore.ieee.org/abstract/document/10447930/)

* [60] J Zheng, S Zhang, Z Wang, X Wang….Head Attention for Multimodal Emotion Recognition
    Multimodal Emotion Recognition is challenging because of the heterogeneity gap among different modalities. Due to the powerful ability of feature abstraction, Deep Neural Networks (DNNs) have exhibited significant success in bridging the heterogeneity gap in cross-modal retrieval and generation tasks. In this work, a DNNs-based Multi-channel Weight-sharing Autoencoder with Cascade Multi-head Attention (MCWSA-CMHA) is proposed to generically address the affective heterogeneity gap in MER. Specifically, multimodal heterogeneity …
    [[_paper_]](https://ieeexplore.ieee.org/abstract/document/9693238/)

* [61] T Shi, SL Huang.Aware Multimodal Fusion Framework for Emotion Recognition in Conversations
    Abstract Emotion Recognition in Conversations (ERC) is an increasingly popular task in the Natural Language Processing community, which seeks to achieve accurate emotion classifications of utterances expressed by speakers during a conversation. Most existing approaches focus on modeling speaker and contextual information based on the textual modality, while the complementarity of multimodal information has not been well leveraged, few current methods have sufficiently captured the complex correlations and mapping …
    [[_paper_]](https://aclanthology.org/2023.acl-long.824/)

* [62] Q Li, Y Gao, C Wang, Y Deng, J Xue….Multilevel_Transformer_for_Multimodal_Emotion_Recognition
    Speech emotion recognition (SER) systems aim to recognize human emotional state during 
human-computer interaction. Most existing SER systems are trained based on utterance-…
    [[_paper_]](https://ieeexplore.ieee.org/abstract/document/10446812/)

* [63] G Aguilar, V Rozgić, W Wang, C Wang.view Models for Emotion Recognition
    Studies on emotion recognition (ER) show that combining lexical and acoustic information results in more robust and accurate models. The majority of the studies focus on settings where both modalities are available in training and evaluation. However, in practice, this is not always the case; getting ASR output may represent a bottleneck in a deployment pipeline due to computational complexity or privacy-related constraints. To address this challenge, we study the problem of efficiently combining acoustic and lexical modalities …
    [[_paper_]](https://arxiv.org/abs/1906.10198)

* [64] J Cheng, I Fostiropoulos, B Boehm….Multimodal Phased Transformer for Sentiment Analysis
    Multimodal Transformers achieve superior performance in multimodal learning tasks. However, the quadratic complexity of the self-attention mechanism in Transformers limits their deployment in low-resource devices and makes their inference and training computationally expensive. We propose multimodal Sparse Phased Transformer (SPT) to alleviate the problem of self-attention complexity and memory footprint. SPT uses a sampling function to generate a sparse attention matrix and compress a long sequence to a shorter …
    [[_paper_]](https://aclanthology.org/2021.emnlp-main.189/)

* [65] S Zou, X Huang, X Shen.Multimodal Prompt Transformer with Hybrid Contrastive Learning for Emotion Recognition in Conversation
    Emotion Recognition in Conversation (ERC) plays an important role in driving the development of human-machine interaction. Emotions can exist in multiple modalities, and multimodal ERC mainly faces two problems:(1) the noise problem in the cross-modal information fusion process, and (2) the prediction problem of less sample emotion labels that are semantically similar but different categories. To address these issues and fully utilize the features of each modality, we adopted the following strategies: first, deep emotion cues …
    [[_paper_]](https://dl.acm.org/doi/abs/10.1145/3581783.3611805)

* [66] X Sun, H He, H Tang, K Zeng….Multimodal rough set transformer for sentiment analysis and emotion recognition
    Sentiment analysis and emotion recognition are crucial tasks that utilize multimodal information. Transformer models have shown exceptional performance in multimodal fusion. However, traditional dot product transformers do not tolerate uncertainty inside sentiment analysis and emotion recognition data. In this study, we introduce rough set self-attention and rough set cross-attention mechanisms for multimodal sentiment analysis and emotion recognition. A common concept is established based on granulation relations to extract …
    [[_paper_]](https://ieeexplore.ieee.org/abstract/document/10263177/)

* [67] L Sun, B Liu, J Tao, Z Lian.Multimodal Speech Emotion Recognition using Cross Attnention with Aligned Audio and Text
    … We should also mention that CAN needs the aligned audio and text as input. However, by 
virtue of the cross-attention mechanism, our model does not need alignment information. The …
    [[_paper_]](https://ieeexplore.ieee.org/abstract/document/9414654/)

* [68] F Chen, J Shao, S Zhu…. Rethinking Graph Neural Networks for Emotion Recognition in Conversation
    Complex relationships of high arity across modality and context dimensions is a critical challenge in the Emotion Recognition in Conversation (ERC) task. Yet, previous works tend to encode multimodal and contextual relationships in a loosely-coupled manner, which may harm relationship modelling. Recently, Graph Neural Networks (GNN) which show advantages in capturing data relations, offer a new solution for ERC. However, existing GNN-based ERC models fail to address some general limits of GNNs, including assuming …
    [[_paper_]](http://openaccess.thecvf.com/content/CVPR2023/html/Chen_Multivariate_Multi-Frequency_and_Multimodal_Rethinking_Graph_Neural_Networks_for_Emotion_CVPR_2023_paper.html)

* [69] A Joshi, A Bhat, A Jain, A Singh…. COntextualized GNN based Multimodal Emotion recognitioN
    Emotions are an inherent part of human interactions, and consequently, it is imperative to develop AI systems that understand and recognize human emotions. During a conversation involving various people, a person's emotions are influenced by the other speaker's utterances and their own emotional state over the utterances. In this paper, we propose COntextualized Graph Neural Network based Multi-modal Emotion recognitioN (COGMEN) system that leverages local information (ie, inter/intra dependency between speakers) and …
    [[_paper_]](https://aclanthology.org/2022.naacl-main.306/)

* [70] Z Li, Y Zhou, Y Liu, F Zhu, C Yang….Learning Model for Multimodal Emotion Recognition
    Multimodal emotion recognition for video has gained considerable attention in recent years, in which three modalities (ie, textual, visual and acoustic) are involved. Due to the diverse levels of informational content related to emotion, three modalities typically possess varying degrees of contribution to emotion recognition. More seriously, there might be inconsistencies between the emotion of individual modality and the video. The challenges mentioned above are caused by the inherent uncertainty of emotion. Inspired by the recent …
    [[_paper_]](https://aclanthology.org/2023.findings-acl.772/)

* [71] C Zhang, Y Zhang, B Cheng. A REINFORCEMENT LEARNING FRAMEWORK FOR MULTIMODAL EMOTION RECOGNITION
    Multimodal Emotion Recognition in Conversation (ERC) has gained significant attention due to its wide-ranging applications in diverse areas. However, most previous approaches focused on modeling context at the semantic level, neglecting the context of dependency information at the emotional level. In this paper, we proposed a novel Reinforcement Learning framework for the multimodal EMOtion recognition task (RL-EMO), which combines a Multi-modal Graph Convolution Network (MMGCN)[1] module with a novel Reinforcement …
    [[_paper_]](https://ieeexplore.ieee.org/abstract/document/10446459/)

* [72] H Yang, X Gao, J Wu, T Gan, N Ding….interaction Modeling For Multimodal Emotion Recognition
    The multimodal emotion recognition in conversation task aims to predict the emotion label for a given utterance with its context and multiple modalities. Existing approaches achieve good results but also suffer from the following two limitations: 1) lacking modeling of diverse dependency ranges, ie, long, short, and independent context-specific representations and without consideration of the different recognition difficulty for each utterance; 2) consistent treatment of the contribution for various modalities. To address the above challenges, we …
    [[_paper_]](https://aclanthology.org/2023.findings-acl.390/)

* [73] H Chen, C Guo, Y Li, P Zhang, D Jiang.Labeling
    This paper presents our solution for the Semi-Supervised Multimodal Emotion Recognition Challenge (MER2023-SEMI), addressing the issue of limited annotated data in emotion recognition. Recently, the self-training-based Semi-Supervised Learning~(SSL) method has demonstrated its effectiveness in various tasks, including emotion recognition. However, previous studies focused on reducing the confirmation bias of data without adequately considering the issue of data imbalance, which is of great importance in emotion …
    [[_paper_]](https://dl.acm.org/doi/abs/10.1145/3581783.3612864)

* [74] U Chinta, J Kalita, A Atyabi. Facial Images and EEG
    Emotion recognition is an important factor in social communication and has a wide range of applications from retail to healthcare. In psychology, emotion recognition focuses on emotional states within non-verbal visual and auditory cues. It is essential to the human ability to associate meaning with events rather than treating them as mere facts. Studies of emotion recognition often utilize data gathered in response to non-verbal cues using modalities such as eye tracking, Electroencephalo-gram (EEG), and facial video and build …
    [[_paper_]](https://ieeexplore.ieee.org/abstract/document/10099070/)

* [75] B Yao, W Shi.CENTRIC MULTIMODAL FUSION NETWORKS FOR EMOTION RECOGNITION IN CONVERSATIONS
    Existing emotion recognition methods in conversations (ERC) focus on using different utterances information between speakers to improve emotion recognition performance, but they ignore the differential contributions of different utterances to emotion recognition. In this paper, we propose a speaker-centric multimodal fusion network for ERC, in which bidirectional gated recurrent units (BiGRU) is used for intra-modal feature fusion and graph convolution is used for speaker-centric cross-modal feature fusion. We construct a speaker …
    [[_paper_]](https://ieeexplore.ieee.org/abstract/document/10447720/)

* [76] D Zhang, F Chen, J Chang, X Chen….Modal Emotion Recognition in Conversations 
    Multi-Modal Emotion Recognition in Conversations (MMERC) is an increasingly active research field that leverages multi-modal signals to understand the feelings behind each utterance. Modeling contextual interactions and multi-modal fusion lie at the heart of this field, with graph-based models recently being widely used for MMERC to capture global multi-modal contextual information. However, these models generally mix all modality representations in a single graph, and utterances in each modality are fully connected …
    [[_paper_]](https://ieeexplore.ieee.org/abstract/document/10219015/)

* [77] X Song, L Huang, H Xue, S Hu.Supervised Prototypical Contrastive Learning for Emotion Recognition in Conversation
    Capturing emotions within a conversation plays an essential role in modern dialogue systems. However, the weak correlation between emotions and semantics brings many challenges to emotion recognition in conversation (ERC). Even semantically similar utterances, the emotion may vary drastically depending on contexts or speakers. In this paper, we propose a Supervised Prototypical Contrastive Learning (SPCL) loss for the ERC task. Leveraging the Prototypical Network, the SPCL targets at solving the imbalanced …
    [[_paper_]](https://arxiv.org/abs/2210.08713)

* [78] X Li.channel compounded Cross Attention for Multimodal Emotion Recognition
    Recently, emotion recognition based on physiological signals has emerged as a field with intensive research. The utilization of multi-modal, multi-channel physiological signals has significantly improved the performance of emotion recognition systems, due to their complementarity. However, effectively integrating emotion-related semantic information from different modalities and capturing inter-modal dependencies remains a challenging issue. Many existing multimodal fusion methods ignore either token-to-token or channel-to-channel …
    [[_paper_]](https://arxiv.org/abs/2306.13592)

* [79] Z Zhao, Y Wang, Y Xu, J Zhang.Scale Fusion Network for Multimodal Emotion Recognition
    As deep learning technology research continues to progress, artificial intelligence technology is gradually empowering various fields. To achieve a more natural human-computer interaction experience, how to accurately recognize emotional state of speech interactions has become a new research hotspot. Sequence modeling methods based on deep learning techniques have promoted the development of emotion recognition, but the mainstream methods still suffer from insufficient multimodal information interaction, difficulty …
    [[_paper_]](https://ieeexplore.ieee.org/abstract/document/10254334/)

* [80] L Stappen, A Baird, L Christ, L Schumann….Emotion, and Stress
    Multimodal Sentiment Analysis (MuSe) 2021 is a challenge focusing on the tasks of sentiment and emotion, as well as physiological-emotion and emotion-based stress recognition through more comprehensively integrating the audio-visual, language, and biological signal modalities. The purpose of MuSe 2021 is to bring together communities from different disciplines; mainly, the audio-visual emotion recognition community (signal-based), the sentiment analysis community (symbol-based), and the health informatics …
    [[_paper_]](https://dl.acm.org/doi/abs/10.1145/3475957.3484450)

* [81] S Qiu, N Sekhar, P Singhal.aware Transformer for Multimodal Emotion Recognition
    Understanding emotion expressions in multimodal signals is key for machines to have a better understanding of human communication. While language, visual and acoustic modalities can provide clues from different perspectives, the visual modality is shown to make minimal contribution to the performance in the emotion recognition field due to its high dimensionality. Therefore, we first leverage the strong multimodality backbone VATT to project the visual signal to the common space with language and acoustic signals. Also, we …
    [[_paper_]](https://aclanthology.org/2023.findings-acl.130/)

* [82] G Hu, TE Lin, Y Zhao, G Lu, Y Wu, Y Li. Towards Unified Multimodal Sentiment Analysis and Emotion Recognition
    Multimodal sentiment analysis (MSA) and emotion recognition in conversation (ERC) are key research topics for computers to understand human behaviors. From a psychological perspective, emotions are the expression of affect or feelings during a short period, while sentiments are formed and held for a longer period. However, most existing works study sentiment and emotion separately and do not fully exploit the complementary knowledge behind the two. In this paper, we propose a multimodal sentiment knowledge-sharing …
    [[_paper_]](https://arxiv.org/abs/2211.11256)

* [83] D Sun, Y He, J Han.USING AUXILIARY TASKS IN MULTIMODAL FUSION OF WAV2VEC 2.0 AND BERT FOR MULTIMODAL EMOTION RECOGNITION
    The lack of data and the difficulty of multimodal fusion have always been challenges for multimodal emotion recognition (MER). In this paper, we propose to use pre-trained models as upstream network, wav2vec 2.0 for audio modality and BERT for text modality, and finetune them in downstream task of MER to cope with the lack of data. For the difficulty of multimodal fusion, we use a K-layer multi-head attention mechanism as a downstream fusion module. Starting from the MER task itself, we design two auxiliary tasks to alleviate …
    [[_paper_]](https://ieeexplore.ieee.org/abstract/document/10096586/)

* [84] J Hu, Y Liu, J Zhao, Q Jin. Multimodal Fusion via Deep Graph Convolution Network for Emotion Recognition in Conversation
    Emotion recognition in conversation (ERC) is a crucial component in affective dialogue systems, which helps the system understand users' emotions and generate empathetic responses. However, most works focus on modeling speaker and contextual information primarily on the textual modality or simply leveraging multimodal information through feature concatenation. In order to explore a more effective way of utilizing both multimodal and long-distance contextual information, we propose a new model based on multimodal fused graph …
    [[_paper_]](https://arxiv.org/abs/2107.06779)

* [85] L Stappen, A Baird, L Christ, L Schumann….Emotion, and Stress
    Multimodal Sentiment Analysis (MuSe) 2021 is a challenge focusing on the tasks of sentiment and emotion, as well as physiological-emotion and emotion-based stress recognition through more comprehensively integrating the audio-visual, language, and biological signal modalities. The purpose of MuSe 2021 is to bring together communities from different disciplines; mainly, the audio-visual emotion recognition community (signal-based), the sentiment analysis community (symbol-based), and the health informatics …
    [[_paper_]](https://dl.acm.org/doi/abs/10.1145/3475957.3484450)

#### _MMER_ ####
* [1] P Yang, F Luo, S Ma, J Lin, X Sun.Label Classification
    Multi-label classification (MLC) aims to predict a set of labels for a given instance. Based on a pre-defined label order, the sequence-to-sequence (Seq2Seq) model trained via maximum likelihood estimation method has been successfully applied to the MLC task and shows powerful ability to capture high-order correlations between labels. However, the output labels are essentially an unordered set rather than an ordered sequence. This inconsistency tends to result in some intractable problems, eg, sensitivity to the label order …
    [[_paper_]](https://aclanthology.org/P19-1518/)

* [2] D Zhou, Y Xiang, L Zhang, C Ye….hop Relation Detection in Knowledge Base Question Answering
    Relation detection in knowledge base question answering, aims to identify the path (s) of relations starting from the topic entity node that is linked to the answer node in knowledge graph. Such path might consist of multiple relations, which we call multi-hop. Moreover, for a single question, there may exist multiple relation paths to the correct answer, which we call multi-label. However, most of existing approaches only detect one single path to obtain the answer without considering other correct paths, which might affect the final performance …
    [[_paper_]](https://aclanthology.org/2021.findings-emnlp.412/)

* [3] C Wu, L Cao, Y Ge, Y Liu, M Zhang, J Su.Level Implicit Discourse Relation Recognition
    Implicit discourse relation recognition (IDRR) is a challenging but crucial task in discourse analysis. Most existing methods train multiple models to predict multi-level labels independently, while ignoring the dependence between hierarchically structured labels. In this paper, we consider multi-level IDRR as a conditional label sequence generation task and propose a Label Dependence-aware Sequence Generation Model (LDSGM) for it. Specifically, we first design a label attentive encoder to learn the global representation of an …
    [[_paper_]](https://ojs.aaai.org/index.php/AAAI/article/view/21401)

* [4] R You, Z Guo, L Cui, X Long, Y Bao….Modality Attention with Semantic Graph Embedding for Multi Label Classification
    Multi-label image and video classification are fundamental yet challenging tasks in computer vision. The main challenges lie in capturing spatial or temporal dependencies between labels and discovering the locations of discriminative features for each class. In order to overcome these challenges, we propose to use cross-modality attention with semantic graph embedding for multi-label classification. Based on the constructed label graph, we propose an adjacency-based similarity graph embedding method to learn semantic label …
    [[_paper_]](https://ojs.aaai.org/index.php/AAAI/article/view/6964)

* [5] S Qian, D Xue, H Zhang, Q Fang, C Xu.modal Retrieval
    Cross-modal retrieval has become an active study field with the expanding scale of multimodal data. To date, most existing methods transform multimodal data into a common representation space where semantic similarities between items can be directly measured across different modalities. However, these methods typically suffer from following limitations: 1) They usually attempt to bridge the modality gap by designing losses in the common representation space which may not be sufficient to eliminate potential …
    [[_paper_]](https://ojs.aaai.org/index.php/AAAI/article/view/16345)

* [6] Y Wang, Y Xie, Y Liu, K Zhou, X Li.modal Fusion
    In multi-label image recognition, it has become a popular method to predict those labels that co-occur in an image via modeling the label dependencies. Previous works focus on capturing the correlation between labels, but neglect to effectively fuse the image features and label embeddings, which severely affects the convergence efficiency of the model and inhibits the further precision improvement of multi-label image recognition. To overcome this shortcoming, in this paper, we introduce Multi-modal Factorized Bilinear pooling (MFB) …
    [[_paper_]](https://dl.acm.org/doi/abs/10.1145/3340531.3411880)

* [7] L Qiu, Y Yang, CC Cao, Y Zheng, H Ngai….Distribution Data
    Perturbation-based techniques are promising for explaining black-box machine learning models due to their effectiveness and ease of implementation. However, prior works have faced the problem of Out-of-Distribution (OoD)—an artifact of randomly perturbed data becoming inconsistent with the original dataset, degrading the reliability of generated explanations, which is still under-explored according to our best knowledge. This work addresses the OoD issue by designing a simple yet effective module that can quantify the …
    [[_paper_]](https://dl.acm.org/doi/abs/10.1145/3485447.3512254)

* [8] H Lian, C Lu, S Li, Y Zhao, C Tang, Y Zong….label Learning
    In the task of multimodal emotion recognition with multi-label learning (MER-MULTI), leveraging the correlation between discrete and dimensional emotions is crucial for improving the model's performance. However, there may be a mismatch between the feature distributions of the training set and the testing set, which could result in the trained model's inability to adapt to the correlations between labels in the testing set. Therefore, a significant challenge in MER-MULTI is how to match the feature distributions of the training set and …
    [[_paper_]](https://dl.acm.org/doi/abs/10.1145/3607865.3613183)

* [9] L Xiao, X Huang, B Chen, L Jing.Label Text Classification
    Multi-label text classification (MLTC) aims to tag most relevant labels for the given document. In this paper, we propose a Label-Specific Attention Network (LSAN) to learn a label-specific document representation. LSAN takes advantage of label semantic information to determine the semantic connection between labels and document for constructing label-specific document representation. Meanwhile, the self-attention mechanism is adopted to identify the label-specific document representation from document content information. In order to …
    [[_paper_]](https://aclanthology.org/D19-1044/)

* [10] Q Ma, C Yuan, W Zhou, S Hu.Label Text Classification
    Multi-label text classification is one of the fundamental tasks in natural language processing. Previous studies have difficulties to distinguish similar labels well because they learn the same document representations for different labels, that is they do not explicitly extract label-specific semantic components from documents. Moreover, they do not fully explore the high-order interactions among these semantic components, which is very helpful to predict tail labels. In this paper, we propose a novel label-specific dual graph neural network (LDGN) …
    [[_paper_]](https://aclanthology.org/2021.acl-long.298/)

* [11] S Ge, Z Jiang, Z Cheng, C Wang, Y Yin….Label Emotion Recognition via Adversarial Masking and Perturbation
    Recognizing emotions from multi-modal data is an emotion recognition task that requires strong multi-modal representation ability. The general approach to this task is to naturally train the representation model on training data without intervention. However, such natural training scheme is prone to modality bias of representation (ie, tending to over-encode some informative modalities while neglecting other modalities) and data bias of training (ie, tending to overfit training data). These biases may lead to instability (eg, performing poorly …
    [[_paper_]](https://dl.acm.org/doi/abs/10.1145/3543507.3583258)

* [12] T Chen, M Xu, X Hui, H Wu….Label Image Recognition
    Recognizing multiple labels of images is a practical and challenging task, and significant progress has been made by searching semantic-aware regions and modeling label dependency. However, current methods cannot locate the semantic regions accurately due to the lack of part-level supervision or semantic guidance. Moreover, they cannot fully explore the mutual interactions among the semantic regions and do not explicitly model the label co-occurrence. To address these issues, we propose a Semantic-Specific Graph …
    [[_paper_]](http://openaccess.thecvf.com/content_ICCV_2019/html/Chen_Learning_Semantic-Specific_Graph_Representation_for_Multi-Label_Image_Recognition_ICCV_2019_paper.html)

* [13] J Zhao, T Zhang, J Hu, Y Liu, Q Jin, X Wang….label Emotional Dialogue Database
    The emotional state of a speaker can be influenced by many different factors in dialogues, such as dialogue scene, dialogue topic, and interlocutor stimulus. The currently available data resources to support such multimodal affective analysis in dialogues are however limited in scale and diversity. In this work, we propose a Multi-modal Multi-scene Multi-label Emotional Dialogue dataset, M3ED, which contains 990 dyadic emotional dialogues from 56 different TV series, a total of 9,082 turns and 24,449 utterances. M3 ED is annotated with 7 …
    [[_paper_]](https://arxiv.org/abs/2205.10237)

* [14] J Zhao, Y Zhao, J Li.label Recognition with Transformer
    Multi-label image recognition aims to recognize multiple objects simultaneously in one image. Recent ideas to solve this problem have focused on learning dependencies of label co-occurrences to enhance the high-level semantic representations. However, these methods usually neglect the important relations of intrinsic visual structures and face difficulties in understanding contextual relationships. To build the global scope of visual context as well as interactions between visual modality and linguistic modality, we propose …
    [[_paper_]](https://dl.acm.org/doi/abs/10.1145/3474085.3475191)

* [15] ZM Chen, XS Wei, P Wang….Label Image Recognition with Graph Convolutional Networks∗
    The task of multi-label image recognition is to predict a set of object labels that present in an image. As objects normally co-occur in an image, it is desirable to model the label dependencies to improve the recognition performance. To capture and explore such important dependencies, we propose a multi-label classification model based on Graph Convolutional Network (GCN). The model builds a directed graph over the object labels, where each node (label) is represented by word embeddings of a label, and GCN is learned …
    [[_paper_]](http://openaccess.thecvf.com/content_CVPR_2019/html/Chen_Multi-Label_Image_Recognition_With_Graph_Convolutional_Networks_CVPR_2019_paper.html)

* [16] HD Le, GS Lee, SH Kim, S Kim, HJ Yang.Level Representation Learning
    Emotion recognition has been an active research area for a long time. Recently, multimodal emotion recognition from video data has grown in importance with the explosion of video content due to the emergence of short video social media platforms. Effectively incorporating information from multiple modalities in video data to learn robust multimodal representation for improving recognition model performance is still the primary challenge for researchers. In this context, transformer architectures have been widely used and have significantly …
    [[_paper_]](https://ieeexplore.ieee.org/abstract/document/10042438/)

* [17] D Zhang, X Ju, J Li, S Li, Q Zhu….label Emotion Detection with Modality and Label Dependence
    As an important research issue in the natural language processing community, multi-label emotion detection has been drawing more and more attention in the last few years. However, almost all existing studies focus on one modality (eg, textual modality). In this paper, we focus on multi-label emotion detection in a multi-modal scenario. In this scenario, we need to consider both the dependence among different labels (label dependence) and the dependence between each predicting label and different modalities (modality …
    [[_paper_]](https://aclanthology.org/2020.emnlp-main.291/)

* [18] D Zhang, X Ju, W Zhang, J Li, S Li, Q Zhu….label Emotion Recognition with Heterogeneous Hierarchical Message Passing
    As an important research issue in affective computing community, multi-modal emotion recognition has become a hot topic in the last few years. However, almost all existing studies perform multiple binary classification for each emotion with focus on complete time series data. In this paper, we focus on multi-modal emotion recognition in a multi-label scenario. In this scenario, we consider not only the label-to-label dependency, but also the feature-to-label and modality-to-label dependencies. Particularly, we propose a heterogeneous …
    [[_paper_]](https://ojs.aaai.org/index.php/AAAI/article/view/17686)

* [19] A Illendula, A Sheth.Multimodal Emotion Classification
    … to study the usage of emojis in different emotional contexts is described in Section 3. We 
present our model and approach of multimodal emotional classification in Section 4. The results …
    [[_paper_]](https://dl.acm.org/doi/abs/10.1145/3308560.3316549)

* [20] M Wołczyk, M Proszewska, Ł Maziarka….trained Models
    Modern generative models achieve excellent quality in a variety of tasks including image or text generation and chemical molecule modeling. However, existing methods often lack the essential ability to generate examples with requested properties, such as the age of the person in the photo or the weight of the generated molecule. Incorporating such additional conditioning factors would require rebuilding the entire architecture and optimizing the parameters from scratch. Moreover, it is difficult to disentangle selected attributes so that to …
    [[_paper_]](https://ojs.aaai.org/index.php/AAAI/article/view/20843)

* [21] K Zhu, J Wu.Label Recognition
    Multi-label image recognition is a challenging computer vision task of practical use. Progresses in this area, however, are often characterized by complicated methods, heavy computations, and lack of intuitive explanations. To effectively capture different spatial regions occupied by objects from different categories, we propose an embarrassingly simple module, named class-specific residual attention (CSRA). CSRA generates class-specific features for every category by proposing a simple spatial attention score, and then combines …
    [[_paper_]](http://openaccess.thecvf.com/content/ICCV2021/html/Zhu_Residual_Attention_A_Simple_but_Effective_Method_for_Multi-Label_Recognition_ICCV_2021_paper.html)

* [22] P Yang, X Sun, W Li, S Ma, W Wu, H Wang.Label Classification
    Multi-label classification is an important yet challenging task in natural language processing. It is more complex than single-label classification in that the labels tend to be correlated. Existing methods tend to ignore the correlations between labels. Besides, different parts of the text can contribute differently for predicting different labels, which is not considered by existing models. In this paper, we propose to view the multi-label classification task as a sequence generation problem, and apply a sequence generation model with a novel …
    [[_paper_]](https://arxiv.org/abs/1806.04822)

* [23] A Ando, R Masumura, H Kamiyama….Label Emotion Existence Model
    This paper presents a novel speech emotion recognition method that addresses the ambiguous nature of emotions in speech. Most conventional methods assume there is only a single ground truth, the dominant emotion, though utterances can contain multiple emotions. In order to solve this problem, several methods that consider ambiguous emotions (eg soft-target training) have been proposed. Unfortunately, training them is difficult since they work by estimating the proportions of all emotions. The proposed method improves both …
    [[_paper_]](https://www.isca-archive.org/interspeech_2019/ando19_interspeech.pdf)

* [24] Y Zhang, M Chen, J Shen, C Wang.Label Emotion Recognition
    Abstract Multi-modal Multi-label Emotion Recognition (MMER) aims to identify various human emotions from heterogeneous visual, audio and text modalities. Previous methods mainly focus on projecting multiple modalities into a common latent space and learning an identical representation for all labels, which neglects the diversity of each modality and fails to capture richer semantic information for each label from different perspectives. Besides, associated relationships of modalities and labels have not been fully exploited. In this paper …
    [[_paper_]](https://ojs.aaai.org/index.php/AAAI/article/view/20895)

* [25] J Shen, W Qiu, Y Meng, J Shang, X Ren….Label Text Classification Using Only Class Names
    Hierarchical multi-label text classification (HMTC) aims to tag each document with a set of classes from a class hierarchy. Most existing HMTC methods train classifiers using massive human-labeled documents, which are often too costly to obtain in real-world applications. In this paper, we explore to conduct HMTC based on only class surface names as supervision signals. We observe that to perform HMTC, human experts typically first pinpoint a few most essential classes for the document as its “core classes”, and then check core classes' …
    [[_paper_]](https://par.nsf.gov/biblio/10311077)

* [26] X Ju, D Zhang, J Li, G Zhou.label Emotion Detection
    Multi-modal utterance-level emotion detection has been a hot research topic in both multi-modal analysis and natural language processing communities. Different from traditional single-label multi-modal sentiment analysis, typical multi-modal emotion detection is naturally a multi-label problem where an utterance often contains multiple emotions. Existing studies normally focus on multi-modal fusion only and transform multi-label emotion classification into multiple binary classification problem independently. As a result, existing …
    [[_paper_]](https://dl.acm.org/doi/abs/10.1145/3394171.3413577)

#### _MSA_ ####
* [1] Not found.20230626
    Not found
    [[_paper_]](Not found)

* [2] S Afzal, HA Khan, IU Khan, MJ Piran….A Comprehensive Survey on Affective Computing_ Challenges, Trends, Applications, and Future Directions
    As the name suggests, affective computing aims to recognize human emotions, sentiments, and feelings. There is a wide range of fields that study affective computing, including languages, sociology, psychology, computer science, and physiology. However, no research has ever been done to determine how machine learning (ML) and mixed reality (XR) interact together. This paper discusses the significance of affective computing, as well as its ideas, conceptions, methods, and outcomes. By using approaches of ML and XR, we …
    [[_paper_]](https://arxiv.org/abs/2305.07665)

* [3] J Peng, T Wu, W Zhang, F Cheng, S Tan, F Yi….stage network for multimodal sentiment analysis
    Sentiment analysis is a challenging but valuable research topic in affective computing. It can improve the quality of various real-world applications, including financial market prediction, disease analysis even politics. As sentiment may be expressed by text, image, audio, video, etc., multimodal sentiment analysis has emerged to capture information in multiple ways. Take video as an example, the analysis process may be difficult since the modalities in the video are heterogeneous and may express different sentiments. To deal with such issues, a …
    [[_paper_]](https://www.sciencedirect.com/science/article/pii/S0957417423002221)

* [4] Y Zhi, J Li, H Wang, J Chen….Modal Interaction Model for Multimodal Sentiment Analysis
    The methods based on multimodal representation learning enhance discriminable sentiment expression for multimodal sentiment analysis (MSA). The modal invariant and specific features serve different purposes in sentiment learning and the diversity of inter-sample and inter-category relationships takes less consideration in previous advances. In this paper, we propose a fine-grained tri-modal interaction model for MSA to refine and enhance the overall affective state at the uni/multi-modal level and label level. Concretely …
    [[_paper_]](https://ieeexplore.ieee.org/abstract/document/10447872/)

* [5] J LIU, H SONG, DP CHEN, B WANG, ZW ZHANG.verbal Information and Contrastive Learning
    Deep learning methods have gained popularity in multimodal sentiment analysis due to their impressive representation and fusion capabilities in recent years. Existing studies often analyze the emotions of individuals using multimodal information such as text, facial expressions, and speech intonation, primarily employing complex fusion methods. However, existing models inadequately consider the dynamic changes in emotions over long time sequences, resulting in suboptimal performance in sentiment analysis. In response to this …
    [[_paper_]](https://jeit.ac.cn/en/article/doi/10.11999/JEIT231274)

* [6] X Sun, X Ren, X Xie.TASK LEARNING
    Sentiment analysis is an important research area in Natural Language Processing (NLP). With the explosion of multimodal data, Multimodal Sentiment Analysis (MSA) attracts more and more attention in recent years. How to Effectively harnessing the interplay between diverse modalities is paramount to achieving comprehensive fusion of MSA. However, current research predominantly emphasizes modality interaction, while overlooking unimodal information, thus neglecting the inherent disparities between modalities. To …
    [[_paper_]](https://ieeexplore.ieee.org/abstract/document/10446040/)

* [7] MM Amin, R Mao, E Cambria, BW Schuller.A Wide Evaluation of ChatGPT on Affective Computing Tasks
    With the rise of foundation models, a new artificial intelligence paradigm has emerged, by simply using general purpose foundation models with prompting to solve problems instead of training a separate machine learning model for each problem. Such models have been shown to have emergent properties of solving problems that they were not initially trained on. The studies for the effectiveness of such models are still quite limited. In this work, we widely study the capabilities of the ChatGPT models, namely GPT-4 and GPT-3.5, on 13 …
    [[_paper_]](https://arxiv.org/abs/2308.13911)

* [8] AAB Zadeh, PP Liang, S Poria, E Cambria….MOSEI Dataset and Interpretable Dynamic Fusion Graph
    Analyzing human multimodal language is an emerging area of research in NLP. Intrinsically this language is multimodal (heterogeneous), sequential and asynchronous; it consists of the language (words), visual (expressions) and acoustic (paralinguistic) modalities all in the form of asynchronous coordinated sequences. From a resource perspective, there is a genuine need for large scale datasets that allow for in-depth studies of this form of language. In this paper we introduce CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU …
    [[_paper_]](https://aclanthology.org/P18-1208/)

* [9] W Yu, H Xu, F Meng, Y Zhu, Y Ma, J Wu….grained Annotations of Modality
    Previous studies in multimodal sentiment analysis have used limited datasets, which only contain unified multimodal annotations. However, the unified annotations do not always reflect the independent sentiment of single modalities and limit the model to capture the difference between modalities. In this paper, we introduce a Chinese single-and multi-modal sentiment analysis dataset, CH-SIMS, which contains 2,281 refined video segments in the wild with both multimodal and independent unimodal annotations. It allows researchers to …
    [[_paper_]](https://aclanthology.org/2020.acl-main.343/)

* [10] K Kim, S Park.One BERT for multimodal sentiment analysis 
    Multimodal sentiment analysis utilizes various modalities such as Text, Vision and Speech to predict sentiment. As these modalities have unique characteristics, methods have been developed for fusing features. However, the overall modality characteristics are not guaranteed, because traditional fusion methods have some loss of intra-modality and inter-modality. To solve this problem, we introduce a single-stream transformer, All-modalities-in-One BERT (AOBERT). The model is pre-trained on two tasks simultaneously: Multimodal …
    [[_paper_]](https://www.sciencedirect.com/science/article/pii/S1566253522002329)

* [11] Not found.Distillation_for_Multimodal_Emotion_Recognition_in_Conversations
    Not found
    [[_paper_]](Not found)

* [12] J Tang, D Liu, X Jin, Y Peng, Q Zhao….Direction Attention Based Fusion Network for Multimodal Sentiment Analysis
    Attention-based networks currently identify their effectiveness in multimodal sentiment analysis. However, existing methods ignore the redundancy of auxiliary modalities. More importantly, existing methods only attend to top-down attention (static process) or down-top attention (implicit process), leading to the coarse-grained multimodal sentiment context. In this paper, during the preprocessing period, we first propose the multimodal dynamic enhanced block to capture the intra-modality sentiment context. This can effectively …
    [[_paper_]](https://ieeexplore.ieee.org/abstract/document/9932611/)

* [13] Z Li, B Xu, C Zhu, T Zhao.Layer Fusion Method for Multimodal Sentiment Detection
    Compared with unimodal data, multimodal data can provide more features to help the model analyze the sentiment of data. Previous research works rarely consider token-level feature fusion, and few works explore learning the common features related to sentiment in multimodal data to help the model fuse multimodal features. In this paper, we propose a Contrastive Learning and Multi-Layer Fusion (CLMLF) method for multimodal sentiment detection. Specifically, we first encode text and image to obtain hidden representations, and …
    [[_paper_]](https://arxiv.org/abs/2204.05515)

* [14] J Yang, Y Yu, D Niu, W Guo, Y Xu. Contrastive Feature Decomposition for Multimodal Sentiment Analysis
    Abstract Multimodal Sentiment Analysis aims to predict the sentiment of video content. Recent research suggests that multimodal sentiment analysis critically depends on learning a good representation of multimodal information, which should contain both modality-invariant representations that are consistent across modalities as well as modality-specific representations. In this paper, we propose ConFEDE, a unified learning framework that jointly performs contrastive representation learning and contrastive feature decomposition to …
    [[_paper_]](https://aclanthology.org/2023.acl-long.421/)

* [15] Y Yu, M Zhao, S Qi, F Sun, B Wang, W Guo…. Contrastive Knowledge Injection for Multimodal Sentiment Analysis
    Multimodal Sentiment Analysis leverages multimodal signals to detect the sentiment of a speaker. Previous approaches concentrate on performing multimodal fusion and representation learning based on general knowledge obtained from pretrained models, which neglects the effect of domain-specific knowledge. In this paper, we propose Contrastive Knowledge Injection (ConKI) for multimodal sentiment analysis, where specific-knowledge representations for each modality can be learned together with general …
    [[_paper_]](https://arxiv.org/abs/2306.15796)

* [16] DS Chauhan, MS Akhtar, A Ekbal….modal Sentiment and Emotion Analysis
    In recent times, multi-modal analysis has been an emerging and highly sought-after field at the intersection of natural language processing, computer vision, and speech processing. The prime objective of such studies is to leverage the diversified information,(eg, textual, acoustic and visual), for learning a model. The effective interaction among these modalities often leads to a better system in terms of performance. In this paper, we introduce a recurrent neural network based approach for the multi-modal sentiment and emotion analysis. The …
    [[_paper_]](https://aclanthology.org/D19-1566/)

* [17] M Huang, C Qing, J Tan, X Xu.Level Sentiment Prediction
    Recently, video sentiment computing has become the focus of research because of its benefits in many applications such as digital marketing, education, healthcare, and so on. The difficulty of video sentiment prediction mainly lies in the regression accuracy of long-term sequences and how to integrate different modalities. In particular, different modalities may express different emotions. In order to maintain the continuity of long time-series sentiments and mitigate the multimodal conflicts, this article proposes a novel Context …
    [[_paper_]](https://ieeexplore.ieee.org/abstract/document/10271721/)

* [18] Y Zeng, W Yan, S Mai, H Hu.Disentanglement Translation Network for multimodal sentiment analysis
    Obtaining an effective joint representation has always been the goal for multimodal tasks. However, distributional gap inevitably exists due to the heterogeneous nature of different modalities, which poses burden on the fusion process and the learning of multimodal representation. The imbalance of modality dominance further aggravates this problem, where inferior modalities may contain much redundancy that introduces additional variations. To address the aforementioned issues, we propose a Disentanglement …
    [[_paper_]](https://www.sciencedirect.com/science/article/pii/S1566253523003470)

* [19] S Mai, H Hu, S Xing. Hierarchical Feature Fusion Network with Local and Global Perspectives for Multimodal Affective Computing
    We propose a general strategy named 'divide, conquer and combine'for multimodal fusion. Instead of directly fusing features at holistic level, we conduct fusion hierarchically so that both local and global interactions are considered for a comprehensive interpretation of multimodal embeddings. In the 'divide'and 'conquer'stages, we conduct local fusion by exploring the interaction of a portion of the aligned feature vectors across various modalities lying within a sliding window, which ensures that each part of multimodal embeddings are …
    [[_paper_]](https://aclanthology.org/P19-1046/)

* [20] Not found.EMNLP21_Multimodal Phased Transformer for Sentiment Analysis
    Not found
    [[_paper_]](Not found)

* [21] A Martins, R Astudillo.Label Classification
    We propose sparsemax, a new activation function similar to the traditional softmax, but able to output sparse probabilities. After deriving its properties, we show how its Jacobian can be efficiently computed, enabling its use in a network trained with backpropagation. Then, we propose a new smooth and convex loss function which is the sparsemax analogue of the logistic loss. We reveal an unexpected connection between this new loss and the Huber classification loss. We obtain promising empirical results in multi-label classification …
    [[_paper_]](https://proceedings.mlr.press/v48/martins16)

* [22] H Liang, W Xie, X He, S Song….MODAL RECOMBINATION FOR MULTIMODAL SENTIMENT ANALYSIS
    Multimodal Sentiment Analysis is a burgeoning research area, leveraging various modalities to predict the sentiment score. Nevertheless, previous studies have disregarded the impact of noise interference on specific modal sentiments during video recording, thereby compromising the accuracy of sentiment prediction. In this paper, we propose the Guided Circular Decomposition and Cross-Modal Recombination (GCD-CMR) model, which aims to eliminate contaminated sentiment features in a fine-grained way. To achieve this, we utilize …
    [[_paper_]](https://ieeexplore.ieee.org/abstract/document/10446166/)

* [23] S Mai, Y Zeng, S Zheng, H Hu.Modal Representation for Multimodal Sentiment Analysis
    The wide application of smart devices enables the availability of multimodal data, which can be utilized in many tasks. In the field of multimodal sentiment analysis, most previous works focus on exploring intra-and inter-modal interactions. However, training a network with cross-modal information (language, audio and visual) is still challenging due to the modality gap. Besides, while learning dynamics within each sample draws great attention, the learning of inter-sample and inter-class relationships is neglected. Moreover, the size of datasets limits …
    [[_paper_]](https://ieeexplore.ieee.org/abstract/document/9767560/)

* [24] X Zhao, Y Chen, W Li, L Gao…. An Extended Multimodal Adaptation Gate for Multimodal Sentiment Analysis
    Human multimodal sentiment analysis is a challenging task that devotes to extract and integrate information from multiple resources, such as language, acoustic and visual information. Recently, multimodal adaptation gate (MAG), an attachment to transformer-based pre-trained language representation models, such as BERT and XLNet, has shown state-of-the-art performance on multimodal sentiment analysis. MAG only uses a 1-layer network to fuse multimodal information directly, and does not pay attention to relationships …
    [[_paper_]](https://ieeexplore.ieee.org/abstract/document/9746536/)

* [25] D Hazarika, S Poria, R Mihalcea…. Interactive Conversational Memory Network for Multimodal Emotion Detection
    Emotion recognition in conversations is crucial for building empathetic machines. Present works in this domain do not explicitly consider the inter-personal influences that thrive in the emotional dynamics of dialogues. To this end, we propose Interactive COnversational memory Network (ICON), a multimodal emotion detection framework that extracts multimodal features from conversational videos and hierarchically models the self-and inter-speaker emotional influences into global memories. Such memories generate contextual summaries …
    [[_paper_]](https://aclanthology.org/D18-1280/)

* [26] C Busso, M Bulut, CC Lee, A Kazemzadeh…. Interactive emotional dyadic motion capture database
    Since emotions are expressed through a combination of verbal and non-verbal channels, a joint analysis of speech and gestures is required to understand expressive human communication. To facilitate such investigations, this paper describes a new corpus named the “interactive emotional dyadic motion capture database”(IEMOCAP), collected by the Speech Analysis and Interpretation Laboratory (SAIL) at the University of Southern California (USC). This database was recorded from ten actors in dyadic sessions with …
    [[_paper_]](https://link.springer.com/article/10.1007/s10579-008-9076-6)

* [27] W Han, H Chen, S Poria.Improving Multimodal Fusion with Hierarchical Mutual Information Maximization for Multimodal Sentiment Analysis
    In multimodal sentiment analysis (MSA), the performance of a model highly depends on the quality of synthesized embeddings. These embeddings are generated from the upstream process called multimodal fusion, which aims to extract and combine the input unimodal raw data to produce a richer multimodal representation. Previous work either back-propagates the task loss or manipulates the geometric property of feature spaces to produce favorable fusion results, which neglects the preservation of critical task-related information that flows …
    [[_paper_]](https://arxiv.org/abs/2109.00412)

* [28] W Rahman, MK Hasan, S Lee, A Zadeh….Integrating Multimodal Information in Large Pretrained Transformers
    Recent Transformer-based contextual word representations, including BERT and XLNet, have shown state-of-the-art performance in multiple disciplines within NLP. Fine-tuning the trained contextual models on task-specific datasets has been the key to achieving superior performance downstream. While fine-tuning these pre-trained models is straight-forward for lexical applications (applications with only language modality), it is not trivial for multimodal language (a growing area in NLP focused on modeling face-to-face communication). Pre …
    [[_paper_]](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8005298/)

* [29] C Chen, H Hong, J Guo, B Song.Intra Modal Representation Augmentation With Trimodal Collaborative Disentanglement Network for Multimodal Sentiment Analysis
    Recently, Multimodal Sentiment Analysis (MSA) is a challenging research area given its complex nature, and humans express emotional cues across various modalities such as language, facial expressions, and speech. Representation and fusion of features are the most crucial tasks in multimodal sentiment analysis research. However, in the current research, most methods ignore the importance of eliminating potential irrelevant features in the original features of each modality and cross-modal common feature. Moreover, the …
    [[_paper_]](https://ieeexplore.ieee.org/abstract/document/10089492/)

* [30] VW Liang, Y Zhang, Y Kwon….modal Contrastive Representation Learning
    We present modality gap, an intriguing geometric phenomenon of the representation space of multi-modal models. Specifically, we show that different data modalities (eg images and text) are embedded at arm's length in their shared representation in multi-modal models such as CLIP. Our systematic analysis demonstrates that this gap is caused by a combination of model initialization and contrastive learning optimization. In model initialization, we show empirically and theoretically that the representation of a common …
    [[_paper_]](https://proceedings.neurips.cc/paper_files/paper/2022/hash/702f4db7543a7432431df588d57bc7c9-Abstract-Conference.html)

* [31] D Hazarika, R Zimmermann, S Poria.Specific Representations for Multimodal Sentiment Analysis
    Multimodal Sentiment Analysis is an active area of research that leverages multimodal signals for affective understanding of user-generated videos. The predominant approach, addressing this task, has been to develop sophisticated fusion techniques. However, the heterogeneous nature of the signals creates distributional modality gaps that pose significant challenges. In this paper, we aim to learn effective modality representations to aid the process of fusion. We propose a novel framework, MISA, which projects each modality to …
    [[_paper_]](https://dl.acm.org/doi/abs/10.1145/3394171.3413678)

* [32] J Zeng, J Zhou, T Liu.Mitigating Inconsistencies in Multimodal Sentiment Analysis under Uncertain Missing Modalities
    For the missing modality problem in Multimodal Sentiment Analysis (MSA), the inconsistency phenomenon occurs when the sentiment changes due to the absence of a modality. The absent modality that determines the overall semantic can be considered as a key missing modality. However, previous works all ignored the inconsistency phenomenon, simply discarding missing modalities or solely generating associated features from available modalities. The neglect of the key missing modality case may lead to incorrect semantic …
    [[_paper_]](https://aclanthology.org/2022.emnlp-main.189/)

* [33] K Yang, H Xu, K Gao.Audio Sentiment Analysis
    Multimodal sentiment analysis is an emerging research field that aims to enable machines to recognize, interpret, and express emotion. Through the cross-modal interaction, we can get more comprehensive emotional characteristics of the speaker. Bidirectional Encoder Representations from Transformers (BERT) is an efficient pre-trained language representation model. Fine-tuning it has obtained new state-of-the-art results on eleven natural language processing tasks like question answering and natural language inference …
    [[_paper_]](https://dl.acm.org/doi/abs/10.1145/3394171.3413690)

* [34] S Mai, H Hu, S Xing.Modality to Modality Translation An Adversarial Representation Learning and Graph Fusion Network for Multimodal Fusion
    Learning joint embedding space for various modalities is of vital importance for multimodal fusion. Mainstream modality fusion approaches fail to achieve this goal, leaving a modality gap which heavily affects cross-modal fusion. In this paper, we propose a novel adversarial encoder-decoder-classifier framework to learn a modality-invariant embedding space. Since the distributions of various modalities vary in nature, to reduce the modality gap, we translate the distributions of source modalities into that of target modality via their respective encoders …
    [[_paper_]](https://aaai.org/ojs/index.php/AAAI/article/view/5347)

* [35] Z Liu, B Zhou, D Chu, Y Sun, L Meng.based multimodal sentiment analysis under uncertain missing modalities
    Multimodal sentiment analysis (MSA) with uncertain missing modalities poses a new challenge in sentiment analysis. To address this problem, efficient MSA models that consider missing modalities have been proposed. However, existing studies have only adopted the concatenation operation for feature fusion while ignoring the deep interactions between different modalities. Moreover, existing studies have failed to take advantage of the text modality, which can achieve better accuracy in sentiment analysis. To tackle the above …
    [[_paper_]](https://www.sciencedirect.com/science/article/pii/S1566253523002890)

* [36] Z Lin, B Liang, Y Long, Y Dang…. Hierarchical Graph Contrastive Learning for Multimodal Sentiment Analysis
    The existing research efforts in Multimodal Sentiment Analysis (MSA) have focused on developing the expressive ability of neural networks to fuse information from different modalities. However, these approaches lack a mechanism to understand the complex relations within and across different modalities, since some sentiments may be scattered in different modalities. To this end, in this paper, we propose a novel hierarchical graph contrastive learning (HGraph-CL) framework for MSA, aiming to explore the intricate …
    [[_paper_]](https://repository.essex.ac.uk/34855/)

* [37] A Zadeh, R Zellers, E Pincus, LP Morency. Multimodal Corpus of Sentiment Intensity and Subjectivity Analysis in Online Opinion Videos
    People are sharing their opinions, stories and reviews through online video sharing websites every day. Studying sentiment and subjectivity in these opinion videos is experiencing a growing attention from academia and industry. While sentiment analysis has been successful for text, it is an understudied research question for videos and multimedia content. The biggest setbacks for studies in this direction are lack of a proper dataset, methodology, baselines and statistical analysis of how information from different modality …
    [[_paper_]](https://arxiv.org/abs/1606.06259)

* [38] J Yang, Y Wang, R Yi, Y Zhu, A Rehman….Temporal Attention Graph for Unaligned Human Multimodal Language Sequences
    Human communication is multimodal in nature; it is through multiple modalities such as language, voice, and facial expressions, that opinions and emotions are expressed. Data in this domain exhibits complex multi-relational and temporal interactions. Learning from this data is a fundamentally challenging research problem. In this paper, we propose Modal-Temporal Attention Graph (MTAG). MTAG is an interpretable graph-based neural model that provides a suitable framework for analyzing multimodal sequential data. We first introduce a …
    [[_paper_]](https://arxiv.org/abs/2010.11985)

* [39] L Xiao, X Wu, W Wu, J Yang….CHANNEL ATTENTIVE GRAPH CONVOLUTIONAL NETWORK WITH SENTIMENT FUSION FOR MULTIMODAL SENTIMENT ANALYSIS
    Nowadays, with the explosive growth of multimodal reviews on social media platforms, multimodal sentiment analysis has recently gained popularity because of its high relevance to these social media posts. Although most previous studies design various fusion frameworks for learning an interactive representation of multiple modalities, they fail to incorporate sentimental knowledge into inter-modality learning. This pa-per proposes a Multi-channel Attentive Graph Convolutional Network (MAGCN), consisting of two main …
    [[_paper_]](https://ieeexplore.ieee.org/abstract/document/9747542/)

* [40] J Zheng, S Zhang, Z Wang, X Wang….Head Attention for Multimodal Emotion Recognition
    Multimodal Emotion Recognition is challenging because of the heterogeneity gap among different modalities. Due to the powerful ability of feature abstraction, Deep Neural Networks (DNNs) have exhibited significant success in bridging the heterogeneity gap in cross-modal retrieval and generation tasks. In this work, a DNNs-based Multi-channel Weight-sharing Autoencoder with Cascade Multi-head Attention (MCWSA-CMHA) is proposed to generically address the affective heterogeneity gap in MER. Specifically, multimodal heterogeneity …
    [[_paper_]](https://ieeexplore.ieee.org/abstract/document/9693238/)

* [41] L Fang, G Liu, R Zhang.GRAINED MULTIMODAL INTERACTION NETWORK FOR SENTIMENT ANALYSIS
    Multimodal sentiment analysis aims to utilize different modalities including language, visual, and audio to identify human emotions in videos. Multimodal interaciton mechanism is the key challenge. Previous works lack modeling of multimodal interaction at different grain levels, and does not suppress redundant information in multimodal interaction. This leads to incomplete multimodal representation with noisy information. To address these issues, we propose Multi-grained Multimodal Interaction Network (MMIN) to provide a more complete …
    [[_paper_]](https://ieeexplore.ieee.org/abstract/document/10446351/)

* [42] S Poria, E Cambria, D Hazarika….level Multiple Attentions for Contextual Multimodal Sentiment Analysis
    Multimodal sentiment analysis involves identifying sentiment in videos and is a developing field of research. Unlike current works, which model utterances individually, we propose a recurrent model that is able to capture contextual information among utterances. In this paper, we also introduce attentionbased networks for improving both context learning and dynamic feature fusion. Our model shows 6-8% improvement over the state of the art on a benchmark dataset.
    [[_paper_]](https://ieeexplore.ieee.org/abstract/document/8215597/)

* [43] MS Akhtar, DS Chauhan, D Ghosal, S Poria….modal Emotion Recognition and Sentiment Analysis
    Related tasks often have inter-dependence on each other and perform better when solved in a joint framework. In this paper, we present a deep multi-task learning framework that jointly performs sentiment and emotion analysis both. The multi-modal inputs (ie, text, acoustic and visual frames) of a video convey diverse and distinctive information, and usually do not have equal contribution in the decision making. We propose a context-level inter-modal attention framework for simultaneously predicting the sentiment and expressed emotions of an …
    [[_paper_]](https://arxiv.org/abs/1905.05812)

* [44] R Lin, H Hu.Modal Prediction for Multimodal Sentiment Analysis
    Multimodal representation learning is a challenging task in which previous work mostly focus on either uni-modality pre-training or cross-modality fusion. In fact, we regard modeling multimodal representation as building a skyscraper, where laying stable foundation and designing the main structure are equally essential. The former is like encoding robust uni-modal representation while the later is like integrating interactive information among different modalities, both of which are critical to learning an effective …
    [[_paper_]](https://arxiv.org/abs/2210.14556)

* [45] PP Liang, Z Liu, A Zadeh, LP Morency.Multimodal Language Analysis with Recurrent Multistage Fusion
    Computational modeling of human multimodal language is an emerging research area in natural language processing spanning the language, visual and acoustic modalities. Comprehending multimodal language requires modeling not only the interactions within each modality (intra-modal interactions) but more importantly the interactions between modalities (cross-modal interactions). In this paper, we propose the Recurrent Multistage Fusion Network (RMFN) which decomposes the fusion problem into multiple stages, each of …
    [[_paper_]](https://arxiv.org/abs/1808.03920)

* [46] Z Wu, Z Gong, J Koo, J Hirschberg.loss Fusion Network for Sentiment Analysis
    This paper investigates the optimal selection and fusion of feature encoders across multiple modalities and combines these in one neural network to improve sentiment detection. We compare different fusion methods and examine the impact of multi-loss training within the multi-modality fusion network, identifying surprisingly important findings relating to subnet performance. We have also found that integrating context significantly enhances model performance. Our best model achieves state-of-the-art performance for three datasets (CMU …
    [[_paper_]](https://aclanthology.org/2024.naacl-long.197/)

* [47] J Cheng, I Fostiropoulos, B Boehm….Multimodal Phased Transformer for Sentiment Analysis
    Multimodal Transformers achieve superior performance in multimodal learning tasks. However, the quadratic complexity of the self-attention mechanism in Transformers limits their deployment in low-resource devices and makes their inference and training computationally expensive. We propose multimodal Sparse Phased Transformer (SPT) to alleviate the problem of self-attention complexity and memory footprint. SPT uses a sampling function to generate a sparse attention matrix and compress a long sequence to a shorter …
    [[_paper_]](https://aclanthology.org/2021.emnlp-main.189/)

* [48] J Zheng, S Zhang, X Wang, Z Zeng.Multimodal Representations Learning Based on Mutual Information Maximization and Minimization and Identity Embedding for Multimodal Sentiment Analysis
    Multimodal sentiment analysis (MSA) is a fundamental complex research problem due to the heterogeneity gap between different modalities and the ambiguity of human emotional expression. Although there have been many successful attempts to construct multimodal representations for MSA, there are still two challenges to be addressed: 1) A more robust multimodal representation needs to be constructed to bridge the heterogeneity gap and cope with the complex multimodal interactions, and 2) the contextual dynamics must be …
    [[_paper_]](https://arxiv.org/abs/2201.03969)

* [49] Z Li, Q Guo, C Feng, L Deng, Q Zhang….Multimodal Sentiment Analysis Based on Interactive Transformer and Soft Mapping
    Multimodal sentiment analysis aims to harvest people's opinions or attitudes from multimedia data through fusion techniques. However, existing fusion methods cannot take advantage of the correlation between multimodal data but introduce interference factors. In this paper, we propose an Interactive Transformer and Soft Mapping based method for multimodal sentiment analysis. In the Interactive Transformer layer, an Interactive Multihead Guided‐Attention structure composed of a pair of Multihead Attention modules is first utilized …
    [[_paper_]](https://onlinelibrary.wiley.com/doi/abs/10.1155/2022/6243347)

* [50] M Chen, S Wang, PP Liang, T Baltrušaitis….Level Fusion and Reinforcement Learning
    With the increasing popularity of video sharing websites such as YouTube and Facebook, multimodal sentiment analysis has received increasing attention from the scientific community. Contrary to previous works in multimodal sentiment analysis which focus on holistic information in speech segments such as bag of words representations and average facial expression intensity, we propose a novel deep architecture for multimodal sentiment analysis that is able to perform modality fusion at the word level. In this paper, we propose …
    [[_paper_]](https://dl.acm.org/doi/abs/10.1145/3136755.3136801)

* [51] R Kaur, S Kautish. A Survey
    … Multimodal sentiments have become the challenge for the … This survey article covers the 
comprehensive overview of the … are presented briefly in this survey. The article is categorized …
    [[_paper_]](https://www.igi-global.com/chapter/multimodal-sentiment-analysis/308579)

* [52] A Gandhi, K Adhvaryu, S Poria, E Cambria, A Hussain. A systematic review of history, datasets, multimodal fusion methods, applications, challenges and future directions
    Sentiment analysis (SA) has gained much traction In the field of artificial intelligence (AI) and natural language processing (NLP). There is growing demand to automate analysis of user sentiment towards products or services. Opinions are increasingly being shared online in the form of videos rather than text alone. This has led to SA using multiple modalities, termed Multimodal Sentiment Analysis (MSA), becoming an important research area. MSA utilises latest advancements in machine learning and deep learning at various stages including for …
    [[_paper_]](https://www.sciencedirect.com/science/article/pii/S1566253522001634)

* [53] X Yang, S Feng, Y Zhang, D Wang.channel Graph Neural Networks
    With the popularity of smartphones, we have witnessed the rapid proliferation of multimodal posts on various social media platforms. We observe that the multimodal sentiment expression has specific global characteristics, such as the interdependencies of objects or scenes within the image. However, most previous studies only considered the representation of a single image-text post and failed to capture the global co-occurrence characteristics of the dataset. In this paper, we propose Multi-channel Graph Neural …
    [[_paper_]](https://aclanthology.org/2021.acl-long.28/)

* [54] Not found.Modality_Interactions
    Not found
    [[_paper_]](Not found)

* [55] MS Akhtar, DS Chauhan, D Ghosal, S Poria….modal Emotion Recognition and Sentiment Analysis(1)
    Related tasks often have inter-dependence on each other and perform better when solved in a joint framework. In this paper, we present a deep multi-task learning framework that jointly performs sentiment and emotion analysis both. The multi-modal inputs (ie, text, acoustic and visual frames) of a video convey diverse and distinctive information, and usually do not have equal contribution in the decision making. We propose a context-level inter-modal attention framework for simultaneously predicting the sentiment and expressed emotions of an …
    [[_paper_]](https://arxiv.org/abs/1905.05812)

* [56] F Wang, S Tian, L Yu, J Liu, J Wang, K Li….9
    Multimodal sentiment analysis is a popular and challenging research topic in natural 
language processing, but the impact of individual modal data in videos on sentiment analysis …
    [[_paper_]](https://link.springer.com/article/10.1007/s12559-022-10073-9)

* [57] H Luo, L Ji, Y Huang, B Wang, S Ji, T Li.Scale Fusion of Locally Descriptors
    Fusion technique is a key research topic in multimodal sentiment analysis. The recent attention-based fusion demonstrates advances over simple operation-based fusion. However, these fusion works adopt single-scale, ie, token-level or utterance-level, unimodal representation. Such single-scale fusion is suboptimal because that different modality should be aligned with different granularities. This paper proposes a fusion model named ScaleVLAD to gather multi-Scale representation from text, video, and audio with shared …
    [[_paper_]](https://arxiv.org/abs/2112.01368)

* [58] H Yang, X Gao, J Wu, T Gan, N Ding….interaction Modeling For Multimodal Emotion Recognition
    The multimodal emotion recognition in conversation task aims to predict the emotion label for a given utterance with its context and multiple modalities. Existing approaches achieve good results but also suffer from the following two limitations: 1) lacking modeling of diverse dependency ranges, ie, long, short, and independent context-specific representations and without consideration of the different recognition difficulty for each utterance; 2) consistent treatment of the contribution for various modalities. To address the above challenges, we …
    [[_paper_]](https://aclanthology.org/2023.findings-acl.390/)

* [59] F Qian, J Han, Y He, T Zheng….supervised Learning for Multimodal Sentiment Analysis
    Abstract Multimodal Sentiment Analysis (MSA) has made great progress that benefits from extraordinary fusion scheme. However, there is a lack of labeled data, resulting in severe overfitting and poor generalization for supervised models applied in this field. In this paper, we propose Sentiment Knowledge Enhanced Self-supervised Learning (SKESL) to capture common sentimental patterns in unlabeled videos, which facilitates further learning on limited labeled data. Specifically, with the help of sentiment knowledge and non-verbal …
    [[_paper_]](https://aclanthology.org/2023.findings-acl.821/)

* [60] Y Wu, Y Zhao, H Yang, S Chen, B Qin, X Cao….Sentiment Word Aware Multimodal Refinement for Multimodal Sentiment Analysis with ASR Errors
    Multimodal sentiment analysis has attracted increasing attention and lots of models have been proposed. However, the performance of the state-of-the-art models decreases sharply when they are deployed in the real world. We find that the main reason is that real-world applications can only access the text outputs by the automatic speech recognition (ASR) models, which may be with errors because of the limitation of model capacity. Through further analysis of the ASR outputs, we find that in some cases the sentiment words, the key …
    [[_paper_]](https://arxiv.org/abs/2203.00257)

* [61] S Lai, J Li, G Guo, X Hu, Y Li, Y Tan, Z Song….Task Learning
    Designing an effective representation learning method for multimodal sentiment analysis tasks is a crucial research direction. The challenge lies in learning both shared and private information in a complete modal representation, which is difficult with uniform multimodal labels and a raw feature fusion approach. In this work, we propose a deep modal shared information learning module based on the covariance matrix to capture the shared information between modalities. Additionally, we use a label generation module based on a …
    [[_paper_]](https://arxiv.org/abs/2305.08473)

* [62] C Zhu, M Chen, S Zhang, C Sun, H Liang, Y Liu…. Sentiment Knowledge Enhanced Attention Fusion Network for multimodal sentiment analysis
    Multimodal sentiment analysis is an active research field that aims to recognize the user's sentiment information from multimodal data. The primary challenge in this field is to develop a high-quality fusion framework that effectively addresses the heterogeneity among different modalities. However, prior research has primarily concentrated on intermodal interactions while neglecting the semantic sentiment information conveyed by words in the text modality. In this paper, we propose the Sentiment Knowledge Enhanced Attention Fusion Network …
    [[_paper_]](https://www.sciencedirect.com/science/article/pii/S1566253523002749)

* [63] M Chen, X Li. Sentimental Words Aware Fusion Network for Multimodal Sentiment Analysis
    Multimodal sentiment analysis aims to predict sentiment of language text with the help of other modalities, such as vision and acoustic features. Previous studies focused on learning the joint representation of multiple modalities, ignoring some useful knowledge contained in language modal. In this paper, we try to incorporate sentimental words knowledge into the fusion network to guide the learning of joint representation of multimodal features. Our method consists of two components: shallow fusion part and aggregation part. For the …
    [[_paper_]](https://aclanthology.org/2020.coling-main.93/)

* [64] Y Wei, S Yuan, R Yang, L Shen, Z Li….View Calibration Network for Multimodal Sentiment Detection
    With the popularity of social media, detecting sentiment from multimodal posts (eg image-text pairs) has attracted substantial attention recently. Existing works mainly focus on fusing different features but ignore the challenge of modality heterogeneity. Specifically, different modalities with inherent disparities may bring three problems: 1) introducing redundant visual features during feature fusion; 2) causing feature shift in the representation space; 3) leading to inconsistent annotations for different modal data. All these issues will increase the …
    [[_paper_]](https://aclanthology.org/2023.acl-long.287/)

* [65] J Zeng, T Liu, J Zhou.assisted Multimodal Sentiment Analysis under Uncertain
    Multimodal sentiment analysis has been studied under the assumption that all modalities are available. However, such a strong assumption does not always hold in practice, and most of multimodal fusion models may fail when partial modalities are missing. Several works have addressed the missing modality problem; but most of them only considered the single modality missing case, and ignored the practically more general cases of multiple modalities missing. To this end, in this paper, we propose a Tag-Assisted Transformer …
    [[_paper_]](https://dl.acm.org/doi/abs/10.1145/3477495.3532064)

* [66] J Yu, J Wang, R Xia, J Li.Target Matching
    Abstract Targeted Multimodal Sentiment Classification (TMSC) aims to identify the sentiment polarities over each target mentioned in a pair of sentence and image. Existing methods to TMSC failed to explicitly capture both coarse-grained and fine-grained image-target matching, including 1) the relevance between the image and the target and 2) the alignment between visual objects and the target. To tackle this issue, we propose a new multi-task learning architecture named coarse-to-fine grained Image-Target Matching network (ITM) …
    [[_paper_]](https://www.ijcai.org/proceedings/2022/0622.pdf)

* [67] M Zhou, W Quan, Z Zhou, K Wang, T Wang….oriented Cross Attention Network for Multimodal Sentiment Analysis
    Multimodal Sentiment Analysis (MSA) endeavors to understand human sentiment by leveraging language, visual, and acoustic modalities. Despite the remarkable performance exhibited by previous MSA approaches, the presence of inherent multimodal heterogeneities poses a challenge, with the contribution of different modalities varying considerably. Past research predominantly focused on improving representation learning techniques and feature fusion strategies. However, many of these efforts overlooked the …
    [[_paper_]](https://arxiv.org/abs/2404.04545)

* [68] C Huang, J Zhang, X Wu, Y Wang, M Li….centered fusion network with crossmodal attention for multimodal sentiment analysis
    Multimodal sentiment analysis (MSA), which goes beyond the analysis of texts to include other modalities such as audio and visual data, has attracted a significant amount of attention. An effective fusion of sentiment information in multiple modalities is key to improving the performance of MSA. However, aligning multiple modalities during the process of fusion faces challenges such as maintaining modal-specific information. This paper proposes a Text-centered Fusion Network with crossmodal Attention (TeFNA), a …
    [[_paper_]](https://www.sciencedirect.com/science/article/pii/S0950705123002526)

* [69] A Zadeh, M Chen, S Poria, E Cambria….Tensor Fusion Network for Multimodal Sentiment Analysis
    Multimodal sentiment analysis is an increasingly popular research area, which extends the conventional language-based definition of sentiment analysis to a multimodal setup where other relevant modalities accompany language. In this paper, we pose the problem of multimodal sentiment analysis as modeling intra-modality and inter-modality dynamics. We introduce a novel model, termed Tensor Fusion Network, which learns both such dynamics end-to-end. The proposed approach is tailored for the volatile nature of spoken language in …
    [[_paper_]](https://arxiv.org/abs/1707.07250)

* [70] D Wang, X Guo, Y Tian, J Liu, LH He, X Luo. A text enhanced transformer fusion network for multimodal sentiment analysis
    Multimodal sentiment analysis (MSA), which aims to recognize sentiment expressed by speakers in videos utilizing textual, visual and acoustic cues, has attracted extensive research attention in recent years. However, textual, visual and acoustic modalities often contribute differently to sentiment analysis. In general, text contains more intuitive sentiment-related information and outperforms nonlinguistic modalities in MSA. Seeking a strategy to take advantage of this property to obtain a fusion representation containing more sentiment …
    [[_paper_]](https://www.sciencedirect.com/science/article/pii/S0031320322007385)

* [71] L Stappen, A Baird, L Schumann…. Collection, Insights and Improvements
    Truly real-life data presents a strong, but exciting challenge for sentiment and emotion research. The high variety of possible 'in-the-wild'properties makes large datasets such as these indispensable with respect to building robust machine learning models. A sufficient quantity of data covering a deep variety in the challenges of each modality to force the exploratory analysis of the interplay of all modalities has not yet been made available in this context. In this contribution, we present MuSe-CaR, a first of its kind multimodal dataset. The …
    [[_paper_]](https://ieeexplore.ieee.org/abstract/document/9484711/)

* [72] X Zhao, Y Chen, S Liu, X Zang, Y Xiang…. A New Token Mixup Multimodal Data Augmentation for Multimodal Sentiment Analysis
    Existing methods for Multimodal Sentiment Analysis (MSA) mainly focus on integrating multimodal data effectively on limited multimodal data. Learning more informative multimodal representation often relies on large-scale labeled datasets, which are difficult and unrealistic to obtain. To learn informative multimodal representation on limited labeled datasets as more as possible, we proposed TMMDA for MSA, a new Token Mixup Multimodal Data Augmentation, which first generates new virtual modalities from the mixed …
    [[_paper_]](https://dl.acm.org/doi/abs/10.1145/3543507.3583406)

* [73] Y Li, W Weng, C Liu.stage contrastive learning and feature hierarchical fusion network for multimodal sentiment analysis
    Multimodal sentiment analysis faces two challenges: modality representation and modality fusion. Most of the existing models rely only on the feature extraction network to learn modality representation, and the fusion mechanism adopted by some models does not perform well. These factors are not conducive to the model learning rich emotional information and further affect the model's predictive ability. To solve these problems, we propose a multimodal sentiment analysis model based on two-stage contrastive learning …
    [[_paper_]](https://link.springer.com/article/10.1007/s00521-024-09634-w)

* [74] Y Zeng, S Mai, H Hu.modal Dynamics for Multimodal Sentiment Analysis
    Multimodal sentiment analysis (MSA) draws increasing attention with the availability of multimodal data. The boost in performance of MSA models is mainly hindered by two problems. On the one hand, recent MSA works mostly focus on learning cross-modal dynamics, but neglect to explore an optimal solution for unimodal networks, which determines the lower limit of MSA models. On the other hand, noisy information hidden in each modality interferes the learning of correct cross-modal dynamics. To address the above …
    [[_paper_]](https://arxiv.org/abs/2111.08451)

* [75] W Yu, H Xu, Z Yuan, J Wu.Task Learning for Multimodal Sentiment Analysis
    Abstract Representation Learning is a significant and challenging task in multimodal learning. Effective modality representations should contain two parts of characteristics: the consistency and the difference. Due to the unified multimodal annota-tion, existing methods are restricted in capturing differenti-ated information. However, additional unimodal annotations are high time-and labor-cost. In this paper, we design a la-bel generation module based on the self-supervised learning strategy to acquire independent unimodal …
    [[_paper_]](https://ojs.aaai.org/index.php/AAAI/article/view/17289)

* [76] Not found._ACL19_Multimodal Transformer for Unaligned Multimodal Language Sequences_169
    Not found
    [[_paper_]](Not found)

* [77] J Tang, K Li, X Jin, A Cichocki, Q Zhao….Translation Fusion Network
    Multimodal sentiment analysis is the challenging research area that attends to the fusion of multiple heterogeneous modalities. The main challenge is the occurrence of some missing modalities during the multimodal fusion procedure. However, the existing techniques require all modalities as input, thus are sensitive to missing modalities at predicting time. In this work, the coupled-translation fusion network (CTFN) is firstly proposed to model bi-direction interplay via couple learning, ensuring the robustness in respect to missing modalities …
    [[_paper_]](https://aclanthology.org/2021.acl-long.412/)

* [78] A Zadeh, YS Cao, S Hessner, PP Liang…. A Multimodal Language Dataset for Spanish, Portuguese, German and French
    Modeling multimodal language is a core research area in natural language processing. While languages such as English have relatively large multimodal language resources, other widely spoken languages across the globe have few or no large-scale datasets in this area. This disproportionately affects native speakers of languages other than English. As a step towards building more equitable and inclusive multimodal systems, we introduce the first large-scale multimodal language dataset for Spanish, Portuguese, German and French …
    [[_paper_]](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8106386/)

* [79] W Han, H Chen, S Poria._EMNLP21_Improving Multimodal Fusion with Hierarchical Mutual Information Maximization for Multimodal Sentiment Analysis
    In multimodal sentiment analysis (MSA), the performance of a model highly depends on the quality of synthesized embeddings. These embeddings are generated from the upstream process called multimodal fusion, which aims to extract and combine the input unimodal raw data to produce a richer multimodal representation. Previous work either back-propagates the task loss or manipulates the geometric property of feature spaces to produce favorable fusion results, which neglects the preservation of critical task-related information that flows …
    [[_paper_]](https://arxiv.org/abs/2109.00412)

* [80] Z Yuan, W Li, H Xu, W Yu.based Feature Reconstruction Network for Robust Multimodal Sentiment Analysis
    Improving robustness against data missing has become one of the core challenges in Multimodal Sentiment Analysis (MSA), which aims to judge speaker sentiments from the language, visual, and acoustic signals. In the current research, translation-based methods and tensor regularization methods are proposed for MSA with incomplete modality features. However, both of them fail to cope with random modality feature missing in non-aligned sequences. In this paper, a transformer-based feature reconstruction network (TFR-Net) is …
    [[_paper_]](https://dl.acm.org/doi/abs/10.1145/3474085.3475585)


### _probe_ ###
* [1] T Le Scao, A Fan, C Akiki, E Pavlick, S Ilić, D Hesslow… - 2023 - inria.hal.science.Access Multilingual Language Model
    … present BLOOM, a 176B-parameter open-access language model … BLOOM is a decoder-only 
Transformer language model that … and 13 programming languages (59 in total). We find that …
    [[_paper_]](https://inria.hal.science/hal-03850124/)

* [2] Z Elyoseph, D Hadar-Shoval, K Asraf….ChatGPT outperforms humans in emotional awareness evaluations
    The artificial intelligence chatbot, ChatGPT, has gained widespread attention for its ability to perform natural language processing tasks and has the fastest-growing user base in history. Although ChatGPT has successfully generated theoretical information in multiple fields, its ability to identify and describe emotions is still unknown. Emotional awareness (EA), the ability to conceptualize one's own and others' emotions, is considered a transdiagnostic mechanism for psychopathology. This study utilized the Levels of Emotional Awareness …
    [[_paper_]](https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2023.1199058/full)

* [3] K Schaaff, C Reinig, T Schlippe.Exploring ChatGPT’s Empathic Abilities
    Empathy is often understood as the ability to share and understand another individual's state of mind or emotion. With the increasing use of chatbots in various domains, eg, children seeking help with homework, individuals looking for medical advice, and people using the chatbot as a daily source of everyday companionship, the importance of empathy in human-computer interaction has become more apparent. Therefore, our study investigates the extent to which ChatGPT based on GPT-3.5 can exhibit empathetic responses and …
    [[_paper_]](https://ieeexplore.ieee.org/abstract/document/10388208/)

* [4] W Zhao, Y Zhao, X Lu, S Wang, Y Tong….Is ChatGPT Equipped with Emotional Dialogue Capabilities_
    This report presents a study on the emotional dialogue capability of ChatGPT, an advanced language model developed by OpenAI. The study evaluates the performance of ChatGPT on emotional dialogue understanding and generation through a series of experiments on several downstream tasks. Our findings indicate that while ChatGPT's performance on emotional dialogue understanding may still lag behind that of supervised models, it exhibits promising results in generating emotional responses. Furthermore, the study suggests …
    [[_paper_]](https://arxiv.org/abs/2304.09582)

* [5] H Touvron, L Martin, K Stone, P Albert….Tuned Chat Models
    … large language models (LLMs) ranging in scale from 7 … fine-tuned LLMs, called Llama 
2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on …
    [[_paper_]](https://arxiv.org/abs/2307.09288)

