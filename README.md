
_Papers_
----------------
### _Missing_modality_ ###
* [1] T Zhou, S Canu, P Vera, S Ruan. [[_modality fusion based deep neural network for brain tumor segmentation with missing MR modalities_]](https://www.sciencedirect.com/science/article/pii/S0925231221013904)      
    Abstract Using multimodal Magnetic Resonance Imaging (MRI) is necessary for accurate brain tumor segmentation. The main problem is that not all types of MRIs are always available in clinical exams. Based on the fact that there is a strong correlation between MR modalities of the same patient, in this work, we propose a novel brain tumor segmentation network in the case of missing one or more modalities. The proposed network consists of three sub-networks: a feature-enhanced generator, a correlation constraint block and a …

* [2] J Zhao, R Li, Q Jin. [[_Missing Modality Imagination Network for Emotion Recognition with Uncertain Missing Modalities_]](https://aclanthology.org/2021.acl-long.203/)      
    Multimodal fusion has been proved to improve emotion recognition performance in previous works. However, in real-world applications, we often encounter the problem of missing modality, and which modalities will be missing is uncertain. It makes the fixed multimodal fusion fail in such cases. In this work, we propose a unified model, Missing Modality Imagination Network (MMIN), to deal with the uncertain missing modality problem. MMIN learns robust joint multimodal representations, which can predict the representation of any …

* [3] W Luo, M Xu, H Lai. [[_Multimodal Reconstruct and Align Net for Missing Modality Problem in Sentiment Analysis_]](https://link.springer.com/chapter/10.1007/978-3-031-27818-1_34)      
    Abstract Multimodal Sentiment Analysis (MSA) aims at recognizing emotion categories by textual, visual, and acoustic cues. However, in real-life scenarios, one or two modalities may be missing due to various reasons. And when text modality is missing, obvious deterioration will be observed since text modality contains much more semantic information compared to vision and audio modality. To this end, we propose the Multimodal Reconstruct and Align Net (MRAN) to tackle the missing modality problem, especially to relieve the decline caused …

* [4] Q Zhang, L Shi, P Liu, Z Zhu, L Xu - 2023 - Springer. [[_integrating consistency and difference networks by transformer for multimodal sentiment analysis_]](https://link.springer.com/article/10.1007/s10489-023-04869-x)      
    The Editor-in-Chief has retracted this article because of an overlap with a previously-published article by different authors [1] due to insufficient citations and inappropriate expressions quoted from [1]. As Table 1 in this article overlaps with Table 2 in [1], and formulas in Sect. 3.2 are the same as in [1] without explicit indication. Moreover, without fair indication, the schematic presentation in the article, is the same as in [1]. The authors did not explicitly state whether they agree to this retraction.

* [5] J Zeng, T Liu, J Zhou. [[_assisted Multimodal Sentiment Analysis under Uncertain Missing Modalities_]](https://dl.acm.org/doi/abs/10.1145/3477495.3532064)      
    Multimodal sentiment analysis has been studied under the assumption that all modalities are available. However, such a strong assumption does not always hold in practice, and most of multimodal fusion models may fail when partial modalities are missing. Several works have addressed the missing modality problem; but most of them only considered the single modality missing case, and ignored the practically more general cases of multiple modalities missing. To this end, in this paper, we propose a Tag-Assisted Transformer …

* [6] S Parthasarathy, S Sundaram. [[_Visual Expression Recognition_]](https://dl.acm.org/doi/abs/10.1145/3395035.3425202)      
    Automatic audio-visual expression recognition can play an important role in communication services such as tele-health, VOIP calls and human-machine interaction. Accuracy of audio-visual expression recognition could benefit from the interplay between the two modalities. However, most audio-visual expression recognition systems, trained in ideal conditions, fail to generalize in real world scenarios where either the audio or visual modality could be missing due to a number of reasons such as limited bandwidth, interactors' orientation, caller …

* [7] C Shang, A Palmer, J Sun, KS Chen…. [[_Missing View Imputation with Generative Adversarial Networks_]](https://ieeexplore.ieee.org/abstract/document/8257992/)      
    In an era when big data are becoming the norm, there is less concern with the quantity but more with the quality and completeness of the data. In many disciplines, data are collected from heterogeneous sources, resulting in multi-view or multi-modal datasets. The missing data problem has been challenging to address in multi-view data analysis. Especially, when certain samples miss an entire view of data, it creates the missing view problem. Classic multiple imputations or matrix completion methods are hardly effective here when no …








  



### _multimodal affective_ ###
#### _MABSA_ ####
* [1] H Zhao, M Yang, X Bai, H Liu. [[_A Survey on Multimodal Aspect-Based Sentiment Analysis_]](https://ieeexplore.ieee.org/abstract/document/10401113/)      
    Multimodal Aspect-Based Sentiment Analysis (MABSA), as an emerging task in the field of sentiment analysis, has recently received widespread attention. Its aim is to combine relevant multimodal data to determine the sentiment polarity of a given aspect in text. Researchers have surveyed both aspect-based sentiment analysis and multimodal sentiment analysis, but, to the best of our knowledge, there is no survey on MABSA. Therefore, in order to assist related researchers to know MABSA better, we surveyed the …

* [2] C Wang, Y Luo, C Meng, F Yuan. [[_An Adaptive Dual Graph Convolution Fusion Network for Aspect-Based Sentiment Analysis_]](https://dl.acm.org/doi/abs/10.1145/3659579)      
    Aspect-based Sentiment Analysis (ABSA), also known as fine-grained sentiment analysis, aims to predict the sentiment polarity of specific aspect words in the sentence. Some studies have explored the semantic correlation between words in sentences through attention-based methods. Other studies have learned syntactic knowledge by using graph convolution networks to introduce dependency relations. These methods have achieved satisfactory results in the ABSA tasks. However, due to the complexity of language, effectively capturing …

* [3] R Zhou, W Guo, X Liu, S Yu, Y Zhang…. [[_AoM Detecting Aspect-oriented Information for Multimodal_]](https://arxiv.org/abs/2306.01004)      
    Multimodal aspect-based sentiment analysis (MABSA) aims to extract aspects from text-image pairs and recognize their sentiments. Existing methods make great efforts to align the whole image to corresponding aspects. However, different regions of the image may relate to different aspects in the same sentence, and coarsely establishing image-aspect alignment will introduce noise to aspect-based sentiment analysis (ie, visual noise). Besides, the sentiment of a specific aspect can also be interfered by descriptions of other aspects (ie …

* [4] L Xiao, X Wu, S Yang, J Xu, J Zhou, L He. [[_Cross-modal fine-grained alignment and fusion network for multimodal aspect-based sentiment analysis_]](https://www.sciencedirect.com/science/article/pii/S0306457323002455)      
    Abstract Multi-modal Aspect-based Sentiment Analysis (MABSA) aims to forecast the polarity of sentiment concerning aspects within a given sentence based on the correlation between the sentence and its accompanying image. Comprehending multi-modal sentiment expression requires strong cross-modal alignment and fusion ability. Previous state-of-the-art (SOTA) models fail to explicitly align valuable visual clues with aspect and sentiment information in textual representations and overlook the utilization of syntactic dependency …

* [5] L Yang, JC Na, J Yu. [[_Cross-Modal Multitask Transformer for End-to-End Multimodal Aspect-Based Sentiment Analysis_]](https://www.sciencedirect.com/science/article/pii/S0306457322001479)      
    As an emerging task in opinion mining, End-to-End Multimodal Aspect-Based Sentiment Analysis (MABSA) aims to extract all the aspect-sentiment pairs mentioned in a pair of sentence and image. Most existing methods of MABSA do not explicitly incorporate aspect and sentiment information in their textual and visual representations and fail to consider the different contributions of visual representations to each word or aspect in the text. To tackle these limitations, we propose a multi-task learning framework named Cross-Modal Multitask …

* [6] Z Yu, J Wang, LC Yu, X Zhang. [[_Dual-Encoder Transformers with Cross-modal Alignment for Multimodal Aspect-based Sentiment Analysis_]](https://aclanthology.org/2022.aacl-main.32/)      
    Multimodal aspect-based sentiment analysis (MABSA) aims to extract the aspect terms from text and image pairs, and then analyze their corresponding sentiment. Recent studies typically use either a pipeline method or a unified transformer based on a cross-attention mechanism. However, these methods fail to explicitly and effectively incorporate the alignment between text and image. Supervised finetuning of the universal transformers for MABSA still requires a certain number of aligned image-text pairs. This study proposes a …

* [7] Jianfei Yu, Jing Jiang, Rui Xia. [[_Entity-Sensitive_Attention_and_Fusion_Network_for_Entity-Level_Multimodal_Sentiment_Classification_]]((https://ieeexplore.ieee.org/document/8926404))      
    Entity-level (aka target-dependent) sentiment analysis of social media posts has recently attracted increasing attention, and its goal is to predict the sentiment orientations over individual target entities mentioned in users' posts. Most existing approaches to this task primarily rely on the textual content, but fail to consider the other important data sources (e.g., images, videos, and user profiles), which can potentially enhance these text-based approaches. Motivated by the observation, we study entity-level multimodal sentiment classification in this article, and aim to explore the usefulness of images for entity-level sentiment detection in social media posts. Specifically, we propose an Entity-Sensitive Attention and Fusion Network (ESAFN) for this task. First, to capture the intra-modality dynamics, ESAFN leverages an effective attention mechanism to generate entity-sensitive textual representations, followed by aggregating them with a textual fusion layer. Next, ESAFN learns the entity-sensitive visual representation with an entity-oriented visual attention mechanism, followed by a gated mechanism to eliminate the noisy visual context. Moreover, to capture the inter-modality dynamics, ESAFN further fuses the textual and visual representations with a bilinear interaction layer. To evaluate the effectiveness of ESAFN, we manually annotate the sentiment orientation over each given entity based on two recently released multimodal NER datasets, and show that ESAFN can significantly outperform several highly competitive unimodal and multimodal methods.

* [8] X Yang, S Feng, D Wang, S Qi, W Wu, Y Zhang…. [[_Few-shot Joint Multimodal Aspect-Sentiment Analysis Based on Generative Multimodal Prompt_]](https://arxiv.org/abs/2305.10169)      
    We have witnessed the rapid proliferation of multimodal data on numerous social media platforms. Conventional studies typically require massive labeled data to train models for Multimodal Aspect-Based Sentiment Analysis (MABSA). However, collecting and annotating fine-grained multimodal data for MABSA is tough. To alleviate the above issue, we perform three MABSA-related tasks with quite a small number of labeled multimodal samples. We first build diverse and comprehensive multimodal few-shot datasets according to the data …

* [9] X Yang, S Feng, D Wang, S Qi, W Wu, Y Zhang…. [[_Few-shot joint multimodal aspect-sentiment analysis based on generative_]](https://arxiv.org/abs/2305.10169)      
    We have witnessed the rapid proliferation of multimodal data on numerous social media platforms. Conventional studies typically require massive labeled data to train models for Multimodal Aspect-Based Sentiment Analysis (MABSA). However, collecting and annotating fine-grained multimodal data for MABSA is tough. To alleviate the above issue, we perform three MABSA-related tasks with quite a small number of labeled multimodal samples. We first build diverse and comprehensive multimodal few-shot datasets according to the data …

* [10] Jun Zhao, Fuping Yang. [[_Fusion_with_GCN_and_SE-ResNeXt_Network_for_Aspect_Based_Multimodal_Sentiment_Analysis_]](https://ieeexplore.ieee.org/document/10082618)      
    Aspect Based Multimodal Sentiment Analysis to determine the sentiment polarity of each aspect mentioned in multimodal posts or comments. However, many models fail to explain the related syntactic constraints and long-term word dependencies, as well as lack image feature extraction. In view of the above problems, we propose a Fusion with GCN and SE-ResNeXt Network(FGSN), which constructs a graph convolution network on the dependency tree of sentences to obtain the context representation and aspects words representation by using syntactic information and word dependency. Also, we use the pretrained network SE-ResNeXt101 to extract image features and generate visual representations. Moreover, the position attention and channel attention mechanism are used to obtain the image features. Finally, we fuse image features and text features to classify sentiment polarity. The experimental results show that our model performs well on the two datasets of TWITTER-2015 and TWITTER-2017.

* [11] Junjie Xu, Shuwen Yang, Luwei Xiao, Zhichao Fu, Xingjiao Wu, Tianlong Ma,... . [[_Graph_Convolution_over_the_Semantic-syntactic_Hybrid_Graph_Enhanced_by_Affective_Knowledge_for_Aspect-level_Sentiment_Classification_]](https://ieeexplore.ieee.org/document/9892027)      
    Aspect-level sentiment classification (ASC), detecting and predicting the sentiment polarity of the given aspecs, has attracted increasing attention in the field of Natural Language Processing (NLP). Recent studies in ASC leveraged the graph based on the dependency tree of the context to incorporate the syntactic information and structure of a sentence for better relation extraction. Some researchers noted that existing methods ignored semantic relations or failed to consider affective dependency information, and then proposed several state-of-art methods tackling the above two limitations. However, these approaches failed to consider both informative relations simultaneously. Therefore, we explore and propose a novel solution based on semantic latent graph and SenticNet to leverage semantic and affective information. Specifically, we build a latent semantic graph based on self-attention networks to parse semantic relations within the contexts. In addition, we utilize affective knowledge from SenticNet to enhance the dependency graphs of sentences. Moreover, we use the gate mechanism to dynamically combine information from both the enhanced dependency graphs and latent semantic graphs. Experimental results on three benchmark datasets illustrate the effectiveness and state-of-the-art performance of our model.

* [12] Jianfei Yu, Kai Chen, Rui Xia. [[_Hierarchical_Interactive_Multimodal_Transformer_for_Aspect-Based_Multimodal_Sentiment_Analysis_]](https://ieeexplore.ieee.org/document/9765342)      
    Aspect-based multimodal sentiment analysis (ABMSA) aims to determine the sentiment polarities of each aspect or entity mentioned in a multimodal post or review. Previous studies to ABMSA can be summarized into two subtasks: aspect-term based multimodal sentiment classification (ATMSC) and aspect-category based multimodal sentiment classification (ACMSC). However, these existing studies have three shortcomings: (1) ignoring the object-level semantics in images; (2) primarily focusing on aspect-text and aspect-image interactions; (3) failing to consider the semantic gap between text and image representations. To tackle these issues, we propose a general Hierarchical Interactive Multimodal Transformer (HIMT) model for ABMSA. Specifically, we extract salient features with semantic concepts from images via an object detection method, and then propose a hierarchical interaction module to first model the aspect-text and aspect-image interactions, followed by capturing the text-image interactions. Moreover, an auxiliary reconstruction module is devised to largely eliminate the semantic gap between text and image representations. Experimental results show that our HIMT model significantly outperforms state-of-the-art methods on two benchmarks for ATMSC and one benchmark for ACMSC.

* [13] K Zhang, K Zhang, M Zhang, H Zhao, Q Liu…. [[_Incorporating Dynamic Semantics into Pre-Trained Language Model for Aspect-based Sentiment Analysis_]](https://arxiv.org/abs/2203.16369)      
    … To equip the pre-trained models with the ability to capture … dynamic semantics, we present 
a Dynamic Re-weighting BERT (DRBERT) model, which considers the aspect-aware dynamic …

* [14] X Ju, D Zhang, R Xiao, J Li, S Li…. [[_Joint Multi-modal Aspect-Sentiment Analysis with Auxiliary Cross-modal Relation Detection_]](https://aclanthology.org/2021.emnlp-main.360/)      
    Aspect terms extraction (ATE) and aspect sentiment classification (ASC) are two fundamental and fine-grained sub-tasks in aspect-level sentiment analysis (ALSA). In the textual analysis, joint extracting both aspect terms and sentiment polarities has been drawn much attention due to the better applications than individual sub-task. However, in the multi-modal scenario, the existing studies are limited to handle each sub-task independently, which fails to model the innate connection between the above two objectives and ignores …

* [15] X Ju, D Zhang, R Xiao, J Li, S Li…. [[_Joint Multi-modal Aspect-Sentiment Analysis with Auxiliary Cross-modal_]](https://aclanthology.org/2021.emnlp-main.360/)      
    Aspect terms extraction (ATE) and aspect sentiment classification (ASC) are two fundamental and fine-grained sub-tasks in aspect-level sentiment analysis (ALSA). In the textual analysis, joint extracting both aspect terms and sentiment polarities has been drawn much attention due to the better applications than individual sub-task. However, in the multi-modal scenario, the existing studies are limited to handle each sub-task independently, which fails to model the innate connection between the above two objectives and ignores …

* [16] F Zhao, C Li, Z Wu, Y Ouyang, J Zhang…. [[_M2DF Multi-grained Multi-curriculum Denoising Framework for_]](https://arxiv.org/abs/2310.14605)      
    Multimodal Aspect-based Sentiment Analysis (MABSA) is a fine-grained Sentiment Analysis task, which has attracted growing research interests recently. Existing work mainly utilizes image information to improve the performance of MABSA task. However, most of the studies overestimate the importance of images since there are many noise images unrelated to the text in the dataset, which will have a negative impact on model learning. Although some work attempts to filter low-quality noise images by setting thresholds, relying on thresholds …

* [17] F Zhao, C Li, Z Wu, Y Ouyang, J Zhang…. [[_M2DF- Multi-grained Multi-curriculum Denoising Framework for Multimodal Aspect-based Sentiment Analysis_]](https://arxiv.org/abs/2310.14605)      
    Multimodal Aspect-based Sentiment Analysis (MABSA) is a fine-grained Sentiment Analysis task, which has attracted growing research interests recently. Existing work mainly utilizes image information to improve the performance of MABSA task. However, most of the studies overestimate the importance of images since there are many noise images unrelated to the text in the dataset, which will have a negative impact on model learning. Although some work attempts to filter low-quality noise images by setting thresholds, relying on thresholds …

* [18] J Zhou, J Zhao, JX Huang, QV Hu, L He. [[_MASAL A large-scale dataset for multimodal aspect-based sentiment_]](https://www.sciencedirect.com/science/article/pii/S0925231221007931)      
    Aspect-based sentiment analysis has obtained great success in recent years. Most of the existing work focuses on determining the sentiment polarity of the given aspect according to the given text, while little attention has been paid to the visual information as well as multimodality content for aspect-based sentiment analysis. Multimodal content is becoming increasingly popular in mainstream online social platforms and can help better extract user sentiments toward a given aspect. There are only few studies focusing on this new task …

* [19]  PictureCarol Xu, PictureXuan Luo, PictureDan Wang. [[_MMCPR: A Chinese Product Review Dataset for Multimodal Aspect-Based Sentiment Analysis_]]((https://dl.acm.org/doi/abs/10.1007/978-3-031-23585-6_8))      
    Aspect-based sentiment analysis (ABSA), which aims to analyze the sentiments toward the extracted aspects, has been attracting considerable interest in the last decade. Most of the existing studies concentrate on determining the sentiment polarity of the given aspect according to only textual content, while there is little research on multimodal aspect-based sentiment analysis (MABSA) due to the scarcity of datasets consisting of multimodality content, such as both texts and images. In this paper, we design and construct a Multimodal Chinese Product Review dataset (MCPR) to support the research of MABSA. MCPR is a collection of 1.5k product reviews involving clothing and furniture departments, from the e-commercial platform JD.com. After aspect-base sentiment annotation and text-image matching, we obtain 2,719 text-image pairs and 610 distinct aspects in total. It is the first aspect-based multimodal Chinese product review dataset.

* [20] Jie Mu, Feiping Nie, Wei Wang, Jian Xu, Jing Zhang, Han Liu. [[_MOCOLNet_A_Momentum_Contrastive_Learning_Network_for_Multimodal_Aspect-Level_Sentiment_Analysis_]](MOCOLNet_A_Momentum_Contrastive_Learning_Network_for_Multimodal_Aspect-Level_Sentiment_Analysis)      
    Multimodal aspect-level sentiment analysis has attracted increasing attention in recent years. However, existing methods have two unaddressed limitations: (1) due to the lack of labelled pre-training data of dedicated sentiment analysis, the methods with a pre-training manner produce suboptimal prediction results; (2) most existing methods employ a self-attention encoder to fuse multimodal tokens, which not only ignores the alignment relationship between different modal tokens but also makes the model unable to capture the semantic links between images and texts. In this paper, we propose a momentum contrastive learning network (MOCOLNet) to overcome above limitations. First, we merge the pre-training stage with the training stage to design an end-to-end training manner which uses less labelled data dedicated to sentiment analysis to obtain better prediction results. Second, we propose a multimodal contrastive learning method to align the different modal representations before data fusing, and design a cross-modal matching strategy to provide semantic interactive information between texts and images. Moreover, we introduce an auxiliary momentum strategy to increase the robustness of model. We also analyse the effectiveness of the proposed multimodal contrastive learning method using a mutual information theory. Experiments verify that the proposed MOCOLNet is superior to other strong baselines.

* [21] Z Zhang, Z Wang, X Li, N Liu, B Guo, Z Yu. [[_ModalNet_an Aspect-level sentiment classification model by exploring multimodal data with fusion discriminant attentional networks_]](https://link.springer.com/article/10.1007/s11280-021-00955-7)      
    Aspect-level sentiment classification aims to identify sentiment polarity over each aspect of a sentence. In the past, such analysis tasks mainly relied on text data. Nowadays, due to the popularization of smart devices and Internet services, people are generating more abundant data, including text, image, video, et al. Multimodal data from the same post (eg, a tweet) usually has certain correlation. For example, image data might has an auxiliary effect on the text data, and reasonable processing of such multimodal data can help obtain much richer …

* [22] H Jin, J Tan, L Liu, L Qiu, S Yao, X Chen…. [[_MSRA- A Multi-Aspect Semantic Relevance Approach for E-Commerce via Multimodal Pre-Training_]](https://dl.acm.org/doi/abs/10.1145/3583780.3615224)      
    To enhance the effectiveness of matching user requests with millions of online products, practitioners invest significant efforts in developing semantic relevance models on large-scale e-commerce platforms. Generally, such semantic relevance models are formulated as text-matching approaches, which measure the relevance between users' search queries and the titles of candidate items (ie, products). However, we argue that conventional relevance methods may lead to sub-optimal performance due to the limited information provided by the …

* [23] J Yang, Y Xiao, X Du. [[_Multi-grained fusion network with self-distillation for aspect-based multimodal sentiment analysis_]](https://www.sciencedirect.com/science/article/pii/S0950705124003599)      
    Aspect-based multimodal sentiment analysis (ABMSA) is an important branch of multimodal sentiment analysis. The goal of ABMSA is to use multimodal information to infer users' sentiment polarity toward the targeted aspect for supporting corresponding decision-making. The existing ABMSA methods usually focus on exploring aspect-aware fine-grained interactions and demonstrate the benefits of integrating multimodal information. However, such approaches still suffer from the following limitations:(1) coarse-grained semantic …

* [24] N Xu, W Mao, G Chen. [[_Multi-Interactive Memory Network for Aspect Based Multimodal Sentiment Analysis_]](https://ojs.aaai.org/index.php/AAAI/article/view/3807)      
    As a fundamental task of sentiment analysis, aspect-level sentiment analysis aims to identify the sentiment polarity of a specific aspect in the context. Previous work on aspect-level sentiment analysis is text-based. With the prevalence of multimodal user-generated content (eg text and image) on the Internet, multimodal sentiment analysis has attracted increasing research attention in recent years. In the context of aspect-level sentiment analysis, multimodal data are often more important than text-only data, and have various correlations …

* [25] N Xu, W Mao, G Chen. [[_Multi_Interactive Memory Network for Aspect Based Multimodal Sentiment Analysis_]](https://ojs.aaai.org/index.php/AAAI/article/view/3807)      
    As a fundamental task of sentiment analysis, aspect-level sentiment analysis aims to identify the sentiment polarity of a specific aspect in the context. Previous work on aspect-level sentiment analysis is text-based. With the prevalence of multimodal user-generated content (eg text and image) on the Internet, multimodal sentiment analysis has attracted increasing research attention in recent years. In the context of aspect-level sentiment analysis, multimodal data are often more important than text-only data, and have various correlations …

* [26] M Anschütz, T Eder, G Groh. [[_Retrieving Users’ Opinions on Social Media with Multimodal Aspect-Based Sentiment Analysis_]](https://ieeexplore.ieee.org/abstract/document/10066699/)      
    People post their opinions and experiences on social media, yielding rich databases of end-users' sentiments. This paper shows to what extent machine learning can analyze and structure these databases. An automated data analysis pipeline is deployed to provide insights into user-generated content for researchers in other domains. First, the domain expert can select an image and a term of interest. Then, the pipeline uses image retrieval to find all images showing similar content and applies aspect-based sentiment analysis to …

* [27] J Yu, J Wang, R Xia, J Li. [[_Targeted Multimodal Sentiment Classification Based on_]](https://www.ijcai.org/proceedings/2022/0622.pdf)      
    Abstract Targeted Multimodal Sentiment Classification (TMSC) aims to identify the sentiment polarities over each target mentioned in a pair of sentence and image. Existing methods to TMSC failed to explicitly capture both coarse-grained and fine-grained image-target matching, including 1) the relevance between the image and the target and 2) the alignment between visual objects and the target. To tackle this issue, we propose a new multi-task learning architecture named coarse-to-fine grained Image-Target Matching network (ITM) …

* [28] Donghong Gu, Jiaqian Wang, Shaohua Cai, Chi Yang, Zhengxin Song. [[_Targeted_Aspect-Based_Multimodal_Sentiment_Analysis_An_Attention_Capsule_Extraction_and_Multi-Head_Fusion_Network_]](https://ieeexplore.ieee.org/document/9606882)      
    Multimodal sentiment analysis has currently identified its significance in a variety of domains. For the purpose of sentiment analysis, different aspects of distinguishing modalities, which correspond to one target, are processed and analyzed. In this work, the researchers propose the targeted aspect-based multimodal sentiment analysis (TABMSA) for the first time. Furthermore, an attention capsule extraction and multi-head fusion network (EF-Net) on the task of TABMSA is devised. The multi-head attention (MHA) based network and the ResNet-152 are employed to deal with texts and images, respectively. The integration of MHA and capsule network aims to capture the interaction among the multimodal inputs. In addition to the targeted aspect, the information from the context and the image is also incorporated for sentiment delivered. The researchers evaluate the proposed model on two manually annotated datasets. the experimental results demonstrate the effectiveness of our proposed model for this new task.

* [29] Y Ling, J Yu, R Xia. [[_Vision-Language Pre-Training for Multimodal Aspect-Based Sentiment Analysis 2_]](https://arxiv.org/abs/2204.07955)      
    As an important task in sentiment analysis, Multimodal Aspect-Based Sentiment Analysis (MABSA) has attracted increasing attention in recent years. However, previous approaches either (i) use separately pre-trained visual and textual models, which ignore the crossmodal alignment or (ii) use vision-language models pre-trained with general pre-training tasks, which are inadequate to identify finegrained aspects, opinions, and their alignments across modalities. To tackle these limitations, we propose a task-specific Vision-Language Pre …

* [30] Y Ling, J Yu, R Xia. [[_Vision-Language Pre-Training for Multimodal Aspect-Based Sentiment Analysis_]](https://arxiv.org/abs/2204.07955)      
    As an important task in sentiment analysis, Multimodal Aspect-Based Sentiment Analysis (MABSA) has attracted increasing attention in recent years. However, previous approaches either (i) use separately pre-trained visual and textual models, which ignore the crossmodal alignment or (ii) use vision-language models pre-trained with general pre-training tasks, which are inadequate to identify finegrained aspects, opinions, and their alignments across modalities. To tackle these limitations, we propose a task-specific Vision-Language Pre …

* [31] QT Truong, HW Lauw. [[_VistaNet_Visual Aspect Attention Network for Multimodal Sentiment Analysis_]](https://ojs.aaai.org/index.php/AAAI/article/view/3799)      
    Detecting the sentiment expressed by a document is a key task for many applications, eg, modeling user preferences, monitoring consumer behaviors, assessing product quality. Traditionally, the sentiment analysis task primarily relies on textual content. Fueled by the rise of mobile phones that are often the only cameras on hand, documents on the Web (eg, reviews, blog posts, tweets) are increasingly multimodal in nature, with photos in addition to textual content. A question arises whether the visual component could be useful for …

* [32] D Lu, L Neves, V Carvalho, N Zhang…. [[_Visual Attention Model for Name Tagging in Multimodal Social Media_]](https://aclanthology.org/P18-1185/)      
    Everyday billions of multimodal posts containing both images and text are shared in social media sites such as Snapchat, Twitter or Instagram. This combination of image and text in a single message allows for more creative and expressive forms of communication, and has become increasingly common in such sites. This new paradigm brings new challenges for natural language understanding, as the textual component tends to be shorter, more informal, and often is only understood if combined with the visual context. In this paper, we …










#### _MECPE_ ####
* [1] W Li, Y Li, V Pandelea, M Ge, L Zhu…. [[_Cause Pair Extraction in Conversations_]](https://ieeexplore.ieee.org/abstract/document/9926166/)      
    Conversational sentiment analysis (CSA) and emotion-cause pair extraction (ECPE) tasks have attracted increasing attention in recent years. The former aims to predict the sentiment states of speakers in a conversation, and the latter is about extracting emotion-cause clauses in a document. However, one drawback of CSA is that it cannot model the causal reasoning among emotion and neutral utterances from different speakers. In this work, we propose a new task: emotion-cause pair extraction in conversations (ECPEC), which aims to …

* [2] SV Mathur, AR Jindal, H Mittal…. [[_Exploring Multimodal Emotion Cause Pair Extraction as Sequence Labelling Task_]](https://arxiv.org/abs/2404.02088)      
    Conversation is the most natural form of human communication, where each utterance can range over a variety of possible emotions. While significant work has been done towards the detection of emotions in text, relatively little work has been done towards finding the cause of the said emotions, especially in multimodal settings. SemEval 2024 introduces the task of Multimodal Emotion Cause Analysis in Conversations, which aims to extract emotions reflected in individual utterances in a conversation involving multiple modalities (textual …

* [3] F Wang, Z Ding, R Xia, Z Li, J Yu. [[_Cause Pair Extraction in Conversations_]](https://ieeexplore.ieee.org/abstract/document/9969873/)      
    Conversation is an important form of human communication and contains a large number of emotions. It is interesting to discover emotions and their causes in conversations. Conversation in its natural form is multimodal. Many studies have been carried out on multimodal emotion recognition in conversations, yet there is still a lack of work on multimodal emotion cause analysis. In this article, we introduce a new task named Multimodal Emotion-Cause Pair Extraction in Conversations, aiming to jointly extract …



#### _MERC_ ####
* [1] S Harata, T Sakuma, S Kato. [[_978-3-031-06388-6_18_]](https://link.springer.com/chapter/10.1007/978-3-031-06388-6_18)      
    In Affective Computing, a mathematical representation of emotions in the computer is 
desirable for emotionally interactive agents. This study aims to obtain a latent representation of …

* [2] FA Rahman, G Lu. [[_A Contextualized Real-Time Multimodal Emotion Recognition for Conversational Agents using Graph Convolutional Networks in Reinforcement Learning_]](https://arxiv.org/abs/2310.18363)      
    Owing to the recent developments in Generative Artificial Intelligence (GenAI) and Large Language Models (LLM), conversational agents are becoming increasingly popular and accepted. They provide a human touch by interacting in ways familiar to us and by providing support as virtual companions. Therefore, it is important to understand the user's emotions in order to respond considerately. Compared to the standard problem of emotion recognition, conversational agents face an additional constraint in that recognition must be real-time …

* [3] W Zheng, J Yu, R Xia, S Wang. [[_A Facial Expression-Aware Multimodal Multi-task Learning Framework for Emotion Recognition in Multi-party Conversations_]](https://aclanthology.org/2023.acl-long.861/)      
    Abstract Multimodal Emotion Recognition in Multiparty Conversations (MERMC) has recently attracted considerable attention. Due to the complexity of visual scenes in multi-party conversations, most previous MERMC studies mainly focus on text and audio modalities while ignoring visual information. Recently, several works proposed to extract face sequences as visual features and have shown the importance of visual information in MERMC. However, given an utterance, the face sequence extracted by previous methods …

* [4] Y Zhang, J Wang, Y Liu, L Rong, Q Zheng, D Song…. [[_A Multitask learning model for multimodal sarcasm, sentiment and emotion recognition in conversations_]](https://www.sciencedirect.com/science/article/pii/S1566253523000040)      
    Sarcasm, sentiment and emotion are tightly coupled with each other in that one helps the understanding of another, which makes the joint recognition of sarcasm, sentiment and emotion in conversation a focus in the research in artificial intelligence (AI) and affective computing. Three main challenges exist: Context dependency, multimodal fusion and multitask interaction. However, most of the existing works fail to explicitly leverage and model the relationships among related tasks. In this paper, we aim to generically address …

* [5] M Sharafi, M Yazdchi, R Rasti, F Nasimi. [[_A novel spatio-temporal convolutional neural framework for multimodal emotion recognition_]](https://www.sciencedirect.com/science/article/pii/S1746809422004694)      
    Proposing a practical method for high-performance emotion recognition could facilitate human–computer interaction. Among existing methods, deep learning techniques have improved the performance of emotion recognition systems. In this work, a new multimodal neural design is presented wherein audio and visual data are combined as the input to a hybrid network comprised of a bidirectional long short term memory (BiLSTM) network and two convolutional neural networks (CNNs). The spatial and temporal features extracted from …

* [6] B Pan, K Hirota, Z Jia, Y Dai. [[_A review of multimodal emotion recognition from datasets, preprocessing, features, and fusion methods_]](https://www.sciencedirect.com/science/article/pii/S092523122300989X)      
    Affective computing is one of the most important research fields in modern human–computer interaction (HCI). The goal of affective computing is to study and develop the theories, methods, and systems that can recognize, explain, process, and simulate human emotions. As a branch of affective computing, emotion recognition aims to enlighten the machine/computer automatically analyzing human emotions, which has received increasing attention from researchers in various fields. Human beings generally observe and …

* [7] DS Chauhan, GV Singh, A Arora, A Ekbal…. [[_A Sentiment and Emotion aware Multimodal Multiparty Humor Recognition in Multilingual Conversational Setting_]](https://aclanthology.org/2022.coling-1.587/)      
    In this paper, we hypothesize that humor is closely related to sentiment and emotions. Also, due to the tremendous growth in multilingual content, there is a great demand for building models and systems that support multilingual information access. To end this, we first extend the recently released Multimodal Multiparty Hindi Humor (M2H2) dataset by adding parallel English utterances corresponding to Hindi utterances and then annotating each utterance with sentiment and emotion classes. We name it Sentiment, Humor, and Emotion aware …

* [8] X Li, J Song, Z Zhao, C Wang…. [[_A SUPERVISED INFORMATION ENHANCED MULTI-GRANULARITY CONTRASTIVE LEARNING FRAMEWORK FOR EEG BASED EMOTION RECOGNITION_]](https://ieeexplore.ieee.org/abstract/document/10447740/)      
    This study introduces a novel Supervised Info-enhanced Contrastive Learning framework for EEG based Emotion Recognition (SI-CLEER). SI-CLEER employs multi-granularity contrastive learning to create robust EEG contextual representations, potentially improving emotion recognition effectiveness. Unlike existing methods solely guided by classification loss, we propose a joint learning model combining self-supervised contrastive learning loss and supervised classification loss. This model optimizes both loss functions, capturing subtle …

* [9] H Ma, J Wang, H Lin, B Zhang…. [[_A Transformer-based Model with Self-distillation for Multimodal Emotion Recognition in Conversations_]](https://ieeexplore.ieee.org/abstract/document/10109845/)      
    Emotion recognition in conversations (ERC), the task of recognizing the emotion of each utterance in a conversation, is crucial for building empathetic machines. Existing studies focus mainly on capturing context-and speaker-sensitive dependencies on the textual modality but ignore the significance of multimodal information. Different from emotion recognition in textual conversations, capturing intra-and inter-modal interactions between utterances, learning weights between different modalities, and enhancing modal …

* [10] K Ali, CE Hughes. [[_A Unified Transformer-based Network for Multimodal Emotion Recognition_]](https://arxiv.org/abs/2308.14160)      
    The development of transformer-based models has resulted in significant advances in addressing various vision and NLP-based research challenges. However, the progress made in transformer-based methods has not been effectively applied to biosensing research. This paper presents a novel Unified Biosensor-Vision Multi-modal Transformer-based (UBVMT) method to classify emotions in an arousal-valence space by combining a 2D representation of an ECG/PPG signal with the face information. To achieve this goal, we …

* [11] J Vazquez-Rodriguez, G Lefebvre…. [[_ACCOMMODATING MISSING MODALITIES IN TIME-CONTINUOUS MULTIMODAL EMOTION RECOGNITION_]](https://ieeexplore.ieee.org/abstract/document/10388079/)      
    Decades of research indicate that emotion recognition is more effective when drawing information from multiple modalities. But what if some modalities are sometimes missing? To address this problem, we propose a novel Transformer-based architecture for recognizing valence and arousal in a time-continuous manner even with missing input modalities. We use a coupling of cross-attention and self-attention mechanisms to emphasize relationships between modalities during time and enhance the learning process on weak salient inputs …

* [12] S Poria, D Hazarika, N Majumder, G Naik…. [[_A Multimodal Multi-Party Dataset for Emotion Recognition in Conversations_]](https://arxiv.org/abs/1810.02508)      
    Emotion recognition in conversations is a challenging task that has recently gained popularity due to its potential applications. Until now, however, a large-scale multimodal multi-party emotional conversational database containing more than two speakers per dialogue was missing. Thus, we propose the Multimodal EmotionLines Dataset (MELD), an extension and enhancement of EmotionLines. MELD contains about 13,000 utterances from 1,433 dialogues from the TV-series Friends. Each utterance is annotated with emotion and …

* [13] JA Miranda-Correa, MK Abadi…. [[_A Dataset for Affect, Personality and Mood Research on Individuals and Groups_]](https://ieeexplore.ieee.org/abstract/document/8554112/)      
    We present AMIGOS-A dataset for Multimodal research of affect, personality traits and mood on Individuals and GrOupS. Different to other databases, we elicited affect using both short and long videos in two social contexts, one with individual viewers and one with groups of viewers. The database allows the multimodal study of the affective responses, by means of neuro-physiological signals of individuals in relation to their personality and mood, and with respect to the social context and videos' duration. The data is collected in two experimental …

* [14] NK Devulapally, S Anand, SD Bhattacharjee…. [[_Adaptive Multimodal Analysis for Speaker Emotion Recognition in Group Conversations_]](https://arxiv.org/abs/2401.15164)      
    Analyzing individual emotions during group conversation is crucial in developing intelligent agents capable of natural human-machine interaction. While reliable emotion recognition techniques depend on different modalities (text, audio, video), the inherent heterogeneity between these modalities and the dynamic cross-modal interactions influenced by an individual's unique behavioral patterns make the task of emotion recognition very challenging. This difficulty is compounded in group settings, where the emotion and its …

* [15] Fei Ma, Shao-Lun Huang, Lin Zhang. [[_An_Efficient_Approach_for_Audio-Visual_Emotion_Recognition_With_Missing_Labels_And_Missing_Modalities_]](https://ieeexplore.ieee.org/document/9428219)      
    Audio-visual emotion recognition is important for human-machine interaction systems by combining the information of audio and visual modalities. Although great progress has been made by previous works using multimodal learning compared with unimodal learning, they still cannot effectively deal with two key challenges. Firstly, it is difficult or expensive to acquire labeled emotional data, which results in a large amount of data with missing labels. Secondly, emotional data often has missing modalities. To address these problems, we propose a unified deep learning framework to efficiently handle missing labels and missing modalities for audio-visual emotion recognition through correlation analysis. Specifically, we consider four types of emotional data during the training stage: complete, label missing, visual missing, and audio missing. We propose a correlation loss based on Hirschfeld-Gebelein-Ŕenyi (HGR) maximal correlation to effectively capture the common information in different types of training data for emotion prediction. Experiments on the eNTERFACE’05 and RAVDESS datasets show that our deep learning approach has high effectiveness for audio-visual emotion recognition.

* [16] Hui Ma, Jian Wang, Hongfei Lin, Bo Zhang, Yijia Zhang, Bo Xu. [[_A_Transformer-Based_Model_With_Self-Distillation_for_Multimodal_Emotion_Recognition_in_Conversations_]](https://ieeexplore.ieee.org/document/10109845)      
    Emotion recognition in conversations (ERC), the task of recognizing the emotion of each utterance in a conversation, is crucial for building empathetic machines. Existing studies focus mainly on capturing context- and speaker-sensitive dependencies on the textual modality but ignore the significance of multimodal information. Different from emotion recognition in textual conversations, capturing intra- and inter-modal interactions between utterances, learning weights between different modalities, and enhancing modal representations play important roles in multimodal ERC. In this paper, we propose a transformer-based model with self-distillation (SDT)The code is available at https://github.com/butterfliesss/SDT.for the task. The transformer-based model captures intra- and inter-modal interactions by utilizing intra- and inter-modal transformers, and learns weights between modalities dynamically by designing a hierarchical gated fusion strategy. Furthermore, to learn more expressive modal representations, we treat soft labels of the proposed model as extra training supervision. Specifically, we introduce self-distillation to transfer knowledge of hard and soft labels from the proposed model to each modality. Experiments on IEMOCAP and MELD datasets demonstrate that SDT outperforms previous state-of-the-art baselines.

* [17] A Jia, Y He, Y Zhang, S Uprety, D Song…. [[_A Multi-Modal Dataset for Human Desire Understanding_]](https://aclanthology.org/2022.naacl-main.108/)      
    Desire is a strong wish to do or have something, which involves not only a linguistic expression, but also underlying cognitive phenomena driving human feelings. As the most primitive and basic human instinct, conscious desire is often accompanied by a range of emotional responses. As a strikingly understudied task, it is difficult for machines to model and understand desire due to the unavailability of benchmarking datasets with desire and emotion labels. To bridge this gap, we present MSED, the first multi-modal and multi-task …

* [18] X Zhang, Y Li. [[_A Cross-Modality Context Fusion and Semantic Refinement Network for Emotion Recognition in Conversation_]](https://aclanthology.org/2023.acl-long.732/)      
    Emotion recognition in conversation (ERC) has attracted enormous attention for its applications in empathetic dialogue systems. However, most previous researches simply concatenate multimodal representations, leading to an accumulation of redundant information and a limited context interaction between modalities. Furthermore, they only consider simple contextual features ignoring semantic clues, resulting in an insufficient capture of the semantic coherence and consistency in conversations. To address these …

* [19] A Joshi, A Bhat, A Jain, A Singh…. [[_COntextualized GNN based Multimodal Emotion recognitioN_]](https://aclanthology.org/2022.naacl-main.306/)      
    Emotions are an inherent part of human interactions, and consequently, it is imperative to develop AI systems that understand and recognize human emotions. During a conversation involving various people, a person's emotions are influenced by the other speaker's utterances and their own emotional state over the utterances. In this paper, we propose COntextualized Graph Neural Network based Multi-modal Emotion recognitioN (COGMEN) system that leverages local information (ie, inter/intra dependency between speakers) and …

* [20] Yahui Fu, Shogo Okada, Longbiao Wang, Lili Guo, Yaodong Song. [[_Context-_and_Knowledge-Aware_Graph_Convolutional_Network_for_Multimodal_Emotion_Recognition_]](Context-_and_Knowledge-Aware_Graph_Convolutional_Network_for_Multimodal_Emotion_Recognition)      
    This work proposes an approach for emotion recognition in conversation that leverages context modeling, knowledge enrichment, and multimodal (text and audio) learning based on a graph convolutional network (GCN). We first construct two distinctive graphs for modeling the contextual interaction and knowledge dynamic. We then introduce an affective lexicon into knowledge graph building to enrich the emotional polarity of each concept, that is the related knowledge of each token in an utterance. Then, we achieve a balance between the context and the affect-enriched knowledge by incorporating them into the new adjacency matrix construction of the GCN architecture, and teach them jointly with multiple modalities to effectively structure the semantics-sensitive and knowledge-sensitive contextual dependence of each conversation. Our model outperforms the state-of-the-art benchmarks by over 22.6% and 11% relative error reduction in terms of weighted-F1 on the IEMOCAP and MELD databases, respectively, demonstrating the superiority of our method in emotion recognition.

* [21] WY Choi, KY Song, CW Lee. [[_Convolutional Attention Networks for Multimodal Emotion Recognition from Speech and Text Data_]](https://aclanthology.org/W18-3304/)      
    Emotion recognition has become a popular topic of interest, especially in the field of human computer interaction. Previous works involve unimodal analysis of emotion, while recent efforts focus on multimodal emotion recognition from vision and speech. In this paper, we propose a new method of learning about the hidden representations between just speech and text data using convolutional attention networks. Compared to the shallow model which employs simple concatenation of feature vectors, the proposed attention model performs …

* [22] Y Wang, Y Li, P Bell, C Lai - researchgate.net. [[_Incongruity-Aware Dynamic Hierarchical Fusion for Multimodal Affect Recognition_]](https://www.researchgate.net/profile/Yuanchao-Li-5/publication/370981780_Cross-Attention_is_Not_Enough_Incongruity-Aware_Multimodal_Sentiment_Analysis_and_Emotion_Recognition/links/64a48b888de7ed28ba74a8aa/Cross-Attention-is-Not-Enough-Incongruity-Aware-Multimodal-Sentiment-Analysis-and-Emotion-Recognition.pdf)      
    Fusing multiple modalities for affective computing tasks has proven effective for performance improvement. However, how multimodal fusion works is not well understood, and its use in the real world usually results in large model sizes. In this work, on sentiment and emotion analysis, we first analyze how the salient affective information in one modality can be affected by the other in crossmodal attention. We find that inter-modal incongruity exists at the latent level due to crossmodal attention. Based on this finding, we propose a lightweight …

* [23] SAM Zaidi, S Latif, J Qadi. [[_Cross-Language Speech Emotion Recognition Using Multimodal Dual Attention Transformers_]](https://arxiv.org/abs/2306.13804)      
    Despite the recent progress in speech emotion recognition (SER), state-of-the-art systems are unable to achieve improved performance in cross-language settings. In this paper, we propose a Multimodal Dual Attention Transformer (MDAT) model to improve cross-language SER. Our model utilises pre-trained models for multimodal feature extraction and is equipped with a dual attention mechanism including graph attention and co-attention to capture complex dependencies across different modalities and achieve improved cross …

* [24] Y Zhang, H Liu, D Wang, D Zhang, T Lou…. [[_Cross-modal credibility modelling for EEG-based multimodal emotion recognition_]](https://iopscience.iop.org/article/10.1088/1741-2552/ad3987/meta)      
    Objective. The study of emotion recognition through electroencephalography (EEG) has garnered significant attention recently. Integrating EEG with other peripheral physiological signals may greatly enhance performance in emotion recognition. Nonetheless, existing approaches still suffer from two predominant challenges: modality heterogeneity, stemming from the diverse mechanisms across modalities, and fusion credibility, which arises when one or multiple modalities fail to provide highly credible signals. Approach. In this paper, we …

* [25] L Xiao, X Wu, S Yang, J Xu, J Zhou, L He. [[_Cross-modal fine-grained alignment and fusion network for multimodal aspect-based sentiment analysis_]](https://www.sciencedirect.com/science/article/pii/S0306457323002455)      
    Abstract Multi-modal Aspect-based Sentiment Analysis (MABSA) aims to forecast the polarity of sentiment concerning aspects within a given sentence based on the correlation between the sentence and its accompanying image. Comprehending multi-modal sentiment expression requires strong cross-modal alignment and fusion ability. Previous state-of-the-art (SOTA) models fail to explicitly align valuable visual clues with aspect and sentiment information in textual representations and overlook the utilization of syntactic dependency …

* [26] Y Li, Y Wang, Z Cui. [[_Decoupled Multimodal Distilling for Emotion Recognition_]](http://openaccess.thecvf.com/content/CVPR2023/html/Li_Decoupled_Multimodal_Distilling_for_Emotion_Recognition_CVPR_2023_paper.html)      
    Human multimodal emotion recognition (MER) aims to perceive human emotions via language, visual and acoustic modalities. Despite the impressive performance of previous MER approaches, the inherent multimodal heterogeneities still haunt and the contribution of different modalities varies significantly. In this work, we mitigate this issue by proposing a decoupled multimodal distillation (DMD) approach that facilitates flexible and adaptive crossmodal knowledge distillation, aiming to enhance the discriminative features of each …

* [27] F Zhang, XC Li, CP Lim, Q Hua, CR Dong, JH Zhai. [[_Deep Emotional Arousal Network for Multimodal Sentiment Analysis and Emotion Recognition_]](https://www.sciencedirect.com/science/article/pii/S1566253522000653)      
    Multimodal sentiment analysis and emotion recognition has become an increasingly popular research area, where the biggest challenge is to efficiently fuse the input information from different modality. The recent success is largely credited to the attention-based models, eg, transformer and its variants. However, the attention-based mechanism often neglects the coherency of human emotion due to its parallel structure. Inspired by the emotional arousal model in cognitive science, a Deep Emotional Arousal Network (DEAN) that is capable of …

* [28] AI Middya, B Nag, S Roy. [[_Deep learning based multimodal emotion recognition using model-level fusion of audio–visual modalities_]](https://www.sciencedirect.com/science/article/pii/S0950705122002593)      
    Emotion identification based on multimodal data (eg, audio, video, text, etc.) is one of the most demanding and important research fields, with various uses. In this context, this research work has conducted a rigorous exploration of model-level fusion to find out the optimal multimodal model for emotion recognition using audio and video modalities. More specifically, separate novel feature extractor networks for audio and video data are proposed. After that, an optimal multimodal emotion recognition model is created by fusing …

* [29] G Arora, M Sabharwal, P Kapila…. [[_Deep Residual Adaptive Neural Network Based Feature Extraction for Cognitive Computing with Multimodal Sentiment Sensing and Emotion Recognition Process_]](https://search.proquest.com/openview/5b82ac981d0318b3a57fe3bd2e737a49/1?pq-origsite=gscholar&cbl=52057)      
    For the healthcare framework, automatic recognition of patients' emotions is considered to be a good facilitator. Feedback about the status of patients and satisfaction levels can be provided automatically to the stakeholders of the healthcare industry. Multimodal sentiment analysis of human is considered as the attractive and hot topic of research in artificial intelligence (AI) and is the much finer classification issue which differs from other classification issues. In cognitive science, as emotional processing procedure has inspired …

* [30] Y Mao, Q Sun, G Liu, X Wang, W Gao, X Li…. [[_Exploring Multi-Modal Emotion Dynamics in Conversations_]](https://arxiv.org/abs/2010.07637)      
    … information in a conversation, however, ignoring the differentiated emotional behaviors within 
… tiated multi-modal emotional behaviors can produce more accurate emotional predictions. …

* [31] Y Guo, C Tang, H Wu, B Chen. [[_EEG EMOTION RECOGNITION BASED ON DYNAMICAL GRAPH ATTENTION NETWORK_]](https://ieeexplore.ieee.org/abstract/document/10447925/)      
    Emotion recognition based on electroencephalography (EEG) signals is one of the current research challenges in this field. In order to learn the optimal graph structure information for each subject, we propose a dynamic graph attention neural network model. The model utilizes a graph attention neural network as a feature learner, dynamically learning channel connections, and enriching feature representations between channels through global attention. To verify the effectiveness of the proposed method, we conducted experiments on …

* [32] GV Singh, M Firdaus, A Ekbal…. [[_A Multimodal Transformer for Identifying Emotions and Intents in Social Conversations_]](https://ieeexplore.ieee.org/abstract/document/9961847/)      
    In the natural language processing community, open-domain conversational agents, also known as chatbots, are gaining popularity. One of the difficulties is getting them to communicate in an emotionally intelligent manner. To generate dialogues, current neural response generation methods depend solely on end-to-end learning from large scale conversation data. Therefore, we introduce a large-scale multi Emotion and Intent guided Multimodal Dialogue (EmoInt-MD) dataset labelled with 32 emotions and 15 empathetic …

* [33] K Ezzameli, H Mahersia. [[_A review_]](https://www.sciencedirect.com/science/article/pii/S156625352300163X)      
    The omnipresence of numerous information sources () in our () daily lives () brings up new alternatives () for emotion recognition in several domains including e-health, e-learning, robotics, and e-commerce. Due to the variety of data,() the research area of multimodal machine learning poses special problems for computer scientists; how did the field of emotion9 recognition progress in each modality and what are the most common strategies for recognizing emotions? What part does deep () learning play in this? What's …

* [34] J Vazquez-Rodriguez, G Lefebvre…. [[_Emotion Recognition with Pre-Trained Transformers Using Multimodal Signals_]](https://ieeexplore.ieee.org/abstract/document/9953852/)      
    In this paper, we address the problem of multimodal emotion recognition from multiple physiological signals. We demonstrate that a Transformer-based approach is suitable for this task. In addition, we present how such models may be pre-trained in a multimodal scenario to improve emotion recognition performances. We evaluate the benefits of using multimodal inputs and pre-training with our approach on a state-of-the-art dataset.

* [35] D Andreoletti, F Cardoso, A Arzillo, L Luceri… - 2021 - ceur-ws.org. [[_Emotion_Recognition_and_Detection_Methods_A_Compre_]](https://ceur-ws.org/Vol-3116/Paper_2.pdf)      
    The paradigm of Smart Cities is based on the idea of enhancing citizens’ life by means of 
digital technologies. The widespread generation of data is seen as the main enabling factor of …

* [36] Lili Guo, Longbiao Wang, Jianwu Dang, Yahui Fu, Jiaxing Liu. [[_Emotion_Recognition_With_Multimodal_Transformer_Fusion_Framework_Based_on_Acoustic_and_Lexical_Information_]](https://ieeexplore.ieee.org/document/9740502)      
    People usually express emotions through paralinguistic and linguistic information in speech. How to effectively integrate linguistic and paralinguistic information for emotion recognition is a challenge. Previous studies have adopted the bidirectional long short-term memory (BLSTM) network to extract acoustic and lexical representations followed by a concatenate layer, and this has become a common method. However, the interaction and influence between different modalities are difficult to promote using simple feature fusion for each sentence. In this article, we propose an implicitly aligned multimodal transformer fusion (IA-MMTF) framework based on acoustic features and text information. This model enables the two modalities to guide and complement each other when learning emotional representations. Thereafter, the weighed fusion is used to control the contributions of different modalities. Thus, we can obtain more complementary emotional representations. Experiments on the interactive emotional dyadic motion capture (IEMOCAP) database and multimodal emotionlines dataset (MELD) show that the proposed method outperforms the baseline BLSTM-based method.

* [37] MM Islam, S Nooruddin, F Karray…. [[_A deep learning based model-level fusion approach_]](https://www.sciencedirect.com/science/article/pii/S1746809424002994)      
    Deep learning techniques have drawn considerable interest in emotion recognition due to recent technological developments in healthcare analytics. Automatic patient emotion recognition can assist healthcare analytics by providing feedback to the stakeholders of competent healthcare about the conditions of the patients and their satisfaction levels. In this paper, we propose a novel model-level fusion technique based on deep learning for enhanced emotion recognition from multimodal signals to monitor patients in connected …

* [38] H Zuo, R Liu, J Zhao, G Gao, H Li. [[_EXPLOITING MODALITY-INVARIANT FEATURE FOR ROBUST MULTIMODAL EMOTION RECOGNITION WITH MISSING MODALITIES_]](https://ieeexplore.ieee.org/abstract/document/10095836/)      
    Multimodal emotion recognition leverages complementary information across modalities to gain performance. However, we cannot guarantee that the data of all modalities are always present in practice. In the studies to predict the missing data across modalities, the inherent difference between heterogeneous modalities, namely the modality gap, presents a challenge. To address this, we propose to use invariant features for a missing modality imagination network (IF-MMIN) which includes two novel mechanisms: 1) an invariant …

* [39] Haolin Zuo, Rui Liu, Jinming Zhao, Guanglai Gao, Haizhou Li. [[_Exploiting_Modality-Invariant_Feature_for_Robust_Multimodal_Emotion_Recognition_with_Missing_Modalities_]](Exploiting_Modality-Invariant_Feature_for_Robust_Multimodal_Emotion_Recognition_with_Missing_Modalities)      
    Multimodal emotion recognition leverages complementary information across modalities to gain performance. However, we cannot guarantee that the data of all modalities are always present in practice. In the studies to predict the missing data across modalities, the inherent difference between heterogeneous modalities, namely the modality gap, presents a challenge. To address this, we propose to use invariant features for a missing modality imagination network (IF-MMIN) which includes two novel mechanisms: 1) an invariant feature learning strategy that is based on the central moment discrepancy (CMD) distance under the full-modality scenario; 2) an invariant feature based imagination module (IF-IM) to alleviate the modality gap during the missing modalities prediction, thus improving the robustness of multimodal joint representation. Comprehensive experiments on the benchmark dataset IEMOCAP demonstrate that the proposed model outperforms all baselines and invariantly improves the overall emotion recognition performance under uncertain missing-modality conditions. We release the code at: [[_this https URL_]](https://github.com/ZhuoYulang/IF-MMIN).

* [40] Z Fang, A He, Q Yu, B Gao, W Ding, T Zhang…. [[_A novel multimodal emotion recognition approach integrating face, body and text_]](https://arxiv.org/abs/2211.15425)      
    Multimodal emotion analysis performed better in emotion recognition depending on more comprehensive emotional clues and multimodal emotion dataset. In this paper, we developed a large multimodal emotion dataset, named" HED" dataset, to facilitate the emotion recognition task, and accordingly propose a multimodal emotion recognition method. To promote recognition accuracy," Feature After Feature" framework was used to explore crucial emotional information from the aligned face, body and text samples. We …

* [41] X Yang, S Feng, D Wang, S Qi, W Wu, Y Zhang…. [[_Few-shot Joint Multimodal Aspect-Sentiment Analysis Based on Generative Multimodal Prompt_]](https://arxiv.org/abs/2305.10169)      
    We have witnessed the rapid proliferation of multimodal data on numerous social media platforms. Conventional studies typically require massive labeled data to train models for Multimodal Aspect-Based Sentiment Analysis (MABSA). However, collecting and annotating fine-grained multimodal data for MABSA is tough. To alleviate the above issue, we perform three MABSA-related tasks with quite a small number of labeled multimodal samples. We first build diverse and comprehensive multimodal few-shot datasets according to the data …

* [42] YP Ruan, S Han, T Li, Y Wu. [[_FUSING MODALITY-SPECIFIC REPRESENTATIONS AND DECISIONS FOR MULTIMODAL EMOTION RECOGNITION_]](https://ieeexplore.ieee.org/abstract/document/10447035/)      
    Multimodal emotion recognition (MER) is important for building humanoid chatbots and has gained increasing attention in recent years. Existing studies have proven that extracting better modality-specific representations, which keep both commonality and individuality information of different modalities, is important for the MER task. However, all these works are restricted in making final predictions based on fusing modality-specific representations, and the effectiveness of the modality-specific decisions has not been studied. In this paper …

* [43] M Jin, J Li. [[_Learning Deep Representations for Multimodal Emotion Recognition_]](https://dl.acm.org/doi/abs/10.1145/3581783.3612074)      
    Multimodal emotion recognition based on electroencephalogram (EEG) and compensating physiological signals (eg, eye tracking) has shown potential in the diagnosis and rehabilitation tracking of depression. Since the multi-channel EEG signals are generally processed as one-dimensional (1-D) graph-like features, existing approaches can only adopt underdeveloped shallow models to recognize emotions. However, these simple models have difficulty decoupling complex emotion patterns due to their limited …

* [44] J Li, X Wang, G Lv, Z Zeng. [[_A graph network based multimodal fusion technique for emotion recognition in conversation_]](https://www.sciencedirect.com/science/article/pii/S0925231223005507)      
    Multimodal machine learning is an emerging area of research, which has received a great deal of scholarly attention in recent years. Up to now, there are few studies on multimodal Emotion Recognition in Conversation (ERC). Since Graph Neural Networks (GNNs) possess the powerful capacity of relational modeling, they have an inherent advantage in the field of multimodal learning. GNNs leverage the graph constructed from multimodal data to perform intra-and inter-modal information interaction, which effectively facilitates the integration and …

* [45] M Sun, X Zhang, J Ma, S Xie, Y Liu…. [[_A Knowledge-guided Dual-inconsistency Network for Multi-modal Rumor Detection_]](https://ieeexplore.ieee.org/abstract/document/10123962/)      
    Rumor spreaders are increasingly utilizing multimedia content to attract the attention and trust of news consumers. Though quite a few rumor detection models have exploited the multi-modal data, they seldom consider the inconsistent semantics between images and texts, and rarely spot the inconsistency among the post contents and background knowledge. In addition, they commonly assume the completeness of multiple modalities and thus are incapable of handling handle missing modalities in real-life scenarios. Motivated by …

* [46] Haiyang Xu, Hui Zhang, Kun Han, Yun Wang, Yiping Peng, Xiangang Li. [[_Interspeech19_Learning Alignment for Multimodal Emotion Recognition from Speech_]](Interspeech19_Learning Alignment for Multimodal Emotion Recognition from Speech)      
    Speech emotion recognition is a challenging problem because human convey emotions in subtle and complex ways. For emotion recognition on human speech, one can either extract emotion related features from audio signals or employ speech recognition techniques to generate text from speech and then apply natural language processing to analyze the sentiment. Further, emotion recognition will be beneficial from using audio-textual multimodal information, it is not trivial to build a system to learn from multimodality. One can build models for two input sources separately and combine them in a decision level, but this method ignores the interaction between speech and text in the temporal domain. In this paper, we propose to use an attention mechanism to learn the alignment between speech frames and text words, aiming to produce more accurate multimodal feature representations. The aligned multimodal features are fed into a sequential model for emotion recognition. We evaluate the approach on the IEMOCAP dataset and the experimental results show the proposed approach achieves the state-of-the-art performance on the dataset.

* [47] C Ziems, W Held, O Shaikh, J Chen, Z Zhang…. [[_Is GPT a Computational Model of Emotion__]](https://direct.mit.edu/coli/article/doi/10.1162/coli_a_00502/118498)      
    … Plutchik’s model is one of the three most recognized discrete emotion … we include GPT-4 
(OpenAI 2023), which is a multimodal model that, at 1.7 trillion parameters, scales up the GPT-3 …

* [48] D Li, Y Wang, K Funakoshi, M Okumura. [[_Joint Modality Fusion and Graph Contrastive Learning for Multimodal Emotion Recognition_]](https://arxiv.org/abs/2311.11009)      
    Multimodal emotion recognition aims to recognize emotions for each utterance of multiple modalities, which has received increasing attention for its application in human-machine interaction. Current graph-based methods fail to simultaneously depict global contextual features and local diverse uni-modal features in a dialogue. Furthermore, with the number of graph layers increasing, they easily fall into over-smoothing. In this paper, we propose a method for joint modality fusion and graph contrastive learning for multimodal emotion …

* [49] J Sun, S Han, YP Ruan, X Zhang…. [[_Layer-wise Fusion with Modality Independence Modeling for Multi-modal Emotion Recognition_]](https://aclanthology.org/2023.acl-long.39/)      
    Multi-modal emotion recognition has gained increasing attention in recent years due to its widespread applications and the advances in multi-modal learning approaches. However, previous studies primarily focus on developing models that exploit the unification of multiple modalities. In this paper, we propose that maintaining modality independence is beneficial for the model performance. According to this principle, we construct a dataset, and devise a multi-modal transformer model. The new dataset, CHinese Emotion Recognition dataset …

* [50] Z Tang, Q Xiao, X Zhou, Y Li, C Chen, K Li. [[_Learning discriminative multi-relation representations for multimodal sentiment analysis_]](https://www.sciencedirect.com/science/article/pii/S0020025523007107)      
    Modality representation learning is a critical issue in multimodal sentiment analysis (MSA). A good sentiment representation should contain as much effective information as possible while being discriminative enough to be better recognized. Previous attention-based MSA methods mainly rely on word-level feature interactions to capture intra-modality and inter-modality relations, which may lead to the loss of essential sentiment information. Furthermore, they primarily focus on information fusion but do not give enough importance to …

* [51] P Wang, S Zeng, J Chen, L Fan, M Chen, Y Wu…. [[_Leveraging Label Information for Multimodal Emotion Recognition_]](https://arxiv.org/abs/2309.02106)      
    Multimodal emotion recognition (MER) aims to detect the emotional status of a given expression by combining the speech and text information. Intuitively, label information should be capable of helping the model locate the salient tokens/frames relevant to the specific emotion, which finally facilitates the MER task. Inspired by this, we propose a novel approach for MER by leveraging label information. Specifically, we first obtain the representative label embeddings for both text and speech modalities, then learn the label …

* [52] V Chudasama, P Kar, A Gudmalwar…. [[_Multi-modal Fusion Network for Emotion Recognition in Conversation_]](http://openaccess.thecvf.com/content/CVPR2022W/MULA/html/Chudasama_M2FNet_Multi-Modal_Fusion_Network_for_Emotion_Recognition_in_Conversation_CVPRW_2022_paper.html)      
    Abstract Emotion Recognition in Conversations (ERC) is crucial in developing sympathetic human-machine interaction. In conversational videos, emotion can be present in multiple modalities, ie, audio, video, and transcript. However, due to the inherent characteristics of these modalities, multi-modal ERC has always been considered a challenging undertaking. Existing ERC research focuses mainly on using text information in a discussion, ignoring the other two modalities. We anticipate that emotion recognition accuracy can be improved by …

* [53] N Wang, H Cao, J Zhao, R Chen…. [[_Missing-Modality Robust Emotion Recognition Framework With Iterative Data Augmentation_]](https://ieeexplore.ieee.org/abstract/document/9868120/)      
    This article deals with the utterance-level modalities missing problem with uncertain patterns on emotion recognition in conversation (ERC) task. Present models generally predict the speaker's emotions by its current utterance and context, which is degraded by modality missing considerably. Our work proposes a framework missing-modality robust emotion recognition (M2R2), which trains emotion recognition model with iterative data augmentation by learned common representation. First, a network called party attentive network (PANet) is …

* [54] Y Zhang, A Jia, B Wang, P Zhang, D Zhao, P Li…. [[_A Multi-modal, Multi-task Interactive Graph Attention Network for Conversational Sentiment Analysis and Emotion Recognition_]](https://dl.acm.org/doi/abs/10.1145/3593583)      
    Sentiment and emotion, which correspond to long-term and short-lived human feelings, are closely linked to each other, leading to the fact that sentiment analysis and emotion recognition are also two interdependent tasks in natural language processing (NLP). One task often leverages the shared knowledge from another task and performs better when solved in a joint learning paradigm. Conversational context dependency, multi-modal interaction, and multi-task correlation are three key factors that contribute to this joint …

* [55] M Ren, X Huang, J Liu, M Liu, X Li…. [[_Multimodal Adversarial Learning Network for Conversational Emotion Recognition_]](https://ieeexplore.ieee.org/abstract/document/10121331/)      
    Multimodal emotion recognition in conversations (ERC) aims to identify the emotional state of constituent utterances expressed by multiple speakers in dialogue from multimodal data. Existing multimodal ERC approaches focus on modeling the global context of the dialogue and neglect to mine the characteristic information from the corresponding utterances expressed by the same speaker. Additionally, information from different modalities exhibits commonality and diversity for emotional expression. The commonality and diversity of …

* [56] M Firdaus, H Chauhan, A Ekbal…. [[_A Multimodal Multi-Label Emotion, Intensity and Sentiment Dialogue Dataset for Emotion Recognition and Sentiment Analysis in Conversations_]](https://aclanthology.org/2020.coling-main.393/)      
    Emotion and sentiment classification in dialogues is a challenging task that has gained popularity in recent times. Humans tend to have multiple emotions with varying intensities while expressing their thoughts and feelings. Emotions in an utterance of dialogue can either be independent or dependent on the previous utterances, thus making the task complex and interesting. Multi-label emotion detection in conversations is a significant task that provides the ability to the system to understand the various emotions of the users …

* [57] Z Lian, H Sun, L Sun, Z Wen, S Zhang, S Chen…. [[_Semi-Supervised Learning, Noise Robustness, and Open-Vocabulary Multimodal Emotion Recognition_]](https://arxiv.org/abs/2404.17113)      
    Multimodal emotion recognition is an important research topic in artificial intelligence. Over the past few decades, researchers have made remarkable progress by increasing dataset size and building more effective architectures. However, due to various reasons (such as complex environments and inaccurate labels), current systems still cannot meet the demands of practical applications. Therefore, we plan to organize a series of challenges around emotion recognition to further promote the development of this field. Last year, we …

* [58] J Zhao, R Li, Q Jin. [[_Missing Modality Imagination Network for Emotion Recognition with Uncertain Missing Modalities_]](https://aclanthology.org/2021.acl-long.203/)      
    Multimodal fusion has been proved to improve emotion recognition performance in previous works. However, in real-world applications, we often encounter the problem of missing modality, and which modalities will be missing is uncertain. It makes the fixed multimodal fusion fail in such cases. In this work, we propose a unified model, Missing Modality Imagination Network (MMIN), to deal with the uncertain missing modality problem. MMIN learns robust joint multimodal representations, which can predict the representation of any …

* [59] X Chen. [[_RULE-BASED NETWORK FOR MULTIMODAL EMOTION RECOGNITION_]](https://ieeexplore.ieee.org/abstract/document/10447930/)      
    Human emotion is usually expressed in multiple modalities, like audio and text. Multimodal methods can boost Emotion Recognition. However, the relationship between audio and text, and their roles in emotion expression have not been fully studied, and hence hinder Multimodal Emotion Recognition (MER). In this work, taking into consideration of the above two things, we propose two rules for MER, which are Rule 1: The audio module should be more expressive than the text module, and Rule 2: The single-modality emotional …

* [60] J Zheng, S Zhang, Z Wang, X Wang…. [[_Multi-Channel Weight-Sharing Autoencoder Based on Cascade Multi-Head Attention for Multimodal Emotion Recognition_]](https://ieeexplore.ieee.org/abstract/document/9693238/)      
    Multimodal Emotion Recognition is challenging because of the heterogeneity gap among different modalities. Due to the powerful ability of feature abstraction, Deep Neural Networks (DNNs) have exhibited significant success in bridging the heterogeneity gap in cross-modal retrieval and generation tasks. In this work, a DNNs-based Multi-channel Weight-sharing Autoencoder with Cascade Multi-head Attention (MCWSA-CMHA) is proposed to generically address the affective heterogeneity gap in MER. Specifically, multimodal heterogeneity …

* [61] T Shi, SL Huang. [[_An Attention-Based Correlation-Aware Multimodal Fusion Framework for Emotion Recognition in Conversations_]](https://aclanthology.org/2023.acl-long.824/)      
    Abstract Emotion Recognition in Conversations (ERC) is an increasingly popular task in the Natural Language Processing community, which seeks to achieve accurate emotion classifications of utterances expressed by speakers during a conversation. Most existing approaches focus on modeling speaker and contextual information based on the textual modality, while the complementarity of multimodal information has not been well leveraged, few current methods have sufficiently captured the complex correlations and mapping …

* [62] Q Li, Y Gao, C Wang, Y Deng, J Xue…. [[_Multilevel_Transformer_for_Multimodal_Emotion_Recognition_]](https://ieeexplore.ieee.org/abstract/document/10446812/)      
    Speech emotion recognition (SER) systems aim to recognize human emotional state during 
human-computer interaction. Most existing SER systems are trained based on utterance-…

* [63] G Aguilar, V Rozgić, W Wang, C Wang. [[_Multimodal and Multi-view Models for Emotion Recognition_]](https://arxiv.org/abs/1906.10198)      
    Studies on emotion recognition (ER) show that combining lexical and acoustic information results in more robust and accurate models. The majority of the studies focus on settings where both modalities are available in training and evaluation. However, in practice, this is not always the case; getting ASR output may represent a bottleneck in a deployment pipeline due to computational complexity or privacy-related constraints. To address this challenge, we study the problem of efficiently combining acoustic and lexical modalities …

* [64] J Cheng, I Fostiropoulos, B Boehm…. [[_Multimodal Phased Transformer for Sentiment Analysis_]](https://aclanthology.org/2021.emnlp-main.189/)      
    Multimodal Transformers achieve superior performance in multimodal learning tasks. However, the quadratic complexity of the self-attention mechanism in Transformers limits their deployment in low-resource devices and makes their inference and training computationally expensive. We propose multimodal Sparse Phased Transformer (SPT) to alleviate the problem of self-attention complexity and memory footprint. SPT uses a sampling function to generate a sparse attention matrix and compress a long sequence to a shorter …

* [65] S Zou, X Huang, X Shen. [[_Multimodal Prompt Transformer with Hybrid Contrastive Learning for Emotion Recognition in Conversation_]](https://dl.acm.org/doi/abs/10.1145/3581783.3611805)      
    Emotion Recognition in Conversation (ERC) plays an important role in driving the development of human-machine interaction. Emotions can exist in multiple modalities, and multimodal ERC mainly faces two problems:(1) the noise problem in the cross-modal information fusion process, and (2) the prediction problem of less sample emotion labels that are semantically similar but different categories. To address these issues and fully utilize the features of each modality, we adopted the following strategies: first, deep emotion cues …

* [66] X Sun, H He, H Tang, K Zeng…. [[_Multimodal rough set transformer for sentiment analysis and emotion recognition_]](https://ieeexplore.ieee.org/abstract/document/10263177/)      
    Sentiment analysis and emotion recognition are crucial tasks that utilize multimodal information. Transformer models have shown exceptional performance in multimodal fusion. However, traditional dot product transformers do not tolerate uncertainty inside sentiment analysis and emotion recognition data. In this study, we introduce rough set self-attention and rough set cross-attention mechanisms for multimodal sentiment analysis and emotion recognition. A common concept is established based on granulation relations to extract …

* [67] L Sun, B Liu, J Tao, Z Lian. [[_Multimodal Speech Emotion Recognition using Cross Attnention with Aligned Audio and Text_]](https://ieeexplore.ieee.org/abstract/document/9414654/)      
    … We should also mention that CAN needs the aligned audio and text as input. However, by 
virtue of the cross-attention mechanism, our model does not need alignment information. The …

* [68] F Chen, J Shao, S Zhu…. [[_Rethinking Graph Neural Networks for Emotion Recognition in Conversation_]](http://openaccess.thecvf.com/content/CVPR2023/html/Chen_Multivariate_Multi-Frequency_and_Multimodal_Rethinking_Graph_Neural_Networks_for_Emotion_CVPR_2023_paper.html)      
    Complex relationships of high arity across modality and context dimensions is a critical challenge in the Emotion Recognition in Conversation (ERC) task. Yet, previous works tend to encode multimodal and contextual relationships in a loosely-coupled manner, which may harm relationship modelling. Recently, Graph Neural Networks (GNN) which show advantages in capturing data relations, offer a new solution for ERC. However, existing GNN-based ERC models fail to address some general limits of GNNs, including assuming …

* [69] A Joshi, A Bhat, A Jain, A Singh…. [[_COntextualized GNN based Multimodal Emotion recognitioN_]](https://aclanthology.org/2022.naacl-main.306/)      
    Emotions are an inherent part of human interactions, and consequently, it is imperative to develop AI systems that understand and recognize human emotions. During a conversation involving various people, a person's emotions are influenced by the other speaker's utterances and their own emotional state over the utterances. In this paper, we propose COntextualized Graph Neural Network based Multi-modal Emotion recognitioN (COGMEN) system that leverages local information (ie, inter/intra dependency between speakers) and …

* [70] Z Li, Y Zhou, Y Liu, F Zhu, C Yang…. [[_A Quantum-Inspired Adaptive-Priority-Learning Model for Multimodal Emotion Recognition_]](https://aclanthology.org/2023.findings-acl.772/)      
    Multimodal emotion recognition for video has gained considerable attention in recent years, in which three modalities (ie, textual, visual and acoustic) are involved. Due to the diverse levels of informational content related to emotion, three modalities typically possess varying degrees of contribution to emotion recognition. More seriously, there might be inconsistencies between the emotion of individual modality and the video. The challenges mentioned above are caused by the inherent uncertainty of emotion. Inspired by the recent …

* [71] C Zhang, Y Zhang, B Cheng. [[_A REINFORCEMENT LEARNING FRAMEWORK FOR MULTIMODAL EMOTION RECOGNITION_]](https://ieeexplore.ieee.org/abstract/document/10446459/)      
    Multimodal Emotion Recognition in Conversation (ERC) has gained significant attention due to its wide-ranging applications in diverse areas. However, most previous approaches focused on modeling context at the semantic level, neglecting the context of dependency information at the emotional level. In this paper, we proposed a novel Reinforcement Learning framework for the multimodal EMOtion recognition task (RL-EMO), which combines a Multi-modal Graph Convolution Network (MMGCN)[1] module with a novel Reinforcement …

* [72] H Yang, X Gao, J Wu, T Gan, N Ding…. [[_Self-adaptive Context and Modal-interaction Modeling For Multimodal Emotion Recognition_]](https://aclanthology.org/2023.findings-acl.390/)      
    The multimodal emotion recognition in conversation task aims to predict the emotion label for a given utterance with its context and multiple modalities. Existing approaches achieve good results but also suffer from the following two limitations: 1) lacking modeling of diverse dependency ranges, ie, long, short, and independent context-specific representations and without consideration of the different recognition difficulty for each utterance; 2) consistent treatment of the contribution for various modalities. To address the above challenges, we …

* [73] H Chen, C Guo, Y Li, P Zhang, D Jiang. [[_Semi-Supervised Multimodal Emotion Recognition with Class-Balanced Pseudo-Labeling_]](https://dl.acm.org/doi/abs/10.1145/3581783.3612864)      
    This paper presents our solution for the Semi-Supervised Multimodal Emotion Recognition Challenge (MER2023-SEMI), addressing the issue of limited annotated data in emotion recognition. Recently, the self-training-based Semi-Supervised Learning~(SSL) method has demonstrated its effectiveness in various tasks, including emotion recognition. However, previous studies focused on reducing the confirmation bias of data without adequately considering the issue of data imbalance, which is of great importance in emotion …

* [74] U Chinta, J Kalita, A Atyabi. [[_Facial Images and EEG_]](https://ieeexplore.ieee.org/abstract/document/10099070/)      
    Emotion recognition is an important factor in social communication and has a wide range of applications from retail to healthcare. In psychology, emotion recognition focuses on emotional states within non-verbal visual and auditory cues. It is essential to the human ability to associate meaning with events rather than treating them as mere facts. Studies of emotion recognition often utilize data gathered in response to non-verbal cues using modalities such as eye tracking, Electroencephalo-gram (EEG), and facial video and build …

* [75] B Yao, W Shi. [[_SPEAKER-CENTRIC MULTIMODAL FUSION NETWORKS FOR EMOTION RECOGNITION IN CONVERSATIONS_]](https://ieeexplore.ieee.org/abstract/document/10447720/)      
    Existing emotion recognition methods in conversations (ERC) focus on using different utterances information between speakers to improve emotion recognition performance, but they ignore the differential contributions of different utterances to emotion recognition. In this paper, we propose a speaker-centric multimodal fusion network for ERC, in which bidirectional gated recurrent units (BiGRU) is used for intra-modal feature fusion and graph convolution is used for speaker-centric cross-modal feature fusion. We construct a speaker …

* [76] D Zhang, F Chen, J Chang, X Chen…. [[_Structure Aware Multi-Graph Network for Multi-Modal Emotion Recognition in Conversations_]](https://ieeexplore.ieee.org/abstract/document/10219015/)      
    Multi-Modal Emotion Recognition in Conversations (MMERC) is an increasingly active research field that leverages multi-modal signals to understand the feelings behind each utterance. Modeling contextual interactions and multi-modal fusion lie at the heart of this field, with graph-based models recently being widely used for MMERC to capture global multi-modal contextual information. However, these models generally mix all modality representations in a single graph, and utterances in each modality are fully connected …

* [77] X Song, L Huang, H Xue, S Hu. [[_Supervised Prototypical Contrastive Learning for Emotion Recognition in Conversation_]](https://arxiv.org/abs/2210.08713)      
    Capturing emotions within a conversation plays an essential role in modern dialogue systems. However, the weak correlation between emotions and semantics brings many challenges to emotion recognition in conversation (ERC). Even semantically similar utterances, the emotion may vary drastically depending on contexts or speakers. In this paper, we propose a Supervised Prototypical Contrastive Learning (SPCL) loss for the ERC task. Leveraging the Prototypical Network, the SPCL targets at solving the imbalanced …

* [78] X Li. [[_Token-channel compounded Cross Attention for Multimodal Emotion Recognition_]](https://arxiv.org/abs/2306.13592)      
    Recently, emotion recognition based on physiological signals has emerged as a field with intensive research. The utilization of multi-modal, multi-channel physiological signals has significantly improved the performance of emotion recognition systems, due to their complementarity. However, effectively integrating emotion-related semantic information from different modalities and capturing inter-modal dependencies remains a challenging issue. Many existing multimodal fusion methods ignore either token-to-token or channel-to-channel …

* [79] Z Zhao, Y Wang, Y Xu, J Zhang. [[_Transformer-Based Deep-Scale Fusion Network for Multimodal Emotion Recognition_]](https://ieeexplore.ieee.org/abstract/document/10254334/)      
    As deep learning technology research continues to progress, artificial intelligence technology is gradually empowering various fields. To achieve a more natural human-computer interaction experience, how to accurately recognize emotional state of speech interactions has become a new research hotspot. Sequence modeling methods based on deep learning techniques have promoted the development of emotion recognition, but the mainstream methods still suffer from insufficient multimodal information interaction, difficulty …

* [80] L Stappen, A Baird, L Christ, L Schumann…. [[_Sentiment, Emotion, Physiological-Emotion, and Stress_]](https://dl.acm.org/doi/abs/10.1145/3475957.3484450)      
    Multimodal Sentiment Analysis (MuSe) 2021 is a challenge focusing on the tasks of sentiment and emotion, as well as physiological-emotion and emotion-based stress recognition through more comprehensively integrating the audio-visual, language, and biological signal modalities. The purpose of MuSe 2021 is to bring together communities from different disciplines; mainly, the audio-visual emotion recognition community (signal-based), the sentiment analysis community (symbol-based), and the health informatics …

* [81] S Qiu, N Sekhar, P Singhal. [[_Topic and Style-aware Transformer for Multimodal Emotion Recognition_]](https://aclanthology.org/2023.findings-acl.130/)      
    Understanding emotion expressions in multimodal signals is key for machines to have a better understanding of human communication. While language, visual and acoustic modalities can provide clues from different perspectives, the visual modality is shown to make minimal contribution to the performance in the emotion recognition field due to its high dimensionality. Therefore, we first leverage the strong multimodality backbone VATT to project the visual signal to the common space with language and acoustic signals. Also, we …

* [82] G Hu, TE Lin, Y Zhao, G Lu, Y Wu, Y Li. [[_Towards Unified Multimodal Sentiment Analysis and Emotion Recognition_]](https://arxiv.org/abs/2211.11256)      
    Multimodal sentiment analysis (MSA) and emotion recognition in conversation (ERC) are key research topics for computers to understand human behaviors. From a psychological perspective, emotions are the expression of affect or feelings during a short period, while sentiments are formed and held for a longer period. However, most existing works study sentiment and emotion separately and do not fully exploit the complementary knowledge behind the two. In this paper, we propose a multimodal sentiment knowledge-sharing …

* [83] D Sun, Y He, J Han. [[_USING AUXILIARY TASKS IN MULTIMODAL FUSION OF WAV2VEC 2.0 AND BERT FOR MULTIMODAL EMOTION RECOGNITION_]](https://ieeexplore.ieee.org/abstract/document/10096586/)      
    The lack of data and the difficulty of multimodal fusion have always been challenges for multimodal emotion recognition (MER). In this paper, we propose to use pre-trained models as upstream network, wav2vec 2.0 for audio modality and BERT for text modality, and finetune them in downstream task of MER to cope with the lack of data. For the difficulty of multimodal fusion, we use a K-layer multi-head attention mechanism as a downstream fusion module. Starting from the MER task itself, we design two auxiliary tasks to alleviate …

* [84] J Hu, Y Liu, J Zhao, Q Jin. [[_Multimodal Fusion via Deep Graph Convolution Network for Emotion Recognition in Conversation_]](https://arxiv.org/abs/2107.06779)      
    Emotion recognition in conversation (ERC) is a crucial component in affective dialogue systems, which helps the system understand users' emotions and generate empathetic responses. However, most works focus on modeling speaker and contextual information primarily on the textual modality or simply leveraging multimodal information through feature concatenation. In order to explore a more effective way of utilizing both multimodal and long-distance contextual information, we propose a new model based on multimodal fused graph …

* [85] L Stappen, A Baird, L Christ, L Schumann…. [[_Sentiment, Emotion, Physiological-Emotion, and Stress_]](https://dl.acm.org/doi/abs/10.1145/3475957.3484450)      
    Multimodal Sentiment Analysis (MuSe) 2021 is a challenge focusing on the tasks of sentiment and emotion, as well as physiological-emotion and emotion-based stress recognition through more comprehensively integrating the audio-visual, language, and biological signal modalities. The purpose of MuSe 2021 is to bring together communities from different disciplines; mainly, the audio-visual emotion recognition community (signal-based), the sentiment analysis community (symbol-based), and the health informatics …



#### _MMER_ ####
* [1] P Yang, F Luo, S Ma, J Lin, X Sun. [[_A Deep Reinforced Sequence-to-Set Model for Multi-Label Classification_]](https://aclanthology.org/P19-1518/)      
    Multi-label classification (MLC) aims to predict a set of labels for a given instance. Based on a pre-defined label order, the sequence-to-sequence (Seq2Seq) model trained via maximum likelihood estimation method has been successfully applied to the MLC task and shows powerful ability to capture high-order correlations between labels. However, the output labels are essentially an unordered set rather than an ordered sequence. This inconsistency tends to result in some intractable problems, eg, sensitivity to the label order …

* [2] D Zhou, Y Xiang, L Zhang, C Ye…. [[_A Divide-And-Conquer Approach for Multi-label Multi-hop Relation Detection in Knowledge Base Question Answering_]](https://aclanthology.org/2021.findings-emnlp.412/)      
    Relation detection in knowledge base question answering, aims to identify the path (s) of relations starting from the topic entity node that is linked to the answer node in knowledge graph. Such path might consist of multiple relations, which we call multi-hop. Moreover, for a single question, there may exist multiple relation paths to the correct answer, which we call multi-label. However, most of existing approaches only detect one single path to obtain the answer without considering other correct paths, which might affect the final performance …

* [3] C Wu, L Cao, Y Ge, Y Liu, M Zhang, J Su. [[_A Label Dependence-Aware Sequence Generation Model for Multi-Level Implicit Discourse Relation Recognition_]](https://ojs.aaai.org/index.php/AAAI/article/view/21401)      
    Implicit discourse relation recognition (IDRR) is a challenging but crucial task in discourse analysis. Most existing methods train multiple models to predict multi-level labels independently, while ignoring the dependence between hierarchically structured labels. In this paper, we consider multi-level IDRR as a conditional label sequence generation task and propose a Label Dependence-aware Sequence Generation Model (LDSGM) for it. Specifically, we first design a label attentive encoder to learn the global representation of an …

* [4] R You, Z Guo, L Cui, X Long, Y Bao…. [[_Cross-Modality Attention with Semantic Graph Embedding for Multi Label Classification_]](https://ojs.aaai.org/index.php/AAAI/article/view/6964)      
    Multi-label image and video classification are fundamental yet challenging tasks in computer vision. The main challenges lie in capturing spatial or temporal dependencies between labels and discovering the locations of discriminative features for each class. In order to overcome these challenges, we propose to use cross-modality attention with semantic graph embedding for multi-label classification. Based on the constructed label graph, we propose an adjacency-based similarity graph embedding method to learn semantic label …

* [5] S Qian, D Xue, H Zhang, Q Fang, C Xu. [[_Dual Adversarial Graph Neural Networks for Multi-label Cross-modal Retrieval_]](https://ojs.aaai.org/index.php/AAAI/article/view/16345)      
    Cross-modal retrieval has become an active study field with the expanding scale of multimodal data. To date, most existing methods transform multimodal data into a common representation space where semantic similarities between items can be directly measured across different modalities. However, these methods typically suffer from following limitations: 1) They usually attempt to bridge the modality gap by designing losses in the common representation space which may not be sufficient to eliminate potential …

* [6] Y Wang, Y Xie, Y Liu, K Zhou, X Li. [[_Fast Graph Convolution Network Based Multi-label Image Recognition via Cross-modal Fusion_]](https://dl.acm.org/doi/abs/10.1145/3340531.3411880)      
    In multi-label image recognition, it has become a popular method to predict those labels that co-occur in an image via modeling the label dependencies. Previous works focus on capturing the correlation between labels, but neglect to effectively fuse the image features and label embeddings, which severely affects the convergence efficiency of the model and inhibits the further precision improvement of multi-label image recognition. To overcome this shortcoming, in this paper, we introduce Multi-modal Factorized Bilinear pooling (MFB) …

* [7] L Qiu, Y Yang, CC Cao, Y Zheng, H Ngai…. [[_Generating Perturbation-based Explanations with Robustness to Out-of-Distribution Data_]](https://dl.acm.org/doi/abs/10.1145/3485447.3512254)      
    Perturbation-based techniques are promising for explaining black-box machine learning models due to their effectiveness and ease of implementation. However, prior works have faced the problem of Out-of-Distribution (OoD)—an artifact of randomly perturbed data becoming inconsistent with the original dataset, degrading the reliability of generated explanations, which is still under-explored according to our best knowledge. This work addresses the OoD issue by designing a simple yet effective module that can quantify the …

* [8] H Lian, C Lu, S Li, Y Zhao, C Tang, Y Zong…. [[_Label Distribution Adaptation for Multimodal Emotion Recognition with Multi-label Learning_]](https://dl.acm.org/doi/abs/10.1145/3607865.3613183)      
    In the task of multimodal emotion recognition with multi-label learning (MER-MULTI), leveraging the correlation between discrete and dimensional emotions is crucial for improving the model's performance. However, there may be a mismatch between the feature distributions of the training set and the testing set, which could result in the trained model's inability to adapt to the correlations between labels in the testing set. Therefore, a significant challenge in MER-MULTI is how to match the feature distributions of the training set and …

* [9] L Xiao, X Huang, B Chen, L Jing. [[_Label-Specific Document Representation for Multi-Label Text Classification_]](https://aclanthology.org/D19-1044/)      
    Multi-label text classification (MLTC) aims to tag most relevant labels for the given document. In this paper, we propose a Label-Specific Attention Network (LSAN) to learn a label-specific document representation. LSAN takes advantage of label semantic information to determine the semantic connection between labels and document for constructing label-specific document representation. Meanwhile, the self-attention mechanism is adopted to identify the label-specific document representation from document content information. In order to …

* [10] Q Ma, C Yuan, W Zhou, S Hu. [[_Label-Specific Dual Graph Neural Network for Multi-Label Text Classification_]](https://aclanthology.org/2021.acl-long.298/)      
    Multi-label text classification is one of the fundamental tasks in natural language processing. Previous studies have difficulties to distinguish similar labels well because they learn the same document representations for different labels, that is they do not explicitly extract label-specific semantic components from documents. Moreover, they do not fully explore the high-order interactions among these semantic components, which is very helpful to predict tail labels. In this paper, we propose a novel label-specific dual graph neural network (LDGN) …

* [11] S Ge, Z Jiang, Z Cheng, C Wang, Y Yin…. [[_Learning Robust Multi-Modal Representation for Multi-Label Emotion Recognition via Adversarial Masking and Perturbation_]](https://dl.acm.org/doi/abs/10.1145/3543507.3583258)      
    Recognizing emotions from multi-modal data is an emotion recognition task that requires strong multi-modal representation ability. The general approach to this task is to naturally train the representation model on training data without intervention. However, such natural training scheme is prone to modality bias of representation (ie, tending to over-encode some informative modalities while neglecting other modalities) and data bias of training (ie, tending to overfit training data). These biases may lead to instability (eg, performing poorly …

* [12] T Chen, M Xu, X Hui, H Wu…. [[_Learning Semantic-Specific Graph Representation for Multi-Label Image Recognition_]](http://openaccess.thecvf.com/content_ICCV_2019/html/Chen_Learning_Semantic-Specific_Graph_Representation_for_Multi-Label_Image_Recognition_ICCV_2019_paper.html)      
    Recognizing multiple labels of images is a practical and challenging task, and significant progress has been made by searching semantic-aware regions and modeling label dependency. However, current methods cannot locate the semantic regions accurately due to the lack of part-level supervision or semantic guidance. Moreover, they cannot fully explore the mutual interactions among the semantic regions and do not explicitly model the label co-occurrence. To address these issues, we propose a Semantic-Specific Graph …

* [13] J Zhao, T Zhang, J Hu, Y Liu, Q Jin, X Wang…. [[_M3ED- Multi-modal Multi-scene Multi-label Emotional Dialogue Database_]](https://arxiv.org/abs/2205.10237)      
    The emotional state of a speaker can be influenced by many different factors in dialogues, such as dialogue scene, dialogue topic, and interlocutor stimulus. The currently available data resources to support such multimodal affective analysis in dialogues are however limited in scale and diversity. In this work, we propose a Multi-modal Multi-scene Multi-label Emotional Dialogue dataset, M3ED, which contains 990 dyadic emotional dialogues from 56 different TV series, a total of 9,082 turns and 24,449 utterances. M3 ED is annotated with 7 …

* [14] J Zhao, Y Zhao, J Li. [[_M3TR- Multi-modal Multi-label Recognition with Transformer_]](https://dl.acm.org/doi/abs/10.1145/3474085.3475191)      
    Multi-label image recognition aims to recognize multiple objects simultaneously in one image. Recent ideas to solve this problem have focused on learning dependencies of label co-occurrences to enhance the high-level semantic representations. However, these methods usually neglect the important relations of intrinsic visual structures and face difficulties in understanding contextual relationships. To build the global scope of visual context as well as interactions between visual modality and linguistic modality, we propose …

* [15] ZM Chen, XS Wei, P Wang…. [[_Multi-Label Image Recognition with Graph Convolutional Networks∗_]](http://openaccess.thecvf.com/content_CVPR_2019/html/Chen_Multi-Label_Image_Recognition_With_Graph_Convolutional_Networks_CVPR_2019_paper.html)      
    The task of multi-label image recognition is to predict a set of object labels that present in an image. As objects normally co-occur in an image, it is desirable to model the label dependencies to improve the recognition performance. To capture and explore such important dependencies, we propose a multi-label classification model based on Graph Convolutional Network (GCN). The model builds a directed graph over the object labels, where each node (label) is represented by word embeddings of a label, and GCN is learned …

* [16] HD Le, GS Lee, SH Kim, S Kim, HJ Yang. [[_Multi-Label Multimodal Emotion Recognition With Transformer-Based Fusion and Emotion-Level Representation Learning_]](https://ieeexplore.ieee.org/abstract/document/10042438/)      
    Emotion recognition has been an active research area for a long time. Recently, multimodal emotion recognition from video data has grown in importance with the explosion of video content due to the emergence of short video social media platforms. Effectively incorporating information from multiple modalities in video data to learn robust multimodal representation for improving recognition model performance is still the primary challenge for researchers. In this context, transformer architectures have been widely used and have significantly …

* [17] D Zhang, X Ju, J Li, S Li, Q Zhu…. [[_Multi-modal Multi-label Emotion Detection with Modality and Label Dependence_]](https://aclanthology.org/2020.emnlp-main.291/)      
    As an important research issue in the natural language processing community, multi-label emotion detection has been drawing more and more attention in the last few years. However, almost all existing studies focus on one modality (eg, textual modality). In this paper, we focus on multi-label emotion detection in a multi-modal scenario. In this scenario, we need to consider both the dependence among different labels (label dependence) and the dependence between each predicting label and different modalities (modality …

* [18] D Zhang, X Ju, W Zhang, J Li, S Li, Q Zhu…. [[_Multi-modal Multi-label Emotion Recognition with Heterogeneous Hierarchical Message Passing_]](https://ojs.aaai.org/index.php/AAAI/article/view/17686)      
    As an important research issue in affective computing community, multi-modal emotion recognition has become a hot topic in the last few years. However, almost all existing studies perform multiple binary classification for each emotion with focus on complete time series data. In this paper, we focus on multi-modal emotion recognition in a multi-label scenario. In this scenario, we consider not only the label-to-label dependency, but also the feature-to-label and modality-to-label dependencies. Particularly, we propose a heterogeneous …

* [19] A Illendula, A Sheth. [[_Multimodal Emotion Classification_]](https://dl.acm.org/doi/abs/10.1145/3308560.3316549)      
    … to study the usage of emojis in different emotional contexts is described in Section 3. We 
present our model and approach of multimodal emotional classification in Section 4. The results …

* [20] M Wołczyk, M Proszewska, Ł Maziarka…. [[_PluGeN Multi-Label Conditional Generation from Pre-trained Models_]](https://ojs.aaai.org/index.php/AAAI/article/view/20843)      
    Modern generative models achieve excellent quality in a variety of tasks including image or text generation and chemical molecule modeling. However, existing methods often lack the essential ability to generate examples with requested properties, such as the age of the person in the photo or the weight of the generated molecule. Incorporating such additional conditioning factors would require rebuilding the entire architecture and optimizing the parameters from scratch. Moreover, it is difficult to disentangle selected attributes so that to …

* [21] K Zhu, J Wu. [[_Residual Attention- A Simple but Effective Method for Multi-Label Recognition_]](http://openaccess.thecvf.com/content/ICCV2021/html/Zhu_Residual_Attention_A_Simple_but_Effective_Method_for_Multi-Label_Recognition_ICCV_2021_paper.html)      
    Multi-label image recognition is a challenging computer vision task of practical use. Progresses in this area, however, are often characterized by complicated methods, heavy computations, and lack of intuitive explanations. To effectively capture different spatial regions occupied by objects from different categories, we propose an embarrassingly simple module, named class-specific residual attention (CSRA). CSRA generates class-specific features for every category by proposing a simple spatial attention score, and then combines …

* [22] P Yang, X Sun, W Li, S Ma, W Wu, H Wang. [[_SGM- Sequence Generation Model for Multi-Label Classification_]](https://arxiv.org/abs/1806.04822)      
    Multi-label classification is an important yet challenging task in natural language processing. It is more complex than single-label classification in that the labels tend to be correlated. Existing methods tend to ignore the correlations between labels. Besides, different parts of the text can contribute differently for predicting different labels, which is not considered by existing models. In this paper, we propose to view the multi-label classification task as a sequence generation problem, and apply a sequence generation model with a novel …

* [23] A Ando, R Masumura, H Kamiyama…. [[_Speech Emotion Recognition based on Multi-Label Emotion Existence Model_]](https://www.isca-archive.org/interspeech_2019/ando19_interspeech.pdf)      
    This paper presents a novel speech emotion recognition method that addresses the ambiguous nature of emotions in speech. Most conventional methods assume there is only a single ground truth, the dominant emotion, though utterances can contain multiple emotions. In order to solve this problem, several methods that consider ambiguous emotions (eg soft-target training) have been proposed. Unfortunately, training them is difficult since they work by estimating the proportions of all emotions. The proposed method improves both …

* [24] Y Zhang, M Chen, J Shen, C Wang. [[_Tailor Versatile Multi-Modal Learning for Multi-Label Emotion Recognition_]](https://ojs.aaai.org/index.php/AAAI/article/view/20895)      
    Abstract Multi-modal Multi-label Emotion Recognition (MMER) aims to identify various human emotions from heterogeneous visual, audio and text modalities. Previous methods mainly focus on projecting multiple modalities into a common latent space and learning an identical representation for all labels, which neglects the diversity of each modality and fails to capture richer semantic information for each label from different perspectives. Besides, associated relationships of modalities and labels have not been fully exploited. In this paper …

* [25] J Shen, W Qiu, Y Meng, J Shang, X Ren…. [[_TaxoClass- Hierarchical Multi-Label Text Classification Using Only Class Names_]](https://par.nsf.gov/biblio/10311077)      
    Hierarchical multi-label text classification (HMTC) aims to tag each document with a set of classes from a class hierarchy. Most existing HMTC methods train classifiers using massive human-labeled documents, which are often too costly to obtain in real-world applications. In this paper, we explore to conduct HMTC based on only class surface names as supervision signals. We observe that to perform HMTC, human experts typically first pinpoint a few most essential classes for the document as its “core classes”, and then check core classes' …

* [26] X Ju, D Zhang, J Li, G Zhou. [[_Transformer-based Label Set Generation for Multi-modal Multi-label Emotion Detection_]](https://dl.acm.org/doi/abs/10.1145/3394171.3413577)      
    Multi-modal utterance-level emotion detection has been a hot research topic in both multi-modal analysis and natural language processing communities. Different from traditional single-label multi-modal sentiment analysis, typical multi-modal emotion detection is naturally a multi-label problem where an utterance often contains multiple emotions. Existing studies normally focus on multi-modal fusion only and transform multi-label emotion classification into multiple binary classification problem independently. As a result, existing …



#### _MSA_ ####
* [1] Huisheng Mao, Baozheng Zhang, Hua Xu, Ziqi Yuan, Yihe Liu. [[_Robust-MSA: Understanding the Impact of Modality Noise
on Multimodal Sentiment Analysis_]](https://arxiv.org/abs/2211.13484)      
    Improving model robustness against potential modality noise, as an essential step for adapting multimodal models to real-world applications, has received increasing attention among researchers. For Multimodal Sentiment Analysis (MSA), there is also a debate on whether multimodal models are more effective against noisy features than unimodal ones. Stressing on intuitive illustration and in-depth analysis of these concerns, we present Robust-MSA, an interactive platform that visualizes the impact of modality noise as well as simple defence methods to help researchers know better about how their models perform with imperfect real-world data.

* [2] S Afzal, HA Khan, IU Khan, MJ Piran…. [[_A Comprehensive Survey on Affective Computing_ Challenges, Trends, Applications, and Future Directions_]](https://arxiv.org/abs/2305.07665)      
    As the name suggests, affective computing aims to recognize human emotions, sentiments, and feelings. There is a wide range of fields that study affective computing, including languages, sociology, psychology, computer science, and physiology. However, no research has ever been done to determine how machine learning (ML) and mixed reality (XR) interact together. This paper discusses the significance of affective computing, as well as its ideas, conceptions, methods, and outcomes. By using approaches of ML and XR, we …

* [3] J Peng, T Wu, W Zhang, F Cheng, S Tan, F Yi…. [[_A fine-grained modal label-based multi-stage network for multimodal sentiment analysis_]](https://www.sciencedirect.com/science/article/pii/S0957417423002221)      
    Sentiment analysis is a challenging but valuable research topic in affective computing. It can improve the quality of various real-world applications, including financial market prediction, disease analysis even politics. As sentiment may be expressed by text, image, audio, video, etc., multimodal sentiment analysis has emerged to capture information in multiple ways. Take video as an example, the analysis process may be difficult since the modalities in the video are heterogeneous and may express different sentiments. To deal with such issues, a …

* [4] Y Zhi, J Li, H Wang, J Chen…. [[_A Fine-Grained Tri-Modal Interaction Model for Multimodal Sentiment Analysis_]](https://ieeexplore.ieee.org/abstract/document/10447872/)      
    The methods based on multimodal representation learning enhance discriminable sentiment expression for multimodal sentiment analysis (MSA). The modal invariant and specific features serve different purposes in sentiment learning and the diversity of inter-sample and inter-category relationships takes less consideration in previous advances. In this paper, we propose a fine-grained tri-modal interaction model for MSA to refine and enhance the overall affective state at the uni/multi-modal level and label level. Concretely …

* [5] J LIU, H SONG, DP CHEN, B WANG, ZW ZHANG. [[_A Multimodal Sentiment Analysis Model Enhanced with Non-verbal Information and Contrastive Learning_]](https://jeit.ac.cn/en/article/doi/10.11999/JEIT231274)      
    Deep learning methods have gained popularity in multimodal sentiment analysis due to their impressive representation and fusion capabilities in recent years. Existing studies often analyze the emotions of individuals using multimodal information such as text, facial expressions, and speech intonation, primarily employing complex fusion methods. However, existing models inadequately consider the dynamic changes in emotions over long time sequences, resulting in suboptimal performance in sentiment analysis. In response to this …

* [6] X Sun, X Ren, X Xie. [[_A NOVEL MULTIMODAL SENTIMENT ANALYSIS MODEL BASED ON GATED FUSION AND MULTI-TASK LEARNING_]](https://ieeexplore.ieee.org/abstract/document/10446040/)      
    Sentiment analysis is an important research area in Natural Language Processing (NLP). With the explosion of multimodal data, Multimodal Sentiment Analysis (MSA) attracts more and more attention in recent years. How to Effectively harnessing the interplay between diverse modalities is paramount to achieving comprehensive fusion of MSA. However, current research predominantly emphasizes modality interaction, while overlooking unimodal information, thus neglecting the inherent disparities between modalities. To …

* [7] MM Amin, R Mao, E Cambria, BW Schuller. [[_A Wide Evaluation of ChatGPT on Affective Computing Tasks_]](https://arxiv.org/abs/2308.13911)      
    With the rise of foundation models, a new artificial intelligence paradigm has emerged, by simply using general purpose foundation models with prompting to solve problems instead of training a separate machine learning model for each problem. Such models have been shown to have emergent properties of solving problems that they were not initially trained on. The studies for the effectiveness of such models are still quite limited. In this work, we widely study the capabilities of the ChatGPT models, namely GPT-4 and GPT-3.5, on 13 …

* [8] AAB Zadeh, PP Liang, S Poria, E Cambria…. [[_CMU-MOSEI Dataset and Interpretable Dynamic Fusion Graph_]](https://aclanthology.org/P18-1208/)      
    Analyzing human multimodal language is an emerging area of research in NLP. Intrinsically this language is multimodal (heterogeneous), sequential and asynchronous; it consists of the language (words), visual (expressions) and acoustic (paralinguistic) modalities all in the form of asynchronous coordinated sequences. From a resource perspective, there is a genuine need for large scale datasets that allow for in-depth studies of this form of language. In this paper we introduce CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU …

* [9] W Yu, H Xu, F Meng, Y Zhu, Y Ma, J Wu…. [[_A Chinese Multimodal Sentiment Analysis Dataset with Fine-grained Annotations of Modality_]](https://aclanthology.org/2020.acl-main.343/)      
    Previous studies in multimodal sentiment analysis have used limited datasets, which only contain unified multimodal annotations. However, the unified annotations do not always reflect the independent sentiment of single modalities and limit the model to capture the difference between modalities. In this paper, we introduce a Chinese single-and multi-modal sentiment analysis dataset, CH-SIMS, which contains 2,281 refined video segments in the wild with both multimodal and independent unimodal annotations. It allows researchers to …

* [10] K Kim, S Park. [[_All-modalities-in-One BERT for multimodal sentiment analysis_]](https://www.sciencedirect.com/science/article/pii/S1566253522002329)      
    Multimodal sentiment analysis utilizes various modalities such as Text, Vision and Speech to predict sentiment. As these modalities have unique characteristics, methods have been developed for fusing features. However, the overall modality characteristics are not guaranteed, because traditional fusion methods have some loss of intra-modality and inter-modality. To solve this problem, we introduce a single-stream transformer, All-modalities-in-One BERT (AOBERT). The model is pre-trained on two tasks simultaneously: Multimodal …

* [11] Hui Ma, Jian Wang, Hongfei Lin, Bo Zhang, Yijia Zhang, Bo Xu. [[_A_Transformer-Based_Model_With_Self-Distillation_for_Multimodal_Emotion_Recognition_in_Conversations_]](https://ieeexplore.ieee.org/document/10109845)      
    Emotion recognition in conversations (ERC), the task of recognizing the emotion of each utterance in a conversation, is crucial for building empathetic machines. Existing studies focus mainly on capturing context- and speaker-sensitive dependencies on the textual modality but ignore the significance of multimodal information. Different from emotion recognition in textual conversations, capturing intra- and inter-modal interactions between utterances, learning weights between different modalities, and enhancing modal representations play important roles in multimodal ERC. In this paper, we propose a transformer-based model with self-distillation (SDT) 1 1
The code is available at https://github.com/butterfliesss/SDT.

* [12] J Tang, D Liu, X Jin, Y Peng, Q Zhao…. [[_Bi-Direction Attention Based Fusion Network for Multimodal Sentiment Analysis_]](https://ieeexplore.ieee.org/abstract/document/9932611/)      
    Attention-based networks currently identify their effectiveness in multimodal sentiment analysis. However, existing methods ignore the redundancy of auxiliary modalities. More importantly, existing methods only attend to top-down attention (static process) or down-top attention (implicit process), leading to the coarse-grained multimodal sentiment context. In this paper, during the preprocessing period, we first propose the multimodal dynamic enhanced block to capture the intra-modality sentiment context. This can effectively …

* [13] Z Li, B Xu, C Zhu, T Zhao. [[_CLMLF-A Contrastive Learning and Multi-Layer Fusion Method for Multimodal Sentiment Detection_]](https://arxiv.org/abs/2204.05515)      
    Compared with unimodal data, multimodal data can provide more features to help the model analyze the sentiment of data. Previous research works rarely consider token-level feature fusion, and few works explore learning the common features related to sentiment in multimodal data to help the model fuse multimodal features. In this paper, we propose a Contrastive Learning and Multi-Layer Fusion (CLMLF) method for multimodal sentiment detection. Specifically, we first encode text and image to obtain hidden representations, and …

* [14] J Yang, Y Yu, D Niu, W Guo, Y Xu. [[_Contrastive Feature Decomposition for Multimodal Sentiment Analysis_]](https://aclanthology.org/2023.acl-long.421/)      
    Abstract Multimodal Sentiment Analysis aims to predict the sentiment of video content. Recent research suggests that multimodal sentiment analysis critically depends on learning a good representation of multimodal information, which should contain both modality-invariant representations that are consistent across modalities as well as modality-specific representations. In this paper, we propose ConFEDE, a unified learning framework that jointly performs contrastive representation learning and contrastive feature decomposition to …

* [15] Y Yu, M Zhao, S Qi, F Sun, B Wang, W Guo…. [[_Contrastive Knowledge Injection for Multimodal Sentiment Analysis_]](https://arxiv.org/abs/2306.15796)      
    Multimodal Sentiment Analysis leverages multimodal signals to detect the sentiment of a speaker. Previous approaches concentrate on performing multimodal fusion and representation learning based on general knowledge obtained from pretrained models, which neglects the effect of domain-specific knowledge. In this paper, we propose Contrastive Knowledge Injection (ConKI) for multimodal sentiment analysis, where specific-knowledge representations for each modality can be learned together with general …

* [16] DS Chauhan, MS Akhtar, A Ekbal…. [[_Context-aware Interactive Attention for Multi-modal Sentiment and Emotion Analysis_]](https://aclanthology.org/D19-1566/)      
    In recent times, multi-modal analysis has been an emerging and highly sought-after field at the intersection of natural language processing, computer vision, and speech processing. The prime objective of such studies is to leverage the diversified information,(eg, textual, acoustic and visual), for learning a model. The effective interaction among these modalities often leads to a better system in terms of performance. In this paper, we introduce a recurrent neural network based approach for the multi-modal sentiment and emotion analysis. The …

* [17] M Huang, C Qing, J Tan, X Xu. [[_Context-Based Adaptive Multimodal Fusion Network for Continuous Frame-Level Sentiment Prediction_]](https://ieeexplore.ieee.org/abstract/document/10271721/)      
    Recently, video sentiment computing has become the focus of research because of its benefits in many applications such as digital marketing, education, healthcare, and so on. The difficulty of video sentiment prediction mainly lies in the regression accuracy of long-term sequences and how to integrate different modalities. In particular, different modalities may express different emotions. In order to maintain the continuity of long time-series sentiments and mitigate the multimodal conflicts, this article proposes a novel Context …

* [18] Y Zeng, W Yan, S Mai, H Hu. [[_Disentanglement Translation Network for multimodal sentiment analysis_]](https://www.sciencedirect.com/science/article/pii/S1566253523003470)      
    Obtaining an effective joint representation has always been the goal for multimodal tasks. However, distributional gap inevitably exists due to the heterogeneous nature of different modalities, which poses burden on the fusion process and the learning of multimodal representation. The imbalance of modality dominance further aggravates this problem, where inferior modalities may contain much redundancy that introduces additional variations. To address the aforementioned issues, we propose a Disentanglement …

* [19] S Mai, H Hu, S Xing. [[_Hierarchical Feature Fusion Network with Local and Global Perspectives for Multimodal Affective Computing_]](https://aclanthology.org/P19-1046/)      
    We propose a general strategy named 'divide, conquer and combine'for multimodal fusion. Instead of directly fusing features at holistic level, we conduct fusion hierarchically so that both local and global interactions are considered for a comprehensive interpretation of multimodal embeddings. In the 'divide'and 'conquer'stages, we conduct local fusion by exploring the interaction of a portion of the aligned feature vectors across various modalities lying within a sliding window, which ensures that each part of multimodal embeddings are …

* [20] Marie-Francine Moens, Xuanjing Huang, Lucia Specia, Scott Wen-tau Yih. [[_Multimodal Phased Transformer for Sentiment Analysis_]](https://aclanthology.org/2021.emnlp-main.189/)      
    Multimodal Transformers achieve superior performance in multimodal learning tasks. However, the quadratic complexity of the self-attention mechanism in Transformers limits their deployment in low-resource devices and makes their inference and training computationally expensive. We propose multimodal Sparse Phased Transformer (SPT) to alleviate the problem of self-attention complexity and memory footprint. SPT uses a sampling function to generate a sparse attention matrix and compress a long sequence to a shorter sequence of hidden states. SPT concurrently captures interactions between the hidden states of different modalities at every layer. To further improve the efficiency of our method, we use Layer-wise parameter sharing and Factorized Co-Attention that share parameters between Cross Attention Blocks, with minimal impact on task performance. We evaluate our model with three sentiment analysis datasets and achieve comparable or superior performance compared with the existing methods, with a 90% reduction in the number of parameters. We conclude that (SPT) along with parameter sharing can capture multimodal interactions with reduced model size and improved sample efficiency.

* [21] A Martins, R Astudillo. [[_A Sparse Model of Attention and Multi-Label Classification_]](https://proceedings.mlr.press/v48/martins16)      
    We propose sparsemax, a new activation function similar to the traditional softmax, but able to output sparse probabilities. After deriving its properties, we show how its Jacobian can be efficiently computed, enabling its use in a network trained with backpropagation. Then, we propose a new smooth and convex loss function which is the sparsemax analogue of the logistic loss. We reveal an unexpected connection between this new loss and the Huber classification loss. We obtain promising empirical results in multi-label classification …

* [22] H Liang, W Xie, X He, S Song…. [[_GUIDED CIRCULAR DECOMPOSITION AND CROSS-MODAL RECOMBINATION FOR MULTIMODAL SENTIMENT ANALYSIS_]](https://ieeexplore.ieee.org/abstract/document/10446166/)      
    Multimodal Sentiment Analysis is a burgeoning research area, leveraging various modalities to predict the sentiment score. Nevertheless, previous studies have disregarded the impact of noise interference on specific modal sentiments during video recording, thereby compromising the accuracy of sentiment prediction. In this paper, we propose the Guided Circular Decomposition and Cross-Modal Recombination (GCD-CMR) model, which aims to eliminate contaminated sentiment features in a fine-grained way. To achieve this, we utilize …

* [23] S Mai, Y Zeng, S Zheng, H Hu. [[_Hybrid Contrastive Learning of Tri-Modal Representation for Multimodal Sentiment Analysis_]](https://ieeexplore.ieee.org/abstract/document/9767560/)      
    The wide application of smart devices enables the availability of multimodal data, which can be utilized in many tasks. In the field of multimodal sentiment analysis, most previous works focus on exploring intra-and inter-modal interactions. However, training a network with cross-modal information (language, audio and visual) is still challenging due to the modality gap. Besides, while learning dynamics within each sample draws great attention, the learning of inter-sample and inter-class relationships is neglected. Moreover, the size of datasets limits …

* [24] X Zhao, Y Chen, W Li, L Gao…. [[_An Extended Multimodal Adaptation Gate for Multimodal Sentiment Analysis_]](https://ieeexplore.ieee.org/abstract/document/9746536/)      
    Human multimodal sentiment analysis is a challenging task that devotes to extract and integrate information from multiple resources, such as language, acoustic and visual information. Recently, multimodal adaptation gate (MAG), an attachment to transformer-based pre-trained language representation models, such as BERT and XLNet, has shown state-of-the-art performance on multimodal sentiment analysis. MAG only uses a 1-layer network to fuse multimodal information directly, and does not pay attention to relationships …

* [25] D Hazarika, S Poria, R Mihalcea…. [[_Interactive Conversational Memory Network for Multimodal Emotion Detection_]](https://aclanthology.org/D18-1280/)      
    Emotion recognition in conversations is crucial for building empathetic machines. Present works in this domain do not explicitly consider the inter-personal influences that thrive in the emotional dynamics of dialogues. To this end, we propose Interactive COnversational memory Network (ICON), a multimodal emotion detection framework that extracts multimodal features from conversational videos and hierarchically models the self-and inter-speaker emotional influences into global memories. Such memories generate contextual summaries …

* [26] C Busso, M Bulut, CC Lee, A Kazemzadeh…. [[_Interactive emotional dyadic motion capture database_]](https://link.springer.com/article/10.1007/s10579-008-9076-6)      
    Since emotions are expressed through a combination of verbal and non-verbal channels, a joint analysis of speech and gestures is required to understand expressive human communication. To facilitate such investigations, this paper describes a new corpus named the “interactive emotional dyadic motion capture database”(IEMOCAP), collected by the Speech Analysis and Interpretation Laboratory (SAIL) at the University of Southern California (USC). This database was recorded from ten actors in dyadic sessions with …

* [27] W Han, H Chen, S Poria. [[_Improving Multimodal Fusion with Hierarchical Mutual Information Maximization for Multimodal Sentiment Analysis_]](https://arxiv.org/abs/2109.00412)      
    In multimodal sentiment analysis (MSA), the performance of a model highly depends on the quality of synthesized embeddings. These embeddings are generated from the upstream process called multimodal fusion, which aims to extract and combine the input unimodal raw data to produce a richer multimodal representation. Previous work either back-propagates the task loss or manipulates the geometric property of feature spaces to produce favorable fusion results, which neglects the preservation of critical task-related information that flows …

* [28] W Rahman, MK Hasan, S Lee, A Zadeh…. [[_Integrating Multimodal Information in Large Pretrained Transformers_]](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8005298/)      
    Recent Transformer-based contextual word representations, including BERT and XLNet, have shown state-of-the-art performance in multiple disciplines within NLP. Fine-tuning the trained contextual models on task-specific datasets has been the key to achieving superior performance downstream. While fine-tuning these pre-trained models is straight-forward for lexical applications (applications with only language modality), it is not trivial for multimodal language (a growing area in NLP focused on modeling face-to-face communication). Pre …

* [29] C Chen, H Hong, J Guo, B Song. [[_Inter-Intra Modal Representation Augmentation With Trimodal Collaborative Disentanglement Network for Multimodal Sentiment Analysis_]](https://ieeexplore.ieee.org/abstract/document/10089492/)      
    Recently, Multimodal Sentiment Analysis (MSA) is a challenging research area given its complex nature, and humans express emotional cues across various modalities such as language, facial expressions, and speech. Representation and fusion of features are the most crucial tasks in multimodal sentiment analysis research. However, in the current research, most methods ignore the importance of eliminating potential irrelevant features in the original features of each modality and cross-modal common feature. Moreover, the …

* [30] VW Liang, Y Zhang, Y Kwon…. [[_Understanding the Modality Gap in Multi-modal Contrastive Representation Learning_]](https://proceedings.neurips.cc/paper_files/paper/2022/hash/702f4db7543a7432431df588d57bc7c9-Abstract-Conference.html)      
    We present modality gap, an intriguing geometric phenomenon of the representation space of multi-modal models. Specifically, we show that different data modalities (eg images and text) are embedded at arm's length in their shared representation in multi-modal models such as CLIP. Our systematic analysis demonstrates that this gap is caused by a combination of model initialization and contrastive learning optimization. In model initialization, we show empirically and theoretically that the representation of a common …

* [31] D Hazarika, R Zimmermann, S Poria. [[_Modality-Invariant and -Specific Representations for Multimodal Sentiment Analysis_]](https://dl.acm.org/doi/abs/10.1145/3394171.3413678)      
    Multimodal Sentiment Analysis is an active area of research that leverages multimodal signals for affective understanding of user-generated videos. The predominant approach, addressing this task, has been to develop sophisticated fusion techniques. However, the heterogeneous nature of the signals creates distributional modality gaps that pose significant challenges. In this paper, we aim to learn effective modality representations to aid the process of fusion. We propose a novel framework, MISA, which projects each modality to …

* [32] J Zeng, J Zhou, T Liu. [[_Mitigating Inconsistencies in Multimodal Sentiment Analysis under Uncertain Missing Modalities_]](https://aclanthology.org/2022.emnlp-main.189/)      
    For the missing modality problem in Multimodal Sentiment Analysis (MSA), the inconsistency phenomenon occurs when the sentiment changes due to the absence of a modality. The absent modality that determines the overall semantic can be considered as a key missing modality. However, previous works all ignored the inconsistency phenomenon, simply discarding missing modalities or solely generating associated features from available modalities. The neglect of the key missing modality case may lead to incorrect semantic …

* [33] K Yang, H Xu, K Gao. [[_Cross-Modal BERT for Text-Audio Sentiment Analysis_]](https://dl.acm.org/doi/abs/10.1145/3394171.3413690)      
    Multimodal sentiment analysis is an emerging research field that aims to enable machines to recognize, interpret, and express emotion. Through the cross-modal interaction, we can get more comprehensive emotional characteristics of the speaker. Bidirectional Encoder Representations from Transformers (BERT) is an efficient pre-trained language representation model. Fine-tuning it has obtained new state-of-the-art results on eleven natural language processing tasks like question answering and natural language inference …

* [34] S Mai, H Hu, S Xing. [[_Modality to Modality Translation An Adversarial Representation Learning and Graph Fusion Network for Multimodal Fusion_]](https://aaai.org/ojs/index.php/AAAI/article/view/5347)      
    Learning joint embedding space for various modalities is of vital importance for multimodal fusion. Mainstream modality fusion approaches fail to achieve this goal, leaving a modality gap which heavily affects cross-modal fusion. In this paper, we propose a novel adversarial encoder-decoder-classifier framework to learn a modality-invariant embedding space. Since the distributions of various modalities vary in nature, to reduce the modality gap, we translate the distributions of source modalities into that of target modality via their respective encoders …

* [35] Z Liu, B Zhou, D Chu, Y Sun, L Meng. [[_Modality translation-based multimodal sentiment analysis under uncertain missing modalities_]](https://www.sciencedirect.com/science/article/pii/S1566253523002890)      
    Multimodal sentiment analysis (MSA) with uncertain missing modalities poses a new challenge in sentiment analysis. To address this problem, efficient MSA models that consider missing modalities have been proposed. However, existing studies have only adopted the concatenation operation for feature fusion while ignoring the deep interactions between different modalities. Moreover, existing studies have failed to take advantage of the text modality, which can achieve better accuracy in sentiment analysis. To tackle the above …

* [36] Z Lin, B Liang, Y Long, Y Dang…. [[_Hierarchical Graph Contrastive Learning for Multimodal Sentiment Analysis_]](https://repository.essex.ac.uk/34855/)      
    The existing research efforts in Multimodal Sentiment Analysis (MSA) have focused on developing the expressive ability of neural networks to fuse information from different modalities. However, these approaches lack a mechanism to understand the complex relations within and across different modalities, since some sentiments may be scattered in different modalities. To this end, in this paper, we propose a novel hierarchical graph contrastive learning (HGraph-CL) framework for MSA, aiming to explore the intricate …

* [37] A Zadeh, R Zellers, E Pincus, LP Morency. [[_Multimodal Corpus of Sentiment Intensity and Subjectivity Analysis in Online Opinion Videos_]](https://arxiv.org/abs/1606.06259)      
    People are sharing their opinions, stories and reviews through online video sharing websites every day. Studying sentiment and subjectivity in these opinion videos is experiencing a growing attention from academia and industry. While sentiment analysis has been successful for text, it is an understudied research question for videos and multimedia content. The biggest setbacks for studies in this direction are lack of a proper dataset, methodology, baselines and statistical analysis of how information from different modality …

* [38] J Yang, Y Wang, R Yi, Y Zhu, A Rehman…. [[_Modal-Temporal Attention Graph for Unaligned Human Multimodal Language Sequences_]](https://arxiv.org/abs/2010.11985)      
    Human communication is multimodal in nature; it is through multiple modalities such as language, voice, and facial expressions, that opinions and emotions are expressed. Data in this domain exhibits complex multi-relational and temporal interactions. Learning from this data is a fundamentally challenging research problem. In this paper, we propose Modal-Temporal Attention Graph (MTAG). MTAG is an interpretable graph-based neural model that provides a suitable framework for analyzing multimodal sequential data. We first introduce a …

* [39] L Xiao, X Wu, W Wu, J Yang…. [[_MULTI-CHANNEL ATTENTIVE GRAPH CONVOLUTIONAL NETWORK WITH SENTIMENT FUSION FOR MULTIMODAL SENTIMENT ANALYSIS_]](https://ieeexplore.ieee.org/abstract/document/9747542/)      
    Nowadays, with the explosive growth of multimodal reviews on social media platforms, multimodal sentiment analysis has recently gained popularity because of its high relevance to these social media posts. Although most previous studies design various fusion frameworks for learning an interactive representation of multiple modalities, they fail to incorporate sentimental knowledge into inter-modality learning. This pa-per proposes a Multi-channel Attentive Graph Convolutional Network (MAGCN), consisting of two main …

* [40] J Zheng, S Zhang, Z Wang, X Wang…. [[_Multi-Channel Weight-Sharing Autoencoder Based on Cascade Multi-Head Attention for Multimodal Emotion Recognition_]](https://ieeexplore.ieee.org/abstract/document/9693238/)      
    Multimodal Emotion Recognition is challenging because of the heterogeneity gap among different modalities. Due to the powerful ability of feature abstraction, Deep Neural Networks (DNNs) have exhibited significant success in bridging the heterogeneity gap in cross-modal retrieval and generation tasks. In this work, a DNNs-based Multi-channel Weight-sharing Autoencoder with Cascade Multi-head Attention (MCWSA-CMHA) is proposed to generically address the affective heterogeneity gap in MER. Specifically, multimodal heterogeneity …

* [41] L Fang, G Liu, R Zhang. [[_MULTI-GRAINED MULTIMODAL INTERACTION NETWORK FOR SENTIMENT ANALYSIS_]](https://ieeexplore.ieee.org/abstract/document/10446351/)      
    Multimodal sentiment analysis aims to utilize different modalities including language, visual, and audio to identify human emotions in videos. Multimodal interaciton mechanism is the key challenge. Previous works lack modeling of multimodal interaction at different grain levels, and does not suppress redundant information in multimodal interaction. This leads to incomplete multimodal representation with noisy information. To address these issues, we propose Multi-grained Multimodal Interaction Network (MMIN) to provide a more complete …

* [42] S Poria, E Cambria, D Hazarika…. [[_Multi-level Multiple Attentions for Contextual Multimodal Sentiment Analysis_]](https://ieeexplore.ieee.org/abstract/document/8215597/)      
    Multimodal sentiment analysis involves identifying sentiment in videos and is a developing field of research. Unlike current works, which model utterances individually, we propose a recurrent model that is able to capture contextual information among utterances. In this paper, we also introduce attentionbased networks for improving both context learning and dynamic feature fusion. Our model shows 6-8% improvement over the state of the art on a benchmark dataset.

* [43] MS Akhtar, DS Chauhan, D Ghosal, S Poria…. [[_Multi-task Learning for Multi-modal Emotion Recognition and Sentiment Analysis_]](https://arxiv.org/abs/1905.05812)      
    Related tasks often have inter-dependence on each other and perform better when solved in a joint framework. In this paper, we present a deep multi-task learning framework that jointly performs sentiment and emotion analysis both. The multi-modal inputs (ie, text, acoustic and visual frames) of a video convey diverse and distinctive information, and usually do not have equal contribution in the decision making. We propose a context-level inter-modal attention framework for simultaneously predicting the sentiment and expressed emotions of an …

* [44] R Lin, H Hu. [[_Multimodal Contrastive Learning via Uni-Modal Coding and Cross-Modal Prediction for Multimodal Sentiment Analysis_]](https://arxiv.org/abs/2210.14556)      
    Multimodal representation learning is a challenging task in which previous work mostly focus on either uni-modality pre-training or cross-modality fusion. In fact, we regard modeling multimodal representation as building a skyscraper, where laying stable foundation and designing the main structure are equally essential. The former is like encoding robust uni-modal representation while the later is like integrating interactive information among different modalities, both of which are critical to learning an effective …

* [45] PP Liang, Z Liu, A Zadeh, LP Morency. [[_Multimodal Language Analysis with Recurrent Multistage Fusion_]](https://arxiv.org/abs/1808.03920)      
    Computational modeling of human multimodal language is an emerging research area in natural language processing spanning the language, visual and acoustic modalities. Comprehending multimodal language requires modeling not only the interactions within each modality (intra-modal interactions) but more importantly the interactions between modalities (cross-modal interactions). In this paper, we propose the Recurrent Multistage Fusion Network (RMFN) which decomposes the fusion problem into multiple stages, each of …

* [46] Z Wu, Z Gong, J Koo, J Hirschberg. [[_Multimodal Multi-loss Fusion Network for Sentiment Analysis_]](https://aclanthology.org/2024.naacl-long.197/)      
    This paper investigates the optimal selection and fusion of feature encoders across multiple modalities and combines these in one neural network to improve sentiment detection. We compare different fusion methods and examine the impact of multi-loss training within the multi-modality fusion network, identifying surprisingly important findings relating to subnet performance. We have also found that integrating context significantly enhances model performance. Our best model achieves state-of-the-art performance for three datasets (CMU …

* [47] J Cheng, I Fostiropoulos, B Boehm…. [[_Multimodal Phased Transformer for Sentiment Analysis_]](https://aclanthology.org/2021.emnlp-main.189/)      
    Multimodal Transformers achieve superior performance in multimodal learning tasks. However, the quadratic complexity of the self-attention mechanism in Transformers limits their deployment in low-resource devices and makes their inference and training computationally expensive. We propose multimodal Sparse Phased Transformer (SPT) to alleviate the problem of self-attention complexity and memory footprint. SPT uses a sampling function to generate a sparse attention matrix and compress a long sequence to a shorter …

* [48] J Zheng, S Zhang, X Wang, Z Zeng. [[_Multimodal Representations Learning Based on Mutual Information Maximization and Minimization and Identity Embedding for Multimodal Sentiment Analysis_]](https://arxiv.org/abs/2201.03969)      
    Multimodal sentiment analysis (MSA) is a fundamental complex research problem due to the heterogeneity gap between different modalities and the ambiguity of human emotional expression. Although there have been many successful attempts to construct multimodal representations for MSA, there are still two challenges to be addressed: 1) A more robust multimodal representation needs to be constructed to bridge the heterogeneity gap and cope with the complex multimodal interactions, and 2) the contextual dynamics must be …

* [49] Z Li, Q Guo, C Feng, L Deng, Q Zhang…. [[_Multimodal Sentiment Analysis Based on Interactive Transformer and Soft Mapping_]](https://onlinelibrary.wiley.com/doi/abs/10.1155/2022/6243347)      
    Multimodal sentiment analysis aims to harvest people's opinions or attitudes from multimedia data through fusion techniques. However, existing fusion methods cannot take advantage of the correlation between multimodal data but introduce interference factors. In this paper, we propose an Interactive Transformer and Soft Mapping based method for multimodal sentiment analysis. In the Interactive Transformer layer, an Interactive Multihead Guided‐Attention structure composed of a pair of Multihead Attention modules is first utilized …

* [50] M Chen, S Wang, PP Liang, T Baltrušaitis…. [[_Multimodal Sentiment Analysis with Word-Level Fusion and Reinforcement Learning_]](https://dl.acm.org/doi/abs/10.1145/3136755.3136801)      
    With the increasing popularity of video sharing websites such as YouTube and Facebook, multimodal sentiment analysis has received increasing attention from the scientific community. Contrary to previous works in multimodal sentiment analysis which focus on holistic information in speech segments such as bag of words representations and average facial expression intensity, we propose a novel deep architecture for multimodal sentiment analysis that is able to perform modality fusion at the word level. In this paper, we propose …

* [51] R Kaur, S Kautish. [[_A Survey_]](https://www.igi-global.com/chapter/multimodal-sentiment-analysis/308579)      
    … Multimodal sentiments have become the challenge for the … This survey article covers the 
comprehensive overview of the … are presented briefly in this survey. The article is categorized …

* [52] A Gandhi, K Adhvaryu, S Poria, E Cambria, A Hussain. [[_A systematic review of history, datasets, multimodal fusion methods, applications, challenges and future directions_]](https://www.sciencedirect.com/science/article/pii/S1566253522001634)      
    Sentiment analysis (SA) has gained much traction In the field of artificial intelligence (AI) and natural language processing (NLP). There is growing demand to automate analysis of user sentiment towards products or services. Opinions are increasingly being shared online in the form of videos rather than text alone. This has led to SA using multiple modalities, termed Multimodal Sentiment Analysis (MSA), becoming an important research area. MSA utilises latest advancements in machine learning and deep learning at various stages including for …

* [53] X Yang, S Feng, Y Zhang, D Wang. [[_Multimodal Sentiment Detection Based on Multi-channel Graph Neural Networks_]](https://aclanthology.org/2021.acl-long.28/)      
    With the popularity of smartphones, we have witnessed the rapid proliferation of multimodal posts on various social media platforms. We observe that the multimodal sentiment expression has specific global characteristics, such as the interdependencies of objects or scenes within the image. However, most previous studies only considered the representation of a single image-text post and failed to capture the global co-occurrence characteristics of the dataset. In this paper, we propose Multi-channel Graph Neural …

* [54] Huan Deng, Zhenguo Yang, Tianyong Hao, Qing Li, Wenyin Liu. [[_Multimodal_Affective_Computing_With_Dense_Fusion_Transformer_for_Inter-_and_Intra-Modality_Interactions_]](Multimodal_Affective_Computing_With_Dense_Fusion_Transformer_for_Inter-_and_Intra-Modality_Interactions)      
    This paper proposes a dense fusion transformer (DFT) framework to integrate textual, acoustic, and visual information for multimodal affective computing. DFT exploits a modality-shared transformer (MT) module to extract the modality-shared features by modelling unimodal, bimodal, and trimodal interactions jointly. MT constructs a series of dense fusion blocks to fuse utterance-level sequential features of the multiple modalities from the perspectives of low-level and high-level semantics. In particular, MT adopts local and global transformers to learn modality-shared representations by modelling inter- and intra-modality interactions. Furthermore, we devise a modality-specific representation (MR) module with a soft orthogonality constraint to penalize the distance between modality-specific and modality-shared representations, which are fused by a transformer to make affective predictions. Extensive experiments conducted on five public benchmark datasets show that DFT outperforms the state-of-the-art baselines.

* [55] MS Akhtar, DS Chauhan, D Ghosal, S Poria…. [[_NAACL19_Multi-task Learning for Multi-modal Emotion Recognition and Sentiment Analysis(1)_]](https://arxiv.org/abs/1905.05812)      
    Related tasks often have inter-dependence on each other and perform better when solved in a joint framework. In this paper, we present a deep multi-task learning framework that jointly performs sentiment and emotion analysis both. The multi-modal inputs (ie, text, acoustic and visual frames) of a video convey diverse and distinctive information, and usually do not have equal contribution in the decision making. We propose a context-level inter-modal attention framework for simultaneously predicting the sentiment and expressed emotions of an …

* [56] F Wang, S Tian, L Yu, J Liu, J Wang, K Li…. [[_s12559-022-10073-9_]](https://link.springer.com/article/10.1007/s12559-022-10073-9)      
    Multimodal sentiment analysis is a popular and challenging research topic in natural 
language processing, but the impact of individual modal data in videos on sentiment analysis …

* [57] H Luo, L Ji, Y Huang, B Wang, S Ji, T Li. [[_Improving Multimodal Sentiment Analysis via Multi-Scale Fusion of Locally Descriptors_]](https://arxiv.org/abs/2112.01368)      
    Fusion technique is a key research topic in multimodal sentiment analysis. The recent attention-based fusion demonstrates advances over simple operation-based fusion. However, these fusion works adopt single-scale, ie, token-level or utterance-level, unimodal representation. Such single-scale fusion is suboptimal because that different modality should be aligned with different granularities. This paper proposes a fusion model named ScaleVLAD to gather multi-Scale representation from text, video, and audio with shared …

* [58] H Yang, X Gao, J Wu, T Gan, N Ding…. [[_Self-adaptive Context and Modal-interaction Modeling For Multimodal Emotion Recognition_]](https://aclanthology.org/2023.findings-acl.390/)      
    The multimodal emotion recognition in conversation task aims to predict the emotion label for a given utterance with its context and multiple modalities. Existing approaches achieve good results but also suffer from the following two limitations: 1) lacking modeling of diverse dependency ranges, ie, long, short, and independent context-specific representations and without consideration of the different recognition difficulty for each utterance; 2) consistent treatment of the contribution for various modalities. To address the above challenges, we …

* [59] F Qian, J Han, Y He, T Zheng…. [[_Sentiment Knowledge Enhanced Self-supervised Learning for Multimodal Sentiment Analysis_]](https://aclanthology.org/2023.findings-acl.821/)      
    Abstract Multimodal Sentiment Analysis (MSA) has made great progress that benefits from extraordinary fusion scheme. However, there is a lack of labeled data, resulting in severe overfitting and poor generalization for supervised models applied in this field. In this paper, we propose Sentiment Knowledge Enhanced Self-supervised Learning (SKESL) to capture common sentimental patterns in unlabeled videos, which facilitates further learning on limited labeled data. Specifically, with the help of sentiment knowledge and non-verbal …

* [60] Y Wu, Y Zhao, H Yang, S Chen, B Qin, X Cao…. [[_Sentiment Word Aware Multimodal Refinement for Multimodal Sentiment Analysis with ASR Errors_]](https://arxiv.org/abs/2203.00257)      
    Multimodal sentiment analysis has attracted increasing attention and lots of models have been proposed. However, the performance of the state-of-the-art models decreases sharply when they are deployed in the real world. We find that the main reason is that real-world applications can only access the text outputs by the automatic speech recognition (ASR) models, which may be with errors because of the limitation of model capacity. Through further analysis of the ASR outputs, we find that in some cases the sentiment words, the key …

* [61] S Lai, J Li, G Guo, X Hu, Y Li, Y Tan, Z Song…. [[_Shared and Private Information Learning in Multimodal Sentiment Analysis with Deep Modal Alignment and Self-supervised Multi-Task Learning_]](https://arxiv.org/abs/2305.08473)      
    Designing an effective representation learning method for multimodal sentiment analysis tasks is a crucial research direction. The challenge lies in learning both shared and private information in a complete modal representation, which is difficult with uniform multimodal labels and a raw feature fusion approach. In this work, we propose a deep modal shared information learning module based on the covariance matrix to capture the shared information between modalities. Additionally, we use a label generation module based on a …

* [62] C Zhu, M Chen, S Zhang, C Sun, H Liang, Y Liu…. [[_Sentiment Knowledge Enhanced Attention Fusion Network for multimodal sentiment analysis_]](https://www.sciencedirect.com/science/article/pii/S1566253523002749)      
    Multimodal sentiment analysis is an active research field that aims to recognize the user's sentiment information from multimodal data. The primary challenge in this field is to develop a high-quality fusion framework that effectively addresses the heterogeneity among different modalities. However, prior research has primarily concentrated on intermodal interactions while neglecting the semantic sentiment information conveyed by words in the text modality. In this paper, we propose the Sentiment Knowledge Enhanced Attention Fusion Network …

* [63] M Chen, X Li. [[_Sentimental Words Aware Fusion Network for Multimodal Sentiment Analysis_]](https://aclanthology.org/2020.coling-main.93/)      
    Multimodal sentiment analysis aims to predict sentiment of language text with the help of other modalities, such as vision and acoustic features. Previous studies focused on learning the joint representation of multiple modalities, ignoring some useful knowledge contained in language modal. In this paper, we try to incorporate sentimental words knowledge into the fusion network to guide the learning of joint representation of multimodal features. Our method consists of two components: shallow fusion part and aggregation part. For the …

* [64] Y Wei, S Yuan, R Yang, L Shen, Z Li…. [[_Tackling Modality Heterogeneity with Multi-View Calibration Network for Multimodal Sentiment Detection_]](https://aclanthology.org/2023.acl-long.287/)      
    With the popularity of social media, detecting sentiment from multimodal posts (eg image-text pairs) has attracted substantial attention recently. Existing works mainly focus on fusing different features but ignore the challenge of modality heterogeneity. Specifically, different modalities with inherent disparities may bring three problems: 1) introducing redundant visual features during feature fusion; 2) causing feature shift in the representation space; 3) leading to inconsistent annotations for different modal data. All these issues will increase the …

* [65] J Zeng, T Liu, J Zhou. [[_Tag-assisted Multimodal Sentiment Analysis under Uncertain_]](https://dl.acm.org/doi/abs/10.1145/3477495.3532064)      
    Multimodal sentiment analysis has been studied under the assumption that all modalities are available. However, such a strong assumption does not always hold in practice, and most of multimodal fusion models may fail when partial modalities are missing. Several works have addressed the missing modality problem; but most of them only considered the single modality missing case, and ignored the practically more general cases of multiple modalities missing. To this end, in this paper, we propose a Tag-Assisted Transformer …

* [66] J Yu, J Wang, R Xia, J Li. [[_Targeted Multimodal Sentiment Classifcation Based on Coarse-to-Fine Grained Image-Target Matching_]](https://www.ijcai.org/proceedings/2022/0622.pdf)      
    Abstract Targeted Multimodal Sentiment Classification (TMSC) aims to identify the sentiment polarities over each target mentioned in a pair of sentence and image. Existing methods to TMSC failed to explicitly capture both coarse-grained and fine-grained image-target matching, including 1) the relevance between the image and the target and 2) the alignment between visual objects and the target. To tackle this issue, we propose a new multi-task learning architecture named coarse-to-fine grained Image-Target Matching network (ITM) …

* [67] M Zhou, W Quan, Z Zhou, K Wang, T Wang…. [[_Text-oriented Cross Attention Network for Multimodal Sentiment Analysis_]](https://arxiv.org/abs/2404.04545)      
    Multimodal Sentiment Analysis (MSA) endeavors to understand human sentiment by leveraging language, visual, and acoustic modalities. Despite the remarkable performance exhibited by previous MSA approaches, the presence of inherent multimodal heterogeneities poses a challenge, with the contribution of different modalities varying considerably. Past research predominantly focused on improving representation learning techniques and feature fusion strategies. However, many of these efforts overlooked the …

* [68] C Huang, J Zhang, X Wu, Y Wang, M Li…. [[_Text-centered fusion network with crossmodal attention for multimodal sentiment analysis_]](https://www.sciencedirect.com/science/article/pii/S0950705123002526)      
    Multimodal sentiment analysis (MSA), which goes beyond the analysis of texts to include other modalities such as audio and visual data, has attracted a significant amount of attention. An effective fusion of sentiment information in multiple modalities is key to improving the performance of MSA. However, aligning multiple modalities during the process of fusion faces challenges such as maintaining modal-specific information. This paper proposes a Text-centered Fusion Network with crossmodal Attention (TeFNA), a …

* [69] A Zadeh, M Chen, S Poria, E Cambria…. [[_Tensor Fusion Network for Multimodal Sentiment Analysis_]](https://arxiv.org/abs/1707.07250)      
    Multimodal sentiment analysis is an increasingly popular research area, which extends the conventional language-based definition of sentiment analysis to a multimodal setup where other relevant modalities accompany language. In this paper, we pose the problem of multimodal sentiment analysis as modeling intra-modality and inter-modality dynamics. We introduce a novel model, termed Tensor Fusion Network, which learns both such dynamics end-to-end. The proposed approach is tailored for the volatile nature of spoken language in …

* [70] D Wang, X Guo, Y Tian, J Liu, LH He, X Luo. [[_A text enhanced transformer fusion network for multimodal sentiment analysis_]](https://www.sciencedirect.com/science/article/pii/S0031320322007385)      
    Multimodal sentiment analysis (MSA), which aims to recognize sentiment expressed by speakers in videos utilizing textual, visual and acoustic cues, has attracted extensive research attention in recent years. However, textual, visual and acoustic modalities often contribute differently to sentiment analysis. In general, text contains more intuitive sentiment-related information and outperforms nonlinguistic modalities in MSA. Seeking a strategy to take advantage of this property to obtain a fusion representation containing more sentiment …

* [71] L Stappen, A Baird, L Schumann…. [[_Collection, Insights and Improvements_]](https://ieeexplore.ieee.org/abstract/document/9484711/)      
    Truly real-life data presents a strong, but exciting challenge for sentiment and emotion research. The high variety of possible 'in-the-wild'properties makes large datasets such as these indispensable with respect to building robust machine learning models. A sufficient quantity of data covering a deep variety in the challenges of each modality to force the exploratory analysis of the interplay of all modalities has not yet been made available in this context. In this contribution, we present MuSe-CaR, a first of its kind multimodal dataset. The …

* [72] X Zhao, Y Chen, S Liu, X Zang, Y Xiang…. [[_A New Token Mixup Multimodal Data Augmentation for Multimodal Sentiment Analysis_]](https://dl.acm.org/doi/abs/10.1145/3543507.3583406)      
    Existing methods for Multimodal Sentiment Analysis (MSA) mainly focus on integrating multimodal data effectively on limited multimodal data. Learning more informative multimodal representation often relies on large-scale labeled datasets, which are difficult and unrealistic to obtain. To learn informative multimodal representation on limited labeled datasets as more as possible, we proposed TMMDA for MSA, a new Token Mixup Multimodal Data Augmentation, which first generates new virtual modalities from the mixed …

* [73] Y Li, W Weng, C Liu. [[_two-stage contrastive learning and feature hierarchical fusion network for multimodal sentiment analysis_]](https://link.springer.com/article/10.1007/s00521-024-09634-w)      
    Multimodal sentiment analysis faces two challenges: modality representation and modality fusion. Most of the existing models rely only on the feature extraction network to learn modality representation, and the fusion mechanism adopted by some models does not perform well. These factors are not conducive to the model learning rich emotional information and further affect the model's predictive ability. To solve these problems, we propose a multimodal sentiment analysis model based on two-stage contrastive learning …

* [74] Y Zeng, S Mai, H Hu. [[_Modulating Unimodal and Cross-modal Dynamics for Multimodal Sentiment Analysis_]](https://arxiv.org/abs/2111.08451)      
    Multimodal sentiment analysis (MSA) draws increasing attention with the availability of multimodal data. The boost in performance of MSA models is mainly hindered by two problems. On the one hand, recent MSA works mostly focus on learning cross-modal dynamics, but neglect to explore an optimal solution for unimodal networks, which determines the lower limit of MSA models. On the other hand, noisy information hidden in each modality interferes the learning of correct cross-modal dynamics. To address the above …

* [75] W Yu, H Xu, Z Yuan, J Wu. [[__AAAI21_Learning Modality-Specific Representations with Self-Supervised Multi-Task Learning for Multimodal Sentiment Analysis_]](https://ojs.aaai.org/index.php/AAAI/article/view/17289)      
    Abstract Representation Learning is a significant and challenging task in multimodal learning. Effective modality representations should contain two parts of characteristics: the consistency and the difference. Due to the unified multimodal annota-tion, existing methods are restricted in capturing differenti-ated information. However, additional unimodal annotations are high time-and labor-cost. In this paper, we design a la-bel generation module based on the self-supervised learning strategy to acquire independent unimodal …

* [76] Yao-Hung Hubert Tsai, Shaojie Bai, Paul Pu Liang, J. Zico Kolter, Louis-Philippe Morency, Ruslan Salakhutdinov. [[__Multimodal Transformer for Unaligned Multimodal Language Sequences_]](https://arxiv.org/abs/1906.00295)      
    Human language is often multimodal, which comprehends a mixture of natural language, facial gestures, and acoustic behaviors. However, two major challenges in modeling such multimodal human language time-series data exist: 1) inherent data non-alignment due to variable sampling rates for the sequences from each modality; and 2) long-range dependencies between elements across modalities. In this paper, we introduce the Multimodal Transformer (MulT) to generically address the above issues in an end-to-end manner without explicitly aligning the data. At the heart of our model is the directional pairwise crossmodal attention, which attends to interactions between multimodal sequences across distinct time steps and latently adapt streams from one modality to another. Comprehensive experiments on both aligned and non-aligned multimodal time-series show that our model outperforms state-of-the-art methods by a large margin. In addition, empirical analysis suggests that correlated crossmodal signals are able to be captured by the proposed crossmodal attention mechanism in MulT.

* [77] J Tang, K Li, X Jin, A Cichocki, Q Zhao…. [[_Hierarchical Learning for Multimodal Sentiment Analysis Using Coupled-Translation Fusion Network_]](https://aclanthology.org/2021.acl-long.412/)      
    Multimodal sentiment analysis is the challenging research area that attends to the fusion of multiple heterogeneous modalities. The main challenge is the occurrence of some missing modalities during the multimodal fusion procedure. However, the existing techniques require all modalities as input, thus are sensitive to missing modalities at predicting time. In this work, the coupled-translation fusion network (CTFN) is firstly proposed to model bi-direction interplay via couple learning, ensuring the robustness in respect to missing modalities …

* [78] A Zadeh, YS Cao, S Hessner, PP Liang…. [[_A Multimodal Language Dataset for Spanish, Portuguese, German and French_]](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8106386/)      
    Modeling multimodal language is a core research area in natural language processing. While languages such as English have relatively large multimodal language resources, other widely spoken languages across the globe have few or no large-scale datasets in this area. This disproportionately affects native speakers of languages other than English. As a step towards building more equitable and inclusive multimodal systems, we introduce the first large-scale multimodal language dataset for Spanish, Portuguese, German and French …

* [79] W Han, H Chen, S Poria. [[__EMNLP21_Improving Multimodal Fusion with Hierarchical Mutual Information Maximization for Multimodal Sentiment Analysis_]](https://arxiv.org/abs/2109.00412)      
    In multimodal sentiment analysis (MSA), the performance of a model highly depends on the quality of synthesized embeddings. These embeddings are generated from the upstream process called multimodal fusion, which aims to extract and combine the input unimodal raw data to produce a richer multimodal representation. Previous work either back-propagates the task loss or manipulates the geometric property of feature spaces to produce favorable fusion results, which neglects the preservation of critical task-related information that flows …

* [80] Z Yuan, W Li, H Xu, W Yu. [[__MM21_Transformer-based Feature Reconstruction Network for Robust Multimodal Sentiment Analysis_]](https://dl.acm.org/doi/abs/10.1145/3474085.3475585)      
    Improving robustness against data missing has become one of the core challenges in Multimodal Sentiment Analysis (MSA), which aims to judge speaker sentiments from the language, visual, and acoustic signals. In the current research, translation-based methods and tensor regularization methods are proposed for MSA with incomplete modality features. However, both of them fail to cope with random modality feature missing in non-aligned sequences. In this paper, a transformer-based feature reconstruction network (TFR-Net) is …




### _probe_ ###
* [1] T Le Scao, A Fan, C Akiki, E Pavlick, S Ilić, D Hesslow… - 2023 - inria.hal.science. [[_A 176B-Parameter Open-Access Multilingual Language Model_]](https://inria.hal.science/hal-03850124/)      
    … present BLOOM, a 176B-parameter open-access language model … BLOOM is a decoder-only 
Transformer language model that … and 13 programming languages (59 in total). We find that …

* [2] Z Elyoseph, D Hadar-Shoval, K Asraf…. [[_ChatGPT outperforms humans in emotional awareness evaluations_]](https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2023.1199058/full)      
    The artificial intelligence chatbot, ChatGPT, has gained widespread attention for its ability to perform natural language processing tasks and has the fastest-growing user base in history. Although ChatGPT has successfully generated theoretical information in multiple fields, its ability to identify and describe emotions is still unknown. Emotional awareness (EA), the ability to conceptualize one's own and others' emotions, is considered a transdiagnostic mechanism for psychopathology. This study utilized the Levels of Emotional Awareness …

* [3] K Schaaff, C Reinig, T Schlippe. [[_Exploring ChatGPT’s Empathic Abilities_]](https://ieeexplore.ieee.org/abstract/document/10388208/)      
    Empathy is often understood as the ability to share and understand another individual's state of mind or emotion. With the increasing use of chatbots in various domains, eg, children seeking help with homework, individuals looking for medical advice, and people using the chatbot as a daily source of everyday companionship, the importance of empathy in human-computer interaction has become more apparent. Therefore, our study investigates the extent to which ChatGPT based on GPT-3.5 can exhibit empathetic responses and …

* [4] W Zhao, Y Zhao, X Lu, S Wang, Y Tong…. [[_Is ChatGPT Equipped with Emotional Dialogue Capabilities__]](https://arxiv.org/abs/2304.09582)      
    This report presents a study on the emotional dialogue capability of ChatGPT, an advanced language model developed by OpenAI. The study evaluates the performance of ChatGPT on emotional dialogue understanding and generation through a series of experiments on several downstream tasks. Our findings indicate that while ChatGPT's performance on emotional dialogue understanding may still lag behind that of supervised models, it exhibits promising results in generating emotional responses. Furthermore, the study suggests …

* [5] H Touvron, L Martin, K Stone, P Albert…. [[_Open Foundation and Fine-Tuned Chat Models_]](https://arxiv.org/abs/2307.09288)      
    … large language models (LLMs) ranging in scale from 7 … fine-tuned LLMs, called Llama 
2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on …



