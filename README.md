
_Papers_
----------------
### _Missing_modality_ ###
* [1]Zhou, T., Canu, S., Vera, P., & Ruan, S. (2021). Feature-enhanced generation and multi-modality fusion based deep neural network for brain tumor segmentation with missing MR modalities. Neurocomputing, 466, 102-112.  
    [[_paper_]](https://www.sciencedirect.com/science/article/abs/pii/S0925231221013904)
* [2]Zhao, J., Li, R., & Jin, Q. (2021, August). Missing modality imagination network for emotion recognition with uncertain missing modalities. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) (pp. 2608-2618).  
    [[_paper_]](https://aclanthology.org/2021.acl-long.203/) [[_code_]](https://github.com/AIM3-RUC/MMIN)
* [3]Luo, W., Xu, M., & Lai, H. (2023, January). Multimodal reconstruct and align net for missing modality problem in sentiment analysis. In International Conference on Multimedia Modeling (pp. 411-422). Cham: Springer Nature Switzerland.  
    [[_paper_]](https://link.springer.com/chapter/10.1007/978-3-031-27818-1_34)
* [4]Zhang, Q., Shi, L., Liu, P., Zhu, Z., & Xu, L. (2023). Retraction Note: ICDN: integrating consistency and difference networks by transformer for multimodal sentiment analysis.  
    [[_paper_]](https://link.springer.com/article/10.1007/s10489-023-04869-x)
* [5]Zeng, J., Liu, T., & Zhou, J. (2022, July). Tag-assisted multimodal sentiment analysis under uncertain missing modalities. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval (pp. 1545-1554).  
    [[_paper_]](https://dl.acm.org/doi/abs/10.1145/3477495.3532064) [[_code_]](https://github.com/JaydenZeng/TATE)
* [6]Parthasarathy, S., & Sundaram, S. (2020, October). Training strategies to handle missing modalities for audio-visual expression recognition. In Companion Publication of the 2020 International Conference on Multimodal Interaction (pp. 400-404).  
    [[_paper_]](https://dl.acm.org/doi/abs/10.1145/3395035.3425202)
* [7]Shang, C., Palmer, A., Sun, J., Chen, K. S., Lu, J., & Bi, J. (2017, December). VIGAN: Missing view imputation with generative adversarial networks. In 2017 IEEE International conference on big data (Big Data) (pp. 766-775). IEEE.  
    [[_paper_]](https://ieeexplore.ieee.org/abstract/document/8257992) [[_code_]](https://github.com/chaoshangcs/VIGAN)



### _multimodal affective_ ###
#### _MABSA_ ####
* [1]Zhao, H., Yang, M., Bai, X., & Liu, H. (2024). A Survey on Multimodal Aspect-Based Sentiment Analysis. IEEE Access.   
    [[_paper_]](https://ieeexplore.ieee.org/abstract/document/10401113)
* [2]Wang, C., Luo, Y., Meng, C., & Yuan, F. (2024). An adaptive Dual Graph Convolution Fusion Network for Aspect-Based Sentiment Analysis. ACM Transactions on Asian and Low-Resource Language Information Processing.    
    [[_paper_]](https://dl.acm.org/doi/abs/10.1145/3659579)
* [3]Zhou, R., Guo, W., Liu, X., Yu, S., Zhang, Y., & Yuan, X. (2023). AoM: Detecting aspect-oriented information for multimodal aspect-based sentiment analysis. arXiv preprint arXiv:2306.01004.     
    [[_paper_]](https://arxiv.org/abs/2306.01004) [[_code_]](https://github.com/SilyRab/AoM)
* [4]Xiao, L., Wu, X., Yang, S., Xu, J., Zhou, J., & He, L. (2023). Cross-modal fine-grained alignment and fusion network for multimodal aspect-based sentiment analysis. Information Processing & Management, 60(6), 103508.    
    [[_paper_]](https://www.sciencedirect.com/science/article/abs/pii/S0306457323002455)
* [5]Yu, Z., Wang, J., Yu, L. C., & Zhang, X. (2022, November). Dual-encoder transformers with cross-modal alignment for multimodal aspect-based sentiment analysis. In Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) (pp. 414-423).    
    [[_paper_]](https://aclanthology.org/2022.aacl-main.32/) [[_code_]](https://github.
com/windforfurture/DTCA)
* [6]Yang, L., Na, J. C., & Yu, J. (2022). Cross-modal multitask transformer for end-to-end multimodal aspect-based sentiment analysis. Information Processing & Management, 59(5), 103038.    
    [[_paper_]](https://www.sciencedirect.com/science/article/abs/pii/S0306457322001479)
* [7]J. Yu, J. Jiang and R. Xia, "Entity-Sensitive Attention and Fusion Network for Entity-Level Multimodal Sentiment Classification," in IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 28, pp. 429-439, 2020, doi: 10.1109/TASLP.2019.2957872.         
    [[_paper_]](https://ieeexplore.ieee.org/document/8926404) [[_code_]](https://github.com/jefferyYu/ESAFN)
* [8]Yang, X., Feng, S., Wang, D., Qi, S., Wu, W., Zhang, Y., ... & Poria, S. (2023). Few-shot joint multimodal aspect-sentiment analysis based on generative multimodal prompt. arXiv preprint arXiv:2305.10169.     
    [[_paper_]](https://arxiv.org/abs/2305.10169) [[_code_]](https://github.com/YangXiaocui1215/GMP.)
* [9]J. Zhao and F. Yang, "Fusion with GCN and SE-ResNeXt Network for Aspect Based Multimodal Sentiment Analysis," 2023 IEEE 6th Information Technology,Networking,Electronic and Automation Control Conference (ITNEC), Chongqing, China, 2023, pp. 336-340, doi: 10.1109/ITNEC56291.2023.10082618.    
    [[_paper_]](https://ieeexplore.ieee.org/document/10082618) 
* [10]





